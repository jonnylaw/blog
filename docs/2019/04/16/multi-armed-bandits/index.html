<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.55.1" />


<title>Multi-armed Bandits in Scala - Bayesian Statistics and Functional Programming </title>
<meta property="og:title" content="Multi-armed Bandits in Scala - Bayesian Statistics and Functional Programming ">



  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/tomorrow-night-eighties.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/">Blog</a></li>
    
    <li><a href="https://github.com/jonnylaw">GitHub</a></li>
    
    <li><a href="https://twitter.com/lawsy">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">4 min read</span>
    

    <h1 class="article-title">Multi-armed Bandits in Scala</h1>

    
    <span class="article-date">2019/04/16</span>
    

    <div class="article-content">
      


<div id="setting-up-the-environment" class="section level2">
<h2>Setting up the Environment</h2>
<p>This post uses <a href="https://almond.sh/">Almond</a> in order to run Scala code in a Jupyter notebook. See <a href="/2019/04/15/scala-and-jupyter-notebook-with-almond/">my previous post</a> for my setup of Jupyter, Ammonite and Almond. That post examined using the Scala libraries EvilPlot (including inline plotting in the Jupyter notebook) and Rainier for Bayesian inference in a simple linear model.</p>
<p>The imports required for this post are:</p>
<pre class="scala"><code>import coursier.MavenRepository

interp.repositories() ++= Seq(MavenRepository(
  &quot;http://dl.bintray.com/cibotech/public&quot;
))

import $ivy.`com.stripe::rainier-core:0.2.2`
import $ivy.`com.stripe::rainier-plot:0.2.2`
import $ivy.`org.scalanlp::breeze:0.13.2`

import breeze.stats.distributions._
import breeze.linalg._
import almond.interpreter.api._</code></pre>
<p>The full working notebook can be found <a href="https://github.com/jonnylaw/blog/tree/master/notebooks/multi_armed_bandit.ipynb">here</a>.</p>
</div>
<div id="a-multi-armed-bandit" class="section level1">
<h1>A Multi-armed Bandit</h1>
<p>A multi-armed bandit is an analogy taken from the one-armed bandit slot machines where a lever is pulled and the player has an unknown probability of a prize. A multi-armed bandit is a generalisation, whereby the player have multiple one-armed bandits each of which could have different rewards. The problem is to determine the best bandit to play. One way to determine this is to randomly pull levers to get information on how frequently each bandit pays out. Assuming the probability of payout is constant in time, then after a period of exploration the player will be able to know which bandits pay out most frequently.</p>
</div>
<div id="epsilon-greedy-method" class="section level1">
<h1>Epsilon Greedy Method</h1>
<p>One strategy to maximise the expected long-term reward from a bandit is to choose the bandit with the largest long-term reward a fraction of the time and the rest of the time choose a bandit uniformly at random in order to continually explore the space of actions. At each time step, the reward for a given action can be calculated and the long-term reward can be calculated as the function:</p>
<p><span class="math display">\[Q_{t+1}(A) = Q_t(A) + \frac{R_t(A) - Q_t(A)}{N_t(A)}\]</span> where <span class="math inline">\(A\)</span> is the current action, <span class="math inline">\(Q_t(A)\)</span> is the long-term reward at time <span class="math inline">\(t\)</span> for action <span class="math inline">\(A\)</span>, <span class="math inline">\(R_t(A)\)</span> is the instantaneous reward for action <span class="math inline">\(A\)</span> at time <span class="math inline">\(t\)</span> and <span class="math inline">\(N_t(A)\)</span> is the total number of times action <span class="math inline">\(A\)</span> has been performed by time step <span class="math inline">\(t\)</span>. Using this calculation of long-term reward a single step of the epsilon-greedy algorithm for a multi-armed bandit can be calculated:</p>
<pre class="scala"><code>case class BanditState(
    reward: Array[Double],
    longTermReward: List[Double],
    actions: Map[Int, Int]
)

def banditStep(
    epsilon: Double,
    reward: Int =&gt; Rand[Double])(s: BanditState): Rand[BanditState] = {
    for {
        u &lt;- Uniform(0, 1)
        nextAction &lt;- if (u &lt; epsilon) {
          Multinomial(DenseVector.ones[Double](s.actions.size))
        } else {
          Rand.always(s.longTermReward.zipWithIndex.maxBy(_._1)._2)
        }
        newReward &lt;- reward(nextAction)
        prevCount = s.actions.get(nextAction).get
        nextCount  = prevCount + 1
        newLongTermReward = s.longTermReward(nextAction) + (newReward - s.longTermReward(nextAction)) / nextCount
    } yield BanditState(s.reward :+ newReward, 
                s.longTermReward.updated(nextAction, newLongTermReward),
                s.actions.updated(nextAction, nextCount))
}</code></pre>
<p>Firstly, we define a <code>BanditState</code> which contains all of the rewards <span class="math inline">\(R_t(A)\)</span> for each time step, a list of length equal to the number of actions containing the long-term reward for each action. <code>actions</code> represents <span class="math inline">\(N_t(A)\)</span> using a map from the index of the action to the count of actions. The algorithm proceeds by sampling a uniform random number, if this number is less than the chosen value of epsilon, then a random action is sampled from a Multinomial distribution with equal probabilities, otherwise the algorithm selects the action which currently has the highest long-term reward. The values are updated according to the formula above.</p>
<p>To run this algorithm for a pre-determined number of steps, realise that it is recursive and completely determined by the count and long-term reward at the previous time step. Hence it can be implemented as a Markov chain.</p>
<pre class="scala"><code>def buildActions(actions: Int): Map[Int, Int] = {
    (0 until actions).map(a =&gt; a -&gt; 0).toMap
}

def bandit(
    epsilon: Double, 
    actions: Int, 
    reward: Int =&gt; Rand[Double],
    n: Int): BanditState = {
    
    val initState = BanditState(Array(0.0), List.fill(10)(0.0), buildActions(actions))
    MarkovChain(initState)(banditStep(epsilon, reward)).steps.drop(n-1).next
}</code></pre>
<p>We can assume that the rewards for each of the ten actions is Normally distributed and define a suitable reward function</p>
<pre class="scala"><code>val qs = Gaussian(0, 1).sample(10)

// The reward is selected from a N(q(A_t), 1)
def r(qa: Seq[Double])(action: Int): Rand[Double] = 
    Gaussian(qa(action), 1)
    
//  qs: IndexedSeq[Double] = Vector(
//    -1.1319170735731177,
//    0.5392647196381599,
//    0.7127636875526561,
//    0.8765526115252499,
//    -0.9555744042626685,
//    -0.2723645491439034,
//    0.10029206857194808,
//    0.3758538986470721,
//    1.9412629812694995,
//    1.0620845496569054
//)</code></pre>
<p>Then the algorithm can be run for a single multi-armed bandit</p>
<pre class="scala"><code>val oneBandit = bandit(0.5, 10, r(qs), 1000)</code></pre>
<p>The distribution of actions at the end of 1,000 steps with epsilon = 0.5 and number of actions 10 is:</p>
<p><img src="/post/2019-04-16-multi-armed-bandits_files/actions_bar.png" alt="actions distribution" width=500 /></p>
<p>The mean reward at action 8 was highest at approximately 1.95, the epsilon-greedy algorithm prefers to take action 8.</p>
</div>
<div id="multiple-multi-armed-bandits" class="section level1">
<h1>Multiple Multi-armed Bandits</h1>
<p>Now to see the behaviour of a typicaly multi-armed bandit with an approximately constant reward function, calculate the average reward for n = 2,000 10-arm bandits each with 1,000 steps.</p>
<pre class="scala"><code>Vector.fill(2000)(DenseVector(bandit(0.1, 10, r(qs), 1000).reward)).
  reduce(_ + _).
  map(_ / 2000)</code></pre>
<p>The average reward for each time step can then be plotted, this can be used to evaluate different choices of epsilon.</p>
<p><img src="/post/2019-04-16-multi-armed-bandits_files/average_reward.png" alt="actions distribution" width=500 /></p>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/scala.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

