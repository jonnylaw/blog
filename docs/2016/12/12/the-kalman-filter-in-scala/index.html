<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.55.1" />


<title>The Kalman Filter in Scala - Bayesian Statistics and Functional Programming </title>
<meta property="og:title" content="The Kalman Filter in Scala - Bayesian Statistics and Functional Programming ">



  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/tomorrow-night-eighties.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/picture_cropped.jpg"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/">Blog</a></li>
    
    <li><a href="https://github.com/jonnylaw">GitHub</a></li>
    
    <li><a href="https://twitter.com/lawsy">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">7 min read</span>
    

    <h1 class="article-title">The Kalman Filter in Scala</h1>

    
    <span class="article-date">2016/12/12</span>
    

    <div class="article-content">
      


<p>A Dynamic Linear Model (DLM) is a special type of state space model, where the state and observation equations are Normally distributed and linear. A general DLM can be written as follows:</p>
<p><span class="math display">\[\begin{aligned} y_t &amp;= F_t x_t + \nu_t, \qquad \nu_t \sim \mathcal{N}(0, V_t) \\
x_t &amp;= G_tx_{t-1} + \omega_t \qquad \omega_t \sim \mathcal{N}(0, W_t), \end{aligned}\]</span></p>
<p><span class="math inline">\(y_t\)</span> represents the observation of the process at time <span class="math inline">\(t\)</span>, <span class="math inline">\(x_t\)</span> is the value of the unobserved state at time <span class="math inline">\(t\)</span>. The observation error <span class="math inline">\(\nu_t\)</span> and the system error <span class="math inline">\(\omega_t\)</span> are independent and identically distributed (iid) Normal random variables. <span class="math inline">\(F_t\)</span> is the observation matrix which transforms the state space to the observation, <span class="math inline">\(G_t\)</span> is the state transition matrix.</p>
<div id="forward-simulating-from-the-dlm" class="section level2">
<h2>Forward Simulating from the DLM</h2>
<p>A first order polynomial DLM with constant <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span>, and <span class="math inline">\(F_t = 1\)</span>, <span class="math inline">\(G_t = 1\)</span>. can be simulated in scala as follows:</p>
<pre class="scala"><code>import breeze.stats.distributions.Gaussian

case class Data(time: Time, observation: Observation, state: Option[State])
case class Parameters(v: Double, w: Double, m0: Double, c0: Double)

def simulate(p: Parameters): Stream[Data] = {
val stateSpace = Stream.iterate(Gaussian(p.m0, sqrt(p.c0)).draw)(x =&gt; Gaussian(x, sqrt(p.w)).draw)
  stateSpace.zipWithIndex map { case (x, t) =&gt;
    Data(t, x + Gaussian(0, sqrt(p.v)).draw, Some(x)) 
  }
}

val p = Parameters(3.0, 0.5, 0.0, 10.0)
// simulate 16 different realisations of 100 observations, representing 16 stations
val data = (1 to 16) map (id =&gt; (id, simulate(p).take(100).toVector))</code></pre>
<p>We use the built in streaming library’s <code>iterate</code> function to specify the evolution of the latent state. The initial state <span class="math inline">\(x_0\)</span> is a sample drawn from a normal distribution with mean <span class="math inline">\(m_0\)</span> and variance <span class="math inline">\(C_0\)</span>, <span class="math inline">\(x_t \sim \mathcal{N}(x_0 ; m_0, C_0)\)</span>. The <a href="https://github.com/scalanlp/breeze/">Breeze numerical computing library</a> provides many statistical and mathematical functions is used. Subsequent states are generated by adding <span class="math inline">\(\mathcal{N}(0, W)\)</span> to the previous state:</p>
<p><span class="math display">\[x_t = x_{t-1} + \omega, \qquad \omega \sim \mathcal{N}(0, V)\]</span></p>
<p>We then construct a <code>Stream</code> of <code>Data</code> objects. The <code>Data</code> object has a timestamped observation and an optional state space. We construct the observation at time <span class="math inline">\(t\)</span> by simply adding the observation noise to the state space <span class="math inline">\(y_t \sim \mathcal{N}(y_t | x_t, V)\)</span>. The state is optional because we can’t observe the state of real data, only simulated data will have a known state.</p>
<p>A graph of the data from four “stations”, produced using <a href="http://ggplot2.org">ggplot2</a> in R is shown in the figure below.</p>
<p><img src="/post/KalmanFilter_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<div id="an-aside-on-referential-transparency" class="section level3">
<h3>An Aside on Referential Transparency</h3>
<p>Note that the function <code>simulate</code> is not referentially transparent, meaning the function will return a different <code>Stream</code> of data each time we run it. Referential transparency is important in functional programming, to allow us to easily reason about complex programs. The Breeze library implements another object for stateful random number generation, the <code>Process</code> object. The <code>MarkovChain</code> object can be used to construct a process without drawing explicitly from the distribution until we run the program. Firstly define a single step of the random walk:</p>
<pre class="scala"><code>import breeze.stats.distributions._

def step_rw(p: Parameters): Double =&gt; Rand[Double] = 
  x =&gt; Gaussian(x, p.w)</code></pre>
<p>If <code>step_rw</code> is supplied with a set of <code>Parameters</code>, it returns a function from the current state, which is assumed to be materialised, to the next state, which is a <code>Rand[Double]</code>. The actual random number isn’t generated until we sample from distribution represented by <code>Rand</code>. Next, we can construct a <code>MarkovChain</code> using the transition kernel <code>stepRw</code>:</p>
<pre class="scala"><code>val random_walk: Process[Double] = MarkovChain(0.0)(step_rw(p))
random_walk.
  steps.
  take(100)</code></pre>
</div>
</div>
<div id="the-kalman-filter" class="section level2">
<h2>The Kalman Filter</h2>
<p>The Kalman Filter can be used to determine the posterior distribution of the state space given the current observation and the <span class="math inline">\(p(x_t|D_{t})\)</span>, where <span class="math inline">\(D_t = \{Y_t, D_{t-1}\}\)</span> and <span class="math inline">\(D_0\)</span> is the initial information, comprising of the parameters mode parameters <span class="math inline">\(W_0\)</span> and <span class="math inline">\(V_0\)</span> and the initial state <span class="math inline">\(x_0 \sim N(m_0, C_0)\)</span>. The full treatment of the Kalman Filter can be found in the excellent Bayesian Forecasting for Dynamic Models by West and Harrison.</p>
<p>I will present the filtering equations for the simple model in this post. Suppose we start with the posterior distribution of <span class="math inline">\(x_{t-1} \sim N(m_{t-1}, C_{t-1})\)</span>. The first thing we need to do is advance the state, the equation to advance the state is a simple Markov transition <span class="math inline">\(x_t = x_{t-1} + \omega_t\)</span>. We simply add the system variance, the system variance is drawn from a Normal distribution with zero mean and variance <span class="math inline">\(W\)</span>. The <a href="https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables">sum of two Normal distributions is Normal</a> with the mean and variance added, hence the prior for <span class="math inline">\((x_t | D_{t-1}) \sim N(m_{t-1}, C_{t-1} + W)\)</span>.</p>
<p>Next we need to construct the observation, using the observation equation which is commonly called the one-step forecast for the series, <span class="math inline">\(y_t = x_t + \nu_t\)</span>, since <span class="math inline">\(\nu_t\)</span> is Normally distributed with zero mean and variance <span class="math inline">\(V\)</span>, the distribution of the one step forecast is, <span class="math inline">\((y_t|D_{t-1}) \sim N(m_{t-1}, C_{t-1} + W + V)\)</span>.</p>
<p>Now, we observe the true value of <span class="math inline">\(y_t\)</span> and are able to construct the posterior distribution of <span class="math inline">\((x_t | D_t) \sim N(m_t, C_t)\)</span>. <span class="math inline">\(m_t = m_{t-1} + A_t e_t\)</span> and <span class="math inline">\(C_t = A_tV\)</span>. <span class="math inline">\(A_t = \frac{C_{t-1} + W}{ C_{t-1} + W + V}\)</span> is known as the regression coefficient, and <span class="math inline">\(e_t = Y_t - m_{t-1}\)</span>. This result can be shown using properties of the multivariate normal distribution, and is presented in full in Bayesian Forecasting for Dynamic Models by West and Harrison.</p>
<p>We now have the equations required to program up the Kalman Filter using Scala.</p>
<pre class="scala"><code>case class FilterState(data: Data, p: Parameters)

def filter(p: Parameters): (FilterState, Data) =&gt; FilterState = (s, d) =&gt; {
  val r = s.p.c0 + p.w
  val q = r + p.v
  val e = d.observation - s.p.m0

  // kalman gain
  val k = r / q
  val c1 = k * p.v
  
  // return the data with the expectation of the hidden state and the updated Parameters
  FilterOut(Data(d.time, d.observation, Some(m1)), Parameters(p.v, p.w, m1, c1))
} </code></pre>
<p>The function filter simply takes in <code>Parameters</code> and one observation, represented by <code>Data</code> and returns the updated parameters required for the next step of the filter. Now we need to write a function which filters a sequence of <code>Data</code>, and returns a sequence consisting of the latent states, which we can do using the function <code>scanLeft</code>.</p>
<p>A simplified function signature of <code>scanLeft</code> is given by: <code>scanLeft[A](l: List[A], z: A)(f: (A, A) =&gt; A): List[A]</code>. A list, <code>l</code> with elements of type <code>A</code> and an initial value, <code>z</code> also of type <code>A</code> if passed to a <code>Function2</code> and accumulated into another list with elements of type <code>A</code>. The function <code>f</code> is applied to each element of the list pairwise, starting the the head of the lift and the zero element, <code>z</code>. Consider calculating the <code>sum</code> of a list of numbers:</p>
<pre class="scala"><code>val numbers = List(1,2,3,4,5)
def sum(a: Int, b: Int): Int = a + b

numbers.scanLeft(0)(sum)
// List(0, 1, 3, 6, 10, 15)</code></pre>
<p>The first calculation is (0 + 1) = 1, which is then used as the first argument in the pairwise <code>sum</code> function, then the second calculation is (1 + 2) = 3, the result of which is used again in the next application of <code>sum</code>. Each intermediate step of the calculation is retained and appended to a list to be output when the list of number is exhausted, so we end up with a cumulative sum, <code>List(0, 1, 3, 6, 10, 15)</code>. We can use <code>scanLeft</code> and the <code>Function2</code>, <code>filter</code> to calculate and retain the latent states in a DLM:</p>
<pre class="scala"><code>def filterSeries(data: Seq[Data], initParams: Parameters): Seq[FilterOut] = {
  val initFilter = FilterState(data.head, params, 0.0) // initialise the filter

  data.
    scanLeft(initFilter)(filter(initParams)).
    drop(1)
}</code></pre>
<p>Now, we can apply the filter to all the stations simultaneously:</p>
<pre class="scala"><code>data.
  groupBy{ case (id, _) =&gt; id }. //groups by id
  map{ case (id, idAndData) =&gt;
  (id, idAndData map (x =&gt; x._2)) }. // changes into (id, data) pairs
  map{ case (id, data) =&gt;
  (id, filterSeries(data.sortBy(_.time), p)) } // apply the filter to the sorted data</code></pre>
<p>We can now plot the results of the filtering using R and ggplot2, overlaid with 95% prediction intervals.</p>
<p><img src="/post/KalmanFilter_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>The full code is available in the <a href="https://github.com/jonnylaw/blog/tree/master/dlm">GitHub Repo</a> associated with this blog.</p>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/scala.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

