[
  {
    "path": "posts/2021-08-16-mc-dropout-uncertainty/",
    "title": "Uncertainty in Neural Networks",
    "description": "Using MC Dropout to get probability intervals for neural network predictions.",
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2021-08-16",
    "categories": [
      "Python",
      "Deep Learning",
      "Bayesian"
    ],
    "contents": "\nRecently, I have been reading Probabilistic Deep Learning which introduces Bayesian methods for fitting Neural Networks using Tensorflow and Keras. One case study consists of a non-linear regression problem for a sinusoidal curve. Additionally, the curve is considered to have heteroskedastic variance, meaning the variance changes along the domain of the function. In this post I will consider approximating a non-linear function with constant (homoskedastic) variance and quantifying the uncertainty. I will be using PyTorch - because why not.\nUncertainty is an important concept in machine learning, if we have confidence in the predictions of model we are able to make more informed decisions. However, neural networks trained using traditional back-propagation typically return a single point estimate by training to minimise a loss function. There have been many attempts to incorporate uncertainty into neural networks, the most principled way is to fit a Bayesian Neural Network (BNNs) by placing prior distributions on the weights and calculating the posterior distribution using Bayes’ theorem:\n\\[p(\\theta| y) = \\frac{p(\\theta)p(y|\\theta)}{\\int_\\theta p(y|\\theta)p(\\theta)}\\]\nWhere \\(\\theta\\) represents the parameters in the neural network, ie. the weights and biases. \\(p(\\theta)\\) is the prior distribution, and \\(p(y|\\theta)\\) is the likelihood. These can all be specified. However, calculating the evidence, the denominator in the equation, \\(\\int_\\theta p(y|\\theta)p(\\theta)\\) is intractable analytically in most real-world problems, including BNNs. The gold standard for computing these integrals for real-world problems is MCMC. In the case of modern deep learning models, the parameter space is too high dimensional (ie. there are too many parameters!) for these methods to be computationally feasible (although (Izmailov et al. 2021) did use HMC for fitting NNs to the CIFAR-10 image dataset and IMDB review dataset using 512 TPUv3 devices). Hence, we look to approximate methods, such as variational inference, which can place additional assumptions on the form on the prior and posterior distributions. There are also non-Bayesian methods (such as deep ensembles) which can be evaluated to have desirable properties such as calibration. From the Wikipedia for statistical calibration “As Philip Dawid puts it,”a forecaster is well calibrated if, for example, of those events to which he assigns a probability 30 percent, the long-run proportion that actually occurs turns out to be 30 percent“.”\nIn this post we will investigate a single easy to implement method for uncertainty estimation, MC dropout (Gal and Ghahramani 2016), applied to a regression problem.\nLet’s start by creating simulating data from a simple model:\n\\[\n\\begin{aligned}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma^2), \\\\\n\\mu_i &= a \\sin(x_i + b).\n\\end{aligned}\n\\]\nWhere \\(a=2\\), \\(b=-3\\) and \\(\\sigma=1\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_data(xs):\n    a = 5\n    b = 3\n    mean = lambda x: a * np.sin(x + b)\n    data = [(x, np.random.normal(loc=mean(x), scale=1, size=1))\n            for x in xs]\n    x, y = zip(*data)\n    return np.array(x), np.array(y)\n\n\nxs = np.random.uniform(0, 10, size=100)\nx, y = generate_data(xs)\nplt.scatter(x, y)\n\n\nCreate a neural net which can learn the non-linear function. We’ll use the class nn.ModuleList to allow us to experiment with different numbers of hidden layers. According to the MC Dropout paper, we must apply dropout to each layer in order for the procedure to be equivalent to variational inference - under a set of assumptions.\n\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\nclass NonLinearRegression(nn.Module):\n    def __init__(self, dropout, hidden_size, hidden_layers):\n        super(NonLinearRegression, self).__init__()\n        self.input = nn.Linear(1, hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        self.linears = nn.ModuleList(\n          [nn.Linear(hidden_size, hidden_size) for i in range(hidden_layers)])\n        self.dropouts = nn.ModuleList(\n          [nn.Dropout(dropout) for i in range(hidden_layers)])\n        self.output = nn.Linear(hidden_size, 1)\n\n    def forward(self, x):\n        x = self.input(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        for l, d in zip(self.linears, self.dropouts):\n          x = l(x)\n          x = F.relu(x)\n          x = d(x)\n        x = self.output(x)\n        return x\n\nSplit the data randomly into training and testing.\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(np.array(list(x)), np.array(list(y)))\n\nTest the untrained model input and output shapes by making a prediction using the training data.\n\nmodel = NonLinearRegression(0.5, 32, 1)\nx_input = torch.tensor(x_train, dtype=torch.float).unsqueeze(-1)\nmodel(x_input).shape\ntorch.Size([75, 1])\n\nUse Skorch to avoid writing the training boilerplate. Use the .fit method provided by Skorch.\n\nfrom skorch.regressor import NeuralNetRegressor\n\nepochs = 500\n\nnet = NeuralNetRegressor(\n    module=NonLinearRegression,\n    module__dropout=0.1,\n    module__hidden_size=64,\n    module__hidden_layers=3,\n    optimizer=torch.optim.AdamW,\n    iterator_train__shuffle=True,\n    max_epochs=500,\n    verbose=0\n)\n\nx_train_input = torch.tensor(x_train, dtype=torch.float).unsqueeze(-1)\nnet.fit(x_train_input, torch.tensor(y_train, dtype=torch.float))\n<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n  module_=NonLinearRegression(\n    (input): Linear(in_features=1, out_features=64, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (linears): ModuleList(\n      (0): Linear(in_features=64, out_features=64, bias=True)\n      (1): Linear(in_features=64, out_features=64, bias=True)\n      (2): Linear(in_features=64, out_features=64, bias=True)\n    )\n    (dropouts): ModuleList(\n      (0): Dropout(p=0.1, inplace=False)\n      (1): Dropout(p=0.1, inplace=False)\n      (2): Dropout(p=0.1, inplace=False)\n    )\n    (output): Linear(in_features=64, out_features=1, bias=True)\n  ),\n)\n\nWe can visualise the learning curve for this model below.\n\n\n\nWe can see in the figure below that the predictions are in the right region, but using only a point prediction we miss out on capturing the uncertainty.\n\nx_test_input = torch.tensor(x_test, dtype=torch.float).unsqueeze(-1)\npreds = net.predict(x_test_input)\n\n\n\n\nMeasurement error with Distribution\nWe can represent the aleatoric uncertainty, ie. the uncertainty inherent in the data using a Normal distribution. We know the observation distribution of the data generating process is Normal with a standard deviation (called scale in PyTorch) of 1.0. We can learn this additional parameter using PyTorch, to do this we can introduce a new loss function and alter the forward function of the Neural Network module to include the standard deviation parameter. First we’ll consider how to write the new loss function, we need to calculate the log-likelihood of the observations then we wish to maximise this. Neural networks in PyTorch have optimisers with minimise the loss function, hence we will minimise the negative log-likelihood. Let’s first consider the probability density function of a Normal distribution with mean \\(\\mu \\in \\mathbb{R}\\) and standard deviation \\(\\sigma \\in \\mathbb{R}^+\\):\n\\[p(y | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2}(y-\\mu)^2\\right)\\]\nWe have multiple observations which are independent and normally distributed, hence for each batch we will need to calculate the product of the likelihood:\n\\[p(\\mathbb{y}|\\mu, \\sigma) = \\prod_{i=1}^B \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2}(y_i-\\mu)^2\\right),\\]\nWhere \\(B\\) is the batch size, this involves multiplying small numbers together which can result in arithmetic underflow, hence we work on the log-scale which changes the calculation to addition:\n\\[\\log p(\\mathbb{y}|\\mu, \\sigma) =  - \\frac{B}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^B (y_i-\\mu)^2  \\]\nIn the case of the Neural net, \\(\\mu\\) is the output and \\(\\sigma\\) is an additional parameter. We can code the log-likelihood by converting this function into PyTorch or using the build in distributions.\n\nfrom torch.distributions import Normal\nimport math\n\nnormal_model = NonLinearRegression(0.1, 64, 3)\ninputs = model(x_input)\nlabels = torch.tensor(y_train, dtype=torch.float)\n\nmu = inputs\nlog_sigma = torch.tensor(0.0, dtype=torch.float)\n\n# Calculate manually\nmanual_ll = - 0.5 * math.log(2.0 * math.pi) - \\\n  log_sigma - 0.5 * ((labels - mu) / log_sigma.exp()) ** 2\n\n# Use the built in log_prob function\nlog_likelihood = Normal(mu, log_sigma.exp()).log_prob(labels)\n\nlog_likelihood.sum(), manual_ll.sum()\n(tensor(-597.9736, grad_fn=<SumBackward0>), tensor(-597.9736, grad_fn=<SumBackward0>))\n\nWe can modify the nn.Module to return an additional parameter representing the log of the standard deviation of the Normal distribution. Additionally, the forward model will return the predicted mean and the global standard deviation - this is necessary when using Skorch, since the standard form of the loss function is def loss(y_pred, y_true).\n\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\nclass NonLinearRegressionNormal(nn.Module):\n    def __init__(self, dropout, hidden_size, hidden_layers):\n        super(NonLinearRegressionNormal, self).__init__()\n        self.input = nn.Linear(1, hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        self.linears = nn.ModuleList(\n          [nn.Linear(hidden_size, hidden_size) for i in range(hidden_layers)])\n        self.dropouts = nn.ModuleList(\n          [nn.Dropout(dropout) for i in range(hidden_layers)])\n        self.output = nn.Linear(hidden_size, 1)\n        self.log_sigma = nn.Parameter(torch.tensor(1.0))\n\n    def forward(self, x):\n        x = self.input(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        for l, d in zip(self.linears, self.dropouts):\n          x = l(x)\n          x = F.relu(x)\n          x = d(x)\n        x = self.output(x)\n        return x, self.log_sigma\n\nThe standard deviation, sigma, is constant, so in a custom training loop (not using skorch) we could simply pass the model definition to the loss function directly and extract the sigma parameter as follows. This would mean the forward function of the nn.Module can remain the same as the first model. The loss function could look like the following:\n\ndef normal_pdf(model, X, y_true):\n  preds = model(X)\n  return Normal(preds, model.log_sigma.exp()).log_prop(y_true).sum\n\nThen create a loss function as an nn.Module:\n\nfrom skorch.regressor import NeuralNetRegressor\n\nclass NormalLoss(nn.Module):\n  def  __init__(self):\n    super(NormalLoss, self).__init__()\n\n  def forward(self, inputs, labels):\n    mu, log_sigma = inputs\n    log_likelihood = Normal(mu, log_sigma.exp()).log_prob(labels)\n\n    return - log_likelihood.sum()\n\nTrain the model as usual:\n\nepochs = 500\n\nnet_normal = NeuralNetRegressor(\n    module=NonLinearRegressionNormal,\n    module__dropout=0.1,\n    module__hidden_size=64,\n    module__hidden_layers=3,\n    optimizer=torch.optim.AdamW,\n    iterator_train__shuffle=True,\n    criterion=NormalLoss,\n    max_epochs=epochs,\n    verbose=0\n)\n\nx_train_input = torch.tensor(x_train, dtype=torch.float).unsqueeze(-1)\nnet_normal.fit(x_train_input, torch.tensor(y_train, dtype=torch.float))\n<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n  module_=NonLinearRegressionNormal(\n    (input): Linear(in_features=1, out_features=64, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (linears): ModuleList(\n      (0): Linear(in_features=64, out_features=64, bias=True)\n      (1): Linear(in_features=64, out_features=64, bias=True)\n      (2): Linear(in_features=64, out_features=64, bias=True)\n    )\n    (dropouts): ModuleList(\n      (0): Dropout(p=0.1, inplace=False)\n      (1): Dropout(p=0.1, inplace=False)\n      (2): Dropout(p=0.1, inplace=False)\n    )\n    (output): Linear(in_features=64, out_features=1, bias=True)\n  ),\n)\n\nThen calculate some predictions, the actual function is plotted using blue, with the predictions and prediction interval plotted in red. This is an MLE solution with a 95% confidence interval.\n\n\n\nUncertainty with MC Dropout\nWe have looked at how to incorporate aleatoric uncertainty, to understand the uncertainty in the parameters (epistemic uncertainty) we can use MC Dropout. Dropout is a method of avoiding overfitting at training time by removing “connections” in a neural network. However, if we leave dropout on when making predictions, then we create an ensemble of models which output slightly different predictions. It turns out that this is equivalent Bayesian variational inference with some assumptions. We can then calculate the mean and and uncertainty intervals we wish.\nWe can easily implement MC dropout and visualise the uncertainty provided with this method. First, we extract the module from the Skorch NeuralNet class and put it in train mode, this means we make predictions with dropout ON.\n\nnet_normal.module_.train()\nNonLinearRegressionNormal(\n  (input): Linear(in_features=1, out_features=64, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (linears): ModuleList(\n    (0): Linear(in_features=64, out_features=64, bias=True)\n    (1): Linear(in_features=64, out_features=64, bias=True)\n    (2): Linear(in_features=64, out_features=64, bias=True)\n  )\n  (dropouts): ModuleList(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Dropout(p=0.1, inplace=False)\n    (2): Dropout(p=0.1, inplace=False)\n  )\n  (output): Linear(in_features=64, out_features=1, bias=True)\n)\n\nLet’s write a convenience function for making multiple predictions and combining them into a single numpy array.\n\ndef get_predictions(model, x, n):\n  # Make multiple predictions\n  mus, sigmas = zip(*[model(x) for _ in range(n)])\n\n  # Sample from the observation distribution using each mean and the global sigma\n  return [Normal(mu, log_sigma.exp()).sample().detach().numpy() for mu in list(mus)]\n\nWe then have a collection of predictions which we can use to calculate summaries using monte carlo. We can calculate the expectation by calculating the mean of all the predictions, and probability intervals by ordering the predictions and selecting the appropriate values, using np.quantile.\n\ndef get_probability_interval(preds, interval):\n  lower_int = (1. - interval) / 2\n  upper_int = interval + lower_int\n\n  # Calculate percentiles and mean\n  lower = np.quantile(preds, lower_int, axis=0)\n  mean = np.mean(preds, axis=0)\n  upper = np.quantile(preds, upper_int, axis=0) \n    \n  return lower, mean, upper\n\nFirst, we will predict a single point, calculate the mean and the 89% probability interval.\n\ntest_point = 5.0\npreds = get_predictions(net_normal.module_, \n    torch.tensor(test_point, dtype=torch.float).unsqueeze(-1), \n    1000)\n\nlower, mean, upper = get_probability_interval(preds, 0.89)\n\n\n\n\nNext, we can use the same method to predict several points and give the impression of a function. If we extend the \\(\\sin\\) function we can see that the uncertainty on the inputs which are out of the domain of the training examples is quite large. We can evaluate the calibration of these intervals in domain and out of domain by creating a new training split which omits data in a certain interval.\n\nx_in = np.linspace(-5, 15, 50)\n\npreds = get_predictions(\n    net_normal.module_, \n    torch.tensor(x_in, dtype=torch.float).unsqueeze(-1),\n    1000)\n\n\n\n\nCalculate coverage of probability interval\nWe can calculate the coverage of the probability interval using an experiment. We first train a neural network generating data from the same noisy function. We have \\(\\mu = 5\\sin(x + 3)\\) and the observations corrupted by Gaussian noise, \\(y_i \\sim \\mathcal{N}(\\mu, 1^2)\\). Then we sample 100 random uniform test points between 0 and 10 and make probabilistic predictions by calculating 1,000 predictions using MC Dropout and calculating 95% probability intervals. This should account for epistemic uncertainty in the parameters and aleatoric uncertainty inherent in the observed data. We then calculate the proportion of predictions which fall into the interval, this should be close to 95%.\n\ndef coverage_experiment():\n  # Generate training data\n  x_train = np.linspace(0, 10, 50)\n  _, y_train = generate_data(x_train)\n\n  # Fit model to training data\n  x_train_input = torch.tensor(x_train, dtype=torch.float).unsqueeze(-1)\n  net_normal.fit(x_train_input, torch.tensor(y_train, dtype=torch.float))\n\n  # Generate testing data from the same generative model\n  n_test = 100\n  x_test = np.random.uniform(0, 10, n_test)\n  _, y_test = generate_data(x_test)\n\n  net_normal.module_.train()\n\n  # Calculate predictions on test data\n  preds = get_predictions(\n      net_normal.module_,\n      torch.tensor(x_test, dtype=torch.float).unsqueeze(-1),\n      1000)\n\n  lower, mean, upper = get_probability_interval(preds, 0.95)\n\n  # Calculate proportion of predictions which fall into interval\n  in_interval = sum([l <= y <= u for y, l, u in zip(y_test, lower, upper)])\n  return in_interval / n_test\n\nLet’s repeat this experiment 100 times and plot a histogram of the resulting coverage.\n\n\n\n\n\n\nGal, Yarin, and Zoubin Ghahramani. 2016. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” In International Conference on Machine Learning, 1050–59. PMLR.\n\n\nIzmailov, Pavel, Sharad Vikram, Matthew D Hoffman, and Andrew Gordon Wilson. 2021. “What Are Bayesian Neural Network Posteriors Really Like?” arXiv Preprint arXiv:2104.14421.\n\n\n\n\n",
    "preview": "posts/2021-08-16-mc-dropout-uncertainty/mc-dropout-uncertainty_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-08-16T16:07:01+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-04-entity-embeddings/",
    "title": "Entity Embeddings",
    "description": "Creating entity embeddings for categorical predictors using Python.",
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2021-08-04",
    "categories": [
      "Python",
      "Deep Learning"
    ],
    "contents": "\nThe tidymodels framework in R has a function for constructing Entity Embeddings from categorical features. The library is know as embed and the heavy lifting (neural network fitting) is performed using Keras. What if we want something similar in Python and sklearn. First, we will aim to understand what an embedding is and how it can be created (and re-used). Embeddings are familiar to those who have used the Word2Vec model for natural language processing (NLP). Word2Vec is a shallow neural network trained on a corpus of (unlabelled) documents. The task of the network is to predict a context word close to the original word. The resulting shallow network has an “embedding matrix” which has the same number of rows as the number of words in the corpus vocabulary and a user-chosen embedding dimension as the number of columns. Each row represents the position of a word in the embedding space and its location is very close to its meaning. Additionally, we can perform arithmetic with the embeddings and get reasonable answers.\nWe can use this same technique to embed high-dimensional categorical variables when we have lots of data. This can be seen in a 2017 publication from ASOS, an online fashion retailer (Customer Lifetime Value Prediction Using Embeddings, Chamberlain et. al., KDD 2017).\nPyTorch Model\nFirst we must create a new Neural Network architecture, in PyTorch this means that we extend torch.nn.Module class which requires the implementation of a forward method. The forward method creates a prediction from the input, the method consists of applications of matrix multiplications and activations. In this case, we have an Embedding module for each categorical feature (of dimension (n_categories, hidden_dim)), created using nn.ModuleList. The embedding module enables us to “look up” the corresponding row of the embedding matrix for that categorical variable and return the embedding for that category. For the continuous features, we have a linear layer of dimension (num_cont, hidden_dim) which can then be combined using torch.cat, the ReLU activation function is used and the output is calculated using another linear layer of dimension (2 hidden_dim, num_output_classes). There is no activation on the network, since it is typically more efficient to use a loss function which also includes the calculation of the activation function, BCEWithLogitsLoss vs BCELoss.\nPyTorch implements the backprop algorithm which will create a backward function, this backward function is the derivative of the network with respect to the input. This derivative is used in the optimisation algorithms to learn the values of the parameters which minimise the loss function.\nWe will start with some imports required to use PyTorch.\n\nimport torch.nn as nn\nimport torch\n\n\nclass EmbeddingClassification(nn.Module):\n    \"\"\"Embed a single categorical predictor\n    \n    Keyword Arguments:\n    \n    num_output_classes: int\n    num_cat_classes: list[int]\n    num_cont: int\n    embedding_dim: int\n    hidden_dim: int\n    \"\"\"\n    def __init__(self, num_output_classes, num_cat_classes, num_cont, \n    embedding_dim=64, hidden_dim=64):\n        super().__init__()\n        # Create an embedding for each categorical input\n        self.embeddings = nn.ModuleList([nn.Embedding(nc, embedding_dim) for nc in num_cat_classes])\n        self.fc1 = nn.Linear(in_features=len(num_cat_classes) * embedding_dim, out_features=hidden_dim)\n        self.fc2 = nn.Linear(in_features=num_cont, out_features=hidden_dim)\n        self.relu = nn.ReLU()\n        self.out = nn.Linear(2 * hidden_dim, num_output_classes)\n        \n    def forward(self, x_cat, x_con):\n        # Embed each of the categorical variables\n        x_embed = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n        x_embed = torch.cat(x_embed, dim=1)\n        x_embed = self.fc1(x_embed)\n        x_con = self.fc2(x_con)\n        x = torch.cat([x_con, x_embed.squeeze()], dim=1)\n        x = self.relu(x)\n        return self.out(x)    \n\nTitanic Dataset\nI will show how the embeddings work in practice using the titanic dataset. This is not the ideal dataset to use with embeddings since each categorical variable has a small number of categories, however it is well understood and useful for pedagogical purposes.\n\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\n\nSEED=7\n\nFirst we must pre-process the data,\nParse the Name column to extract the title as an additional categorical variable\nSelect the columns to include\nInterpolate the numeric columns using a KNNImputer\nSplit the data into a training/test split\n\ndf = pd.read_csv('titanic.csv')\n\n# Derive title column\ndf['Title'] = df['Name'].str.extract('([A-Za-z]+\\.)', expand = False)\n\n# Count the occurences of Title by category\ndef get_category_count(x, categorical_columns):\n    return [Counter(x[c]) for c in categorical_columns]\n  \n# Filter low occurences (less than or equal to 3?)\ncat_counts = get_category_count(df, [\"Title\"])\nrare_titles = [k for k, v in cat_counts[0].items() if v < 3]\ndf['Title'].replace(to_replace=rare_titles, value='other', inplace=True)\n\ninclude = ['Sex', 'Age', 'Fare', 'Title']\nx = df[include]\ny = df['Survived']\n\n# Define the numeric and categorical columns\nnum_cols = ['Fare', 'Age']\ncat_cols = ['Sex', 'Title']\n\n# Split the data into training and test\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=SEED)\n\n# Interpolate the numeric columns using KNN and scale using the StandardScaler\nnum_standard = Pipeline(steps=[\n    ('imputer', KNNImputer(n_neighbors=5)),\n    ('scaler', StandardScaler())\n])\n\nWe then need to encode the categorical variables by assigning a number to each of the possiblities. This allows us to use an embedding layer in the PyTorch neural network (NN). NNs only work with numerical input.\n\npreprocessor = ColumnTransformer(\ntransformers=[\n    (\"num_std\", num_standard, num_cols),\n    (\"ordinal_encode\", OrdinalEncoder(), cat_cols),\n    ]\n)\n\npreprocessed = preprocessor.fit_transform(x_train, y=None)\npreprocessed_df = pd.DataFrame(preprocessed, columns=num_cols + cat_cols)\n\n\ndef get_categorical_dimensions(x, categorical_columns):\n  count_of_classes = get_category_count(x, categorical_columns)\n  return [len(count) for count in count_of_classes]\n\n\ndef entity_encoding_classification(x, categorical_columns, num_classes):\n  \"\"\"\n  Convenience function for the EmbeddingClassification model which     \n  \n  Keyword Arguments:\n  x: pandas df\n  y: target column\n  categorical_columns: list[int] a list of the indices of the categorical columns\n  num_classes: int the number of output classes of the target column\n  \"\"\"\n  x_con = x.drop(categorical_columns, axis=1)\n  categorical_dimension = get_categorical_dimensions(x, categorical_columns)\n  return EmbeddingClassification(num_classes, categorical_dimension, len(x_con.columns))\n\nNow we can create the Pytorch model using the function we just defined.\n\nmodel = entity_encoding_classification(x_train, cat_cols, 1)\n\nWe will not write out own training loop, instead we will use the Skorch library. The Skorch library allows us to use the sklearn API with our own PyTorch models. Skorch provides classes, such as NeuralNetBinaryClassifier, with default loss functions (binary cross entropy in this case), train/validation split logic, console logging of loss, validation loss etc. These can be customised, and additional call-backs can be added such as model checkpointing, early stopping, custom scoring metrics and all metrics from sklearn. Other types of model (regression, semi-supervised, reinforcement learning etc.) can be fit using the more generic class NeuralNet.\n\nfrom skorch import NeuralNetBinaryClassifier\n\nnet = NeuralNetBinaryClassifier(\n    module = model,\n    iterator_train__shuffle=True, \n    max_epochs=100,\n    verbose=False\n)\n\nTo pass multiple arguments to the forward method of the Skorch model we must specify a SliceDict such that Skorch can access the data and pass it to the module properly.\n\nfrom skorch.helper import SliceDict\n\nXs = SliceDict(\n    x_cat=preprocessed_df[cat_cols].to_numpy(dtype=\"long\"), \n    x_con=torch.tensor(preprocessed_df[num_cols].to_numpy(), dtype=torch.float)\n)\n\nWe can now use the sklearn fit method with our PyTorch model. This trains the weights of the neural network using back-propagation.\n\nnet.fit(Xs, y=torch.tensor(y_train, dtype=torch.float))\n<class 'skorch.classifier.NeuralNetBinaryClassifier'>[initialized](\n  module_=EmbeddingClassification(\n    (embeddings): ModuleList(\n      (0): Embedding(2, 64)\n      (1): Embedding(7, 64)\n    )\n    (fc1): Linear(in_features=128, out_features=64, bias=True)\n    (fc2): Linear(in_features=2, out_features=64, bias=True)\n    (relu): ReLU()\n    (out): Linear(in_features=128, out_features=1, bias=True)\n  ),\n)\n\nFinally, we pre-process the test data by re-using the pipelines from the training data and we can calculate the test accuracy.\n\nfrom sklearn.metrics import classification_report\n\nx_test_pre = preprocessor.transform(x_test)\npreprocessed_test = pd.DataFrame(x_test_pre, columns=num_cols + cat_cols)\n\nXs_test = SliceDict(\n    x_cat=preprocessed_test[cat_cols].to_numpy(dtype=\"long\"), \n    x_con=torch.tensor(preprocessed_test[num_cols].to_numpy(), dtype=torch.float)\n)\n\nnet.score(Xs_test, y_test)\n0.726457399103139\n\nThis post shows how to implement entity embeddings using Python, and how to incorporate custom PyTorch models into an sklearn pipeline.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-04T13:39:15+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-02-neural-networks-in-r/",
    "title": "Neural Networks in R",
    "description": "This post explores how to create a simple neural network to learn a linear function and a non-linear function using both standard R and the Torch library for R.",
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2021-02-02",
    "categories": [
      "R",
      "Deep Learning"
    ],
    "contents": "\nIn this post we will see how to train a neural network model using R and the Torch R library which is a port of the Python torch library without dependencies on Python. Torch provides tensors (n-dimensional arrays), automatic differentiation of tensors, optimisation routines and additional helpers for common deep learning tasks such as computer vision and audio processing.\nA neural network is built of layers. A layer consists of a set of weights, which are the parameters of the layer and an activation function (this can have additional parameters too). A single layer looks like the linear predictor in a generalised linear model,\n\\[\n\\eta = g(x^Tw),\n\\]\nwhere \\(\\eta\\) is the linear predictor, \\(g\\) is the linking function and \\(w\\) represent the coefficients of the covariates \\(x\\). In machine learning, \\(x\\) is referred to simply as the input, \\(w\\) are the weights and \\(g\\) is the activation function. There is often an additional parameter, termed the intercept in generalised linear models and the bias in machine learning. This can be rolled in to the weight vector by appending a one to the input / covariates. We can encapsulate this logic in a function\n\n\nlayer <- function(input, weight, activation) \n  activation(input %*% weight)\n\n\n\nWe also require a loss function, to understand how well our model is fitting to the data. For a regression problem we can use squared loss.\n\n\nloss <- function(pred, y) {\n  sum((y - pred)^2)\n}\n\n\n\nWe will define a simple linear regression problem, our observations are noisy observations of a straight line.\n\\[y \\sim \\mathcal{N}(x^Tw, 2.0)\\]\n\n\nx <- seq(-10, 10, length.out=100)\ny <- rnorm(100, mean = 2 * x + 3, sd = 2.0)\nqplot(x, y, geom = \"point\")\n\n\n\n\nWe wish to learn the relationship between the inputs \\(x\\) and the outputs \\(y\\) using the model. To understand how well the model fits the observed data, we make a prediction by passing an observed input to the model (which is defined as a single layer), then we calculate how far the prediction is from the observed output using the squared loss function. The activation function is linear, or the identity function (function(x) x).\n\n\nw <- matrix(c(1, 1))\nx_intercept <- cbind(1, x)\nprediction <- layer(x_intercept, w, function(x) x)\nloss(prediction, y)\n\n\n[1] 4476.152\n\nTo find the best fit for this model to the observed data, we need to manipulate the weights to reduce the loss function. We can think of the weights as parameterising a family of related models, some of which may fit the data well.\nOptimisation\nTo find the maximum of a function in calculus, we first calculate the derivative and determine the point at which the slope is equal to zero. This can find both maximums and minimums, so we can additionally calculate the second derivative and if it’s negative then we have a maximum. This is fine for linear optimisation, however when it comes to non-linear optimisation we have to be more creative. We can use gradient descent to take steps in the opposite direction of the gradient to find the minimum of a non-linear function\n\n\ngradient_step <- function(learning_rate, params, grad_params) \n  params - learning_rate * grad_params\n\n\n\nWe must calculate the derivative of the network with respect to the weight parameters. For a single layer network with univariate inputs, a linear activation function and a squared loss function the derivative is\n\\[\\frac{d \\text{ network}}{dw} = \\frac{d}{dw} (y - x^Tw)^2 = -2x^T(y-x^Tw).\\]\nWe can encapsulate this derivative as a function.\n\n\nnetwork_gradient <- function(x, y, w) {\n  -2 * t(x) %*% (y - x %*% w)\n}\n\n\n\nWe can check the analytically calculated gradient using Torch. First we use our calculation of the gradient.\n\n\nx_test <- matrix(rnorm(2))\nw <- matrix(rnorm(2))\ny_test <- matrix(rnorm(1))\nnetwork_gradient(t(x_test), y_test, w)\n\n\n            [,1]\n[1,]  0.31105026\n[2,] -0.07820897\n\nThen we must write the forward function of the model, pred and calculate the loss using the functions available on Torch tensors. We can then call backward() which performs reverse mode automatic differentiation then we can access the grad attribute of any tensor which has requires_grad = TRUE.\n\n\nnetwork_gradient_torch <- function(x, y, w) {\n  pred <- w$t()$mm(x)\n  loss <- (y - pred)$pow(2)$sum()\n  loss$backward()\n  w$grad\n}\n\nnetwork_gradient_torch(x = torch_tensor(as.matrix(x_test)),\n                       y = torch_tensor(as.matrix(y_test)),\n                       w = torch_tensor(as.matrix(w), requires_grad = TRUE))\n\n\ntorch_tensor\n 0.3111\n-0.0782\n[ CPUFloatType{2,1} ]\n\nWe can fit this simple model by writing a training loop which updates the parameters using gradient descent. We keep track of the loss function at each iteration of gradient descent and plot it.\n\n\nobserved <- tibble(x, y) %>%\n  sample_n(50)\n\n\n\n\n\ntraining_loop <- function(x, y, init_w, epochs, learning_rate) {\n  # Record all changes to parameters and training loss\n  ws <- matrix(NA_real_, nrow = epochs + 1, ncol = length(init_w))\n  ws[1,] = init_w\n  losses <- numeric(length(y))\n  \n  for (i in seq_len(epochs)) {\n    # Pad the input with 1 for intercept/bias\n    input <- cbind(rep(1, times = length(x)), x)\n    \n    # Made a prediction using the model\n    pred <- layer(input, ws[i,], function(z) z)\n    \n    # We can calculate and log/print the loss \n    losses[i] <- loss(y, pred)\n    \n    # calculate the gradient at this point\n    gradient <- network_gradient(x = input, y = y, w = ws[i, ])\n    \n    # Update using gradient descent\n    ws[i + 1, ] <- gradient_step(learning_rate, \n                                 ws[i, ], \n                                 gradient)\n  }\n  \n  list(weights = ws, losses = losses)\n}\n\n\n\n\n\n\nSince this model is so small, consisting of only two weights. We can plot the actual function learned by the model using geom_abline.\n\n\n\nWe can try to use this model on a simple non-linear regression problem, of course we probably won’t do very well here! We define the regression problem as\n\\[y \\sim \\mathcal{N}(4\\sin(x), 1^2).\\]\nWe plot the true function and the observed values in red below.\n\n\n\n\n\n\nWe can then plot the learned function against the observed values and the true function. We can see that a straight line is not a good fit for this data, we need more flexibility in the network.\n\n\nqplot(x, y, geom = \"line\", colour = \"truth\") +\n  geom_point(data = non_linear, aes(x, y_obs, colour = \"observed\")) +\n  geom_abline(intercept = out$weights[201,1], slope = out$weights[201,1]) +\n  theme(legend.position = \"none\")\n\n\n\n\nUsing Torch\nIf we want to approximate a non-linear function we best use non-linear activation functions. We can calculate the derivative of each layer using automatic differentiation. We will use the R Torch library. We now initialise a torch tensor with the same values as x and pass it through the layers. We must re-write the layer and loss functions assuming the input is a torch_tensor. First we will re-write the linear example using Torch\n\n\nmodel <- nn_linear(1, 1)\n\nloss <- function(y, pred) {\n  (y - pred)$pow(2)$sum()\n}\n\ntrain_torch <- function(x, y, model, loss, epochs) {\n  alpha <- 1e-4\n  x_in <- torch_tensor(as.matrix(x))\n  for (i in seq_len(epochs)) {\n    pred <- model(x_in)\n    losses <- loss(torch_tensor(y), pred)\n    model$zero_grad()\n    losses$backward()\n    if (i %% 10 == 0)\n      cat(\"Epoch: \", i, \"   Loss: \", losses$item(), \"\\n\")\n    with_no_grad({\n      model$parameters %>% \n        purrr::walk(function(param) param$sub_(alpha * param$grad))\n    })\n  }\n  model\n}\n\ntrained_model <- train_torch(scale(observed$x), scale(observed$y), model, loss, 200)\n\n\nEpoch:  10    Loss:  94.43447 \nEpoch:  20    Loss:  77.64619 \nEpoch:  30    Loss:  63.87258 \nEpoch:  40    Loss:  52.57226 \nEpoch:  50    Loss:  43.30109 \nEpoch:  60    Loss:  35.69468 \nEpoch:  70    Loss:  29.45408 \nEpoch:  80    Loss:  24.33403 \nEpoch:  90    Loss:  20.13331 \nEpoch:  100    Loss:  16.68685 \nEpoch:  110    Loss:  13.8592 \nEpoch:  120    Loss:  11.53925 \nEpoch:  130    Loss:  9.635837 \nEpoch:  140    Loss:  8.074168 \nEpoch:  150    Loss:  6.792887 \nEpoch:  160    Loss:  5.741646 \nEpoch:  170    Loss:  4.879141 \nEpoch:  180    Loss:  4.171486 \nEpoch:  190    Loss:  3.590877 \nEpoch:  200    Loss:  3.114503 \n\n\n\n\nTry the non-linear example with multiple layers and a non-linear activation function from the first layer (where the input goes). We’ll also try a different optimizer, Adam.\n\n\ntrain_torch <- function(x, y, model, loss, epochs, learning_rate) {\n  x_in <- x\n  optimiser <- optim_adam(model$parameters, lr = learning_rate)\n  for (i in seq_len(epochs)) {\n    pred <- model(x_in)\n    losses <- loss(y, pred)\n    model$zero_grad()\n    losses$backward()\n    if (i %% 10 == 0)\n      cat(\"Epoch: \", i, \"   Loss: \", losses$item(), \"\\n\")\n    optimiser$step()\n  }\n  model\n}\n\nmodel <- nn_sequential(\n  nn_linear(1, 64),\n  nn_relu(),\n  nn_linear(64, 1)\n)\n\ntrained_model <-\n  train_torch(torch_tensor(as.matrix(non_linear$x)),\n              torch_tensor(as.matrix(non_linear$y_obs)),\n              model,\n              nnf_mse_loss,\n              100, \n              0.1)\n\n\nEpoch:  10    Loss:  6.005287 \nEpoch:  20    Loss:  2.650542 \nEpoch:  30    Loss:  0.5133222 \nEpoch:  40    Loss:  0.4129792 \nEpoch:  50    Loss:  0.2928792 \nEpoch:  60    Loss:  0.22907 \nEpoch:  70    Loss:  0.2131358 \nEpoch:  80    Loss:  0.1997202 \nEpoch:  90    Loss:  0.1865921 \nEpoch:  100    Loss:  0.1764488 \n\nWe can plot the predictions alongside the observed values and the true function.\n\n\n\n\n\n\n",
    "preview": "posts/2021-02-02-neural-networks-in-r/neural-networks-in-r_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-02-02T11:03:20+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-05-01-hidden-markov-model/",
    "title": "Functional Programming and Hidden Markov Models",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2020-05-01",
    "categories": [
      "Bayesian",
      "R"
    ],
    "contents": "\nThe hidden Markov model is a state-space model with a discrete latent state, \\(x_{1:T}\\) and noisy observations \\(y_{1:T}\\). The model can be described mathematically as\n\\[p(y_{1:T}, x_{1:T}) = p(x_1)p(y_1|x_1)\\prod_{t=2}^Tp(y_t|x_t)p(x_t|x_{t-1})\\]\nWhere \\(y_{1:T} = y_1, \\dots, y_T\\) represents the sequence of observed values and \\(x_{1:T} = x_1, \\dots, x_T\\) is the sequence of latent, unobserved values. The state space is assumed to be finite and countable, \\(X \\in \\{1,\\dots,K\\}\\) and the time gaps between each observation are constant. The observation distribution can be either continuous or discrete.\nThe model can be visualised using a state-transition diagram where observed nodes are rectangular and latent nodes are circular. The arrows represent any transitions which can be made and also convey conditional independence assumptions in the Model. The state forms a first order Markov process, which means \\(p(x_t|x_{t-1},\\dots,x_1) = p(x_t|x_{t-1})\\) and each observation is conditionally independent of all others given the corresponding value of the latent state at that time, \\(y_t \\perp\\!\\!\\!\\perp y_{1:t-1},y_{t+1:T}\\)\n\n\n\n\nExample: The occasionally dishonest casino\nThe casino can choose to use a fair dice, in which case the observation distribution is categorical with probabilities \\(p = \\{\\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}\\}\\). The casino can also choose a loaded dice which has the following probabilities, \\(p = \\{\\frac{1}{10}, \\frac{1}{10}, \\frac{1}{10}, \\frac{1}{10}, \\frac{1}{10}, \\frac{1}{5}\\}\\), hence it is more likely to roll a six with the loaded dice.\nWe want to infer when the casino is using the loaded dice, the latent state \\(x \\in \\{L, F\\}\\) for loaded and fair respectively. We assume we already know the transition matrix\n\\[P = \\begin{pmatrix}\n\\alpha & 1 - \\alpha \\\\\n1 - \\beta & \\beta\n\\end{pmatrix}.\\]\n\n\n\nThe observation distribution is \\(p(y_t|x_t = j)\\). This implies we have a two element vector since the state can take one of two values \\(j = \\{L, F\\}\\).\n\n\n\n\n\n\nWe can simulate data from this process by specifying values of the parameters, \\(\\alpha = 0.3\\) and \\(\\beta = 0.1\\). This means if the casino is using the loaded dice we will transition to the fair dice with probability \\(1 - \\alpha\\) and if the casino is using the fair dice there is a \\(1 - \\beta\\) probability of transitioning to the loaded dice. The algorithm to simulate from this Hidden Markov model is\nSpecify the initial state of the dice, \\(x_1 = L\\)\nSimulate an initial observation conditional on the dice used, \\(y_1 \\sim p(y_1|x_1)\\)\nSimulate the next transition by drawing from a categorical distribution with probabilities corresponding to the row of the transition matrix corresponding the current state, \\(P_{x_t, \\cdot}\\)\nSimulate an observation conditional on the state, \\(y_t \\sim p(y_t|x_t)\\)\nRepeat 3 - 4 until the desired number of realisations are simulated\n\n\n\nThe plot below shows 300 simulations from the occasionally dishonest casino.\n\n\n\nFiltering\nNow we wish to identify when the casino is using the loaded dice. We can use the forward filtering algorithm. The first step of the forward algorithm is to the prediction step:\n\\[p(x_t = k|y_{1:t-1}) = \\sum_{j=1}^K p(x_t = k|x_{t-1}=j)p(x_{t-1}=k|y_{1:t-1})\\]\nThen we observe a new value of the process \\(y_t\\), and perform the update step where we apply Bayes’ Theorem\n\\[\\begin{aligned}\np(x_t = k \\mid y_{1:t}) = p(x_t = k\\mid y_t,y_{1:t-1}) &= \\frac{p(y_{t}|x_t = k, y_{1:t-1})p(x_t = k|y_{1:t-1})}{p(y_{t})} \\\\\n&= \\frac{p(y_{t}|x_t = k)p(x_t = k|y_{1:t-1})}{\\sum_{j=1}^Kp(y_{t}|x_t = j)p(x_t = j|y_{1:t-1})}\n\\end{aligned}\\]\nWhich can be calculated recursively by defining \\(\\alpha_t(k) = p(x_t = k \\mid y_{1:t})\\) then we have the recursive update\n\\[\\begin{aligned}\n\\alpha_t(k) &= p(y_{t}|x_t = k)p(x_t = k|y_{1:t-1}) \\\\\n&= p(y_{t}|x_t = k)\\sum_{j=1}^Kp(x_t = k|x_{t-1} = j)p(x_{t-1}\\mid y_{1:t-1})\\\\\n&= p(y_{t}|x_t = k)\\sum_{j=1}^Kp(x_t = k|x_{t-1} = j)\\alpha_{t-1}(k)\n\\end{aligned}\\]\nEssentially we have use the posterior of the previous time point, \\(\\alpha_{t-1}(k)\\) as the prior for next observation. Then we advance the latent-state using the transition distribution \\(p(x_t = k|x_{t-1} = j)\\) and calculate the likelihood of the observation at time \\(t\\) using the observation distribution \\(p(y_{t}|x_t = k)\\).\nTo implement the forward algorithm in R we can use a higher-order function, a fold, from the R package purrr. A higher-order function is a function which accepts a function as an argument or returns a function instead of a value such as a double or int. This might seem strange at first, but it is very useful and quite common in statistics. Consider maximising a function using an optimisation routine, we pass in a function and the initial arguments and the optimisation function and the function is evaluated at many different values until a plausible maximum is found. This is the basis of the optim function in R.\nHigher-order functions can be motivated by considering a foundational principle of functional programming, to write pure functions which do not mutate state. A pure function is one which returns the same output value for the same function arguments. This means we can’t mutate state by sampling random numbers, write to disk or a database etc. Advanced functional programming languages, such as Haskell, encapsulates this behaviour in Monads. However, Monads and other higher-kinded types are not present in R. While we can’t use all the useful elements of functional programming in R, we can use some, such as higher-order functions.\nOne result of avoiding mutable state is that we can’t write a for-loop, since a for-loop has a counter which is mutated at each iteration (i = i + 1). To overcome this apparent obstacle we can use recursion. Consider the simple example of adding together all elements in a vector, if we are naive we can write a for-loop.\n\n\nseq <- 1:10\ntotal <- 0\nfor (i in seq_along(seq)) {\n  total = total + seq[i]\n}\ntotal\n\n\n[1] 55\n\nThis implementation has two variables which are mutated to calculate the final results. To avoid mutating state, we can write a recursive function which calls itself.\n\n\nloop <- function(total, seq) {\n  if (length(seq) == 0) {\n    total\n  } else {\n    loop(total + seq[1], seq[-1])\n  }\n}\n\nloop(0, 1:10)\n\n\n[1] 55\n\nR does not have tail-call elimination and hence this recursive function will not work with long sequences, however it does not mutate any state. We can generalise this function to be a higher-order function.\n\n\nfold <- function(init, seq, f) {\n  if (length(seq) == 0) {\n    init\n  } else {\n    fold(f(init, seq[1]), seq[-1], f)\n  }\n}\n\n\n\nHere the function fold applies the user-specified binary function f to the initial value init and the first element of the sequence. The result of applying f to these values is then used as the next initial value with the rest of the sequence. We can use this to calculate any binary reduction we can think of.\n\n\nfold(1, seq, function(x, y) x * y)\n\n\n[1] 3628800\n\nThis example is equivalent to the reduce function in purrr. purrr::reduce by default can be used to combine the elements of a vector or list using a binary function starting with the first element in the list. For instance we can calculate the sum of a vector of numbers\n\n\npurrr::reduce(1:10, function(x, y) x + y)\n\n\n[1] 55\n\nWe can also use the function shorthand provided in purrr, where function(x, y) x + y can be written ~ .x + .y.\nOther arguments provided to the reduce function can change its behaviour such as reversing the direction by changing the .direction argument (which will not affect the above computation, since addition is associative, ie. \\((1 + (2 + 3)) = ((1 + 2) + 3)\\)). We can also provide an initial value (.init) to the computation, instead of starting with the first (or last) element of the list.\npurrr::accumulate is similar to reduce, however it does not discard intermediate computations.\n\n\npurrr::accumulate(1:10, `+`)\n\n\n [1]  1  3  6 10 15 21 28 36 45 55\n\nHence, if we change the direction this will change the output.\n\n\npurrr::accumulate(1:10, `+`, .dir = \"backward\")\n\n\n [1] 55 54 52 49 45 40 34 27 19 10\n\nThese functions can appear strange at first, however they don’t suffer from common problems such as off-by-one errors when writing a for-loop with indexing.\nThe accumulate function can be used to write the forward algorithm by first writing a single step in the forward algorithm. The function forward_step accepts the current smoothed state at time t-1, alpha, and the observed value at time t, y. The arguments observation and P represent the observation distribution and the transition matrix respectively and remain constant in this example\n\n\nforward_step <- function(alpha, y, observation, P) {\n  normalise(observation(y) * t(P) %*% alpha)\n}\n\n\n\nThe forward algorithm can then be written using the accumulate function by first calculating the initial value of alpha and using this as the value .init then the function forward_step is used with the values of observation and P set. accumulate then takes uses the initial value, .init and the first value of the observations, ys (technically the second since we use the first to initialise alpha) to produce the next alpha value. This new alpha value is passed to the next invocation of forward_step along with the next observed value and so on until the observation vector is exhausted.\n\n\nforward <- function(ys, x0, observation, P) {\n  alpha <- normalise(observation(ys[1]) * x0)\n  purrr::accumulate(\n    ys[-1],\n    forward_step,\n    observation = observation,\n    P = P,\n    .init = alpha\n  )\n}\n\n\n\nWe assume that the dice used for the initial roll can be either loaded or fair with equal probability.\n\n\n\n\n\n\n\n\n\nParameter inference\nWe can calculate the log-probability of the evidence using the forward algorithm, this is the sum of un-normalised filtering distribution\n\\[\\log p(y_{1:T}) = \\log \\sum_{i=1}^T\\sum_{j=1}^K p(x_t=j\\mid y_{1:t-1})p(y_t|x_t = j)\\]\nThis can be used in a Metropolis-Hastings algorithm to determine the posterior distribution of the parameters in the transition matrix, \\(\\alpha\\) and \\(\\beta\\). We can keep a running total of log-likelihood by returning a list from the forward step containing the log-likelihood and the posterior probability of the states given the observation.\n\n\nll_step <- function(state, y, observation, P) {\n  unnorm_state <- observation(y) * t(P) %*% state[[2]]\n  list(\n    state[[1]] + sum(log(unnorm_state)),\n    normalise(unnorm_state)\n  )\n}\n\n\n\nTo return only the log-likelihood we can use purrr::reduce.\n\n\nlog_likelihood <- function(ys, x0, observation, P) {\n  alpha <- normalise(observation(ys[1]) * x0)\n  init <- list(0, alpha)\n  purrr::reduce(ys, function(x, y) ll_step(x, y, observation, P), .init = init)[[1]]\n}\n\n\n\nWe can use this marginal-likelihood in a Metropolis-Hastings algorithm. We define the prior on the parameters of the transition matrix to be independent Gamma distributions with shape, \\(\\alpha = 3\\), and rate \\(\\beta = 3/0.1\\). The log-posterior is the sum of the log-likelihood calculated using the forward filtering algorithm and the log-prior.\n\n\n\nThe proposal distribution is a normal centered at the un-constrained value of the parameter. We use the logit function to transform \\(\\alpha\\) and \\(\\beta\\) from \\(\\operatorname{logit}:[0, 1] \\rightarrow \\mathbb{R}\\) then propose using a Normal distribution centered as the un-constrained value and proceed to transform the parameter back to the original scale using the logistic function, \\(\\operatorname{logistic}:\\mathbb{R} \\rightarrow [0, 1]\\).\n\n\n\n\n\n\nWe draw 10,000 iterations from the Metropolis algorithm, the parameter diagnostics are plotted below.\n\n\n\n\n\n\n",
    "preview": "posts/2020-05-01-hidden-markov-model/hidden-markov-model_files/figure-html5/hmm-1.png",
    "last_modified": "2021-02-02T11:07:11+00:00",
    "input_file": {},
    "preview_width": 831,
    "preview_height": 303
  },
  {
    "path": "posts/2020-04-19-multi-state-survival-models/",
    "title": "Multi State Models",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2020-04-19",
    "categories": [
      "R",
      "Bayesian"
    ],
    "contents": "\nMulti-state models are used to model disease progression. The model is a continuous time Markov process. The states and time of transitions are fully observed. There are three states a patient can be in, “healthy”, “illness” and “deceased”. The possible pairs of transitions between these states include healthy -> illness, illness -> healthy, illness -> death and healthy -> death. The model can be expressed as a directed graph.\n\n\n\nThe state “Deceased” is an absorbing state, whereas the other two states are transient. This means we can’t transition away from the Deceased state.\nThe process is a Markov process, it has a transition kernel, \\(p(x_{i,t_j}|x_{i,t_{j-1}}, \\theta)\\) which depends only on the previous state and some static parameters \\(\\theta\\). The state for the \\(i^{\\text{th}}\\) patient at time \\(t_j\\) is written \\(x_{i,t_j}\\). The parameters for the transition kernel populate the transition rate matrix. The rate matrix is the derivative of the transition kernel at \\(t=0\\)\n\\[\\begin{aligned}\nQ &= \\frac{d}{dt}P(t)\\Bigr|_{\\substack{t=0}} \\\\\n&= \\lim_{\\delta t \\rightarrow 0}\\frac{P(\\delta t) - P(0)}{\\delta t} \\\\\n&= \\lim_{\\delta t \\rightarrow 0}\\frac{P(\\delta t) - I}{\\delta t}\n\\end{aligned}\\]\nThen we can rearrange for the transition kernel,\n\\[P(\\delta t) = I + Q \\delta t.\\]\nThis gives the infinitesimal transition for a very small time increment. Using this result, we can solve for the transition kernel over a finite time\n\\[\\begin{aligned}\n\\frac{d}{dt}P(t) &= \\frac{P(t + dt) - P(t)}{dt} \\\\\n&= \\frac{P(dt)P(t) - P(t)}{dt}\\\\\n&= \\frac{(P(dt) - I)}{dt}P(t)\\\\\n&= QP(t).\n\\end{aligned}\\]\nThen solve the resulting differential equation to see that \\(P(t) = \\exp(Qt)\\), hence the transition matrix is the matrix exponential of the transition rate matrix. The transition rate matrix for this problem can be written as\n\\[Q = \\begin{pmatrix} \n-\\sum_{j\\neq 1}q_{1j} & q_{12} & q_{13} \\\\\nq_{21} & -\\sum_{j\\neq 2}q_{2j} & q_{13} \\\\\n0 & 0 & 0 \n\\end{pmatrix}\\]\nThe non-zero elements of the rate-matrix are potential transitions. The final state is absorbing - hence we can’t transition from it.\nSimulating Data from the Model\nWe can simulate forward using a grid of times, \\(t_i = t_0, \\dots, t_n\\) where \\(t_i - t_{i-1} = \\Delta t\\) is a small time increment, using the exact solution for the transition rate matrix \\(P(\\Delta t) = \\exp(Q \\Delta t)\\).\nSample the rate parameters from the prior distribution\nConstruct a rate matrix from each sample\nCompute the matrix exponential and simulate forward conditional on the previous step\nForward simulation is drawing next state, \\(x_t\\) from a categorical distribution with probabilities from the row of the transition matrix corresponding to the state at time \\(t-1\\), \\(P(\\Delta t)_{x_{t-1}\\cdot}\\).\nWe use Gamma priors for the non-zero elements of the rate matrix:\n\\[\\begin{aligned}\nq_{12} &\\sim \\textrm{Gamma}(3, 3/0.05)\\\\\nq_{13} &\\sim \\textrm{Gamma}(3, 3/0.001)\\\\\nq_{21} &\\sim \\textrm{Gamma}(3, 3/0.02)\\\\\nq_{23} &\\sim \\textrm{Gamma}(3, 3/0.01)\n\\end{aligned}\\].\n\n\n\n\n\n\nThe following function populates a rate matrix given a vector of hazards.\n\n\nbuild_rate_matrix <- function(lambda, n_states = 3, n_absorbing = 1) {\n  q <- matrix(rep(0, times = n_states * n_states), byrow = TRUE, nrow = n_states)\n  k <- 1\n  for (i in 1:n_states) {\n    for (j in 1:n_states) {\n      if (i != j & i <= (n_states - n_absorbing)) {\n        q[i, j] = lambda[k]\n        k = k + 1\n      }\n    }\n  }\n  \n  ## fix the diagonal to be negative the sum of the remaining elements in the row\n  diag(q) = -rowSums(q)\n  q\n}\n\n\n\nThe next function can be used to simulate the process on a grid of times \\(t_1, \\dots, t_n\\). The step-size between each realisation must be small enough to see any transitions.\n\n\n# Simulate the markov chain\nsim_markov <- function(lambdas, step_size, n_steps) {\n  state <- numeric(n_steps)\n  \n  ## Build the rate and transition matrices\n  rate_matrix <- build_rate_matrix(lambdas)\n  transition_matrix <- Matrix::expm(rate_matrix * step_size)\n  n_states <- nrow(rate_matrix)\n  \n  # initial state is always healthy\n  state[1] <- 1\n\n  for (i in 2:n_steps) {\n    state[i] <- sample(x = n_states, size = 1, prob = transition_matrix[state[i-1], ])\n  }\n  tibble(time = seq_len(n_steps), state = state)\n}\n\n\n\n\n\n\nSimulating 1,000 trajectories for (50 * 0.1 = 5 days) transitions results in the following state transitions\n\n# A tibble: 3 x 4\n   from   `1`   `2`   `3`\n  <dbl> <dbl> <dbl> <dbl>\n1     1 43000   227     1\n2     2    11  5700     2\n3     3     0     0    59\n\nInstead of simulating on a fine grid we can using discrete event simulation:\nGiven the initial state is \\(i\\)\nSimulate the time to the next event, \\(t \\sim \\text{Exp}(-q_{ii})\\)\nSimulate the next state by simulating from a distribution with pmf \\(q_{ij} / q_{ii}, i \\neq j\\)\nReturn the sample path if we have landed in an absorbing state, \\(q_{ii} = 0\\)\n\n\nsim_exact <- function(x, Q, n) {\n  xs = numeric(n + 1)\n  ts = numeric(n)\n  r = nrow(Q)\n  t = 0\n  ts[1] <- t\n  xs[1] = x\n  for (i in seq_len(n)) {\n    t <- t + rexp(1, -Q[x, x]) # Sim time to next observation\n    weights <- Q[x, ] # Calculate the probability of transitioning away to another state\n    weights[x] <- 0 # We can't stay in the same state\n    x <- sample(r, 1, prob = weights) # Sample the next state\n    xs[i + 1] <- x # add to vector of states\n    ts[i + 1] <- t # add to vector of event times\n    if (Q[x, x] == 0) { # If the new state is an absorbing state then return event times\n      return(tibble(time = ts[seq_len(i + 1)], state = xs[seq_len(i + 1)]))\n    }\n  }\n  tibble(time = ts, state = xs) # Return event times without absorbing\n}\n\n\n\n\n        1    2   3\n[1,]    0 3019  80\n[2,] 2150    0 869\n\nThe figure below shows the mean time in each state with 66% and 95% credible intervals. This is calculated by sampling 1,000 rate matrices from the prior distribution and calculating \\(1 / q_i = \\sum_{j\\neq i}q_{ij}\\) using each sample.\n\n\n\nSimulating a real-world example\nIn a real world example patients are observed at a different number of times, \\(n_i\\) represents the number of state observations for patient \\(i\\). To simulate realistic data we will randomise the number of steps recorded for each “patient” using the exact algorithm. Let’s sample the number of steps uniformly between 1 and 5.\n\n\n\nThe plot below shows patient journeys for patients 3, 11, 13 and 100. Patient 3 has only two observed state changes and censoring on the final state. Patient 11 has multiple observed transitions through illness and healthy, then a final transition to the absorbing state.\n\n\n\nWe can write down the likelihood for the \\(i^{\\text{th}}\\) patient with state transitions at times, \\(t_{ij}, j = 0,\\dots,n_i\\)\n\\[\\begin{aligned}\np(X_{i,t_{0:n_i}}|Q) &= p(x_{i,0})\\prod_{j=1}^{n_i}p(x_{i,t_j}|x_{i,t_{j-1}}, Q) \\\\\n&= \\prod_{j=1}^{n_i}\\exp(Q(t_{ij} - t_{ij-1}))_{x_{i,t_j},x_{i,t_{j-1}}}\n\\end{aligned}\\]\nThe initial state is fixed at one and the transition kernel is given by the matrix exponential of the rate matrix. There is one problem with this likelihood: It does not incorporate censored paths.\nA multi-state model such as this can be thought of as multiple survival models. I introduced Survival models in a previous blog post. In a survival model, the transition is governed by a hazard function. The hazard function for transitioning to state \\(j\\) from state \\(i\\) is\n\\[H_{ij}(t_1, t_2) = \\int_{u=t_1}^{t_2} q_{ij}\\, du = (t_2-t_1)q_{ij}.\\]\nThe Survival function, \\(Pr(T > t)\\) is:\n\\[S_{ij}(t) = \\exp(-H_{ij}(0, t)) = \\exp(-tq_{ij})\\]\nThe pdf is \\(f_{ij}(t) = \\frac{d}{dt}F_{ij}(t)\\) where \\(F_{ij}(t) = 1 - S_{ij}(t)\\), hence \\(f_{ij} = q_{ij}\\exp(-tq_{ij})\\).\nLet’s consider the likelihood for patient 13.\n\npatient\ntime\nstate\n13\n0.00\n1\n13\n3.93\n2\n13\n8.20\n1\n\nFor each state transition all other possible transitions are considered censored. The data-frame for patient 13 can be re-written by first considering a data-frame of all possible transitions and determining which transitions end in an absorbing state.\n\n\n\nWe can then create a “from” column representing the previous state of a transition, then left join the possible states using “from”. Observed states are when the “to” in both tables match, censored states are non-matching.\n\npatient\ntime\nto.x\nfrom\nto.y\nabsorbing\nobserved\n13\n0.00\n1\nNA\nNA\nNA\nNA\n13\n3.93\n2\n1\n2\nFALSE\nTRUE\n13\n3.93\n2\n1\n3\nTRUE\nFALSE\n13\n8.20\n1\n2\n1\nFALSE\nTRUE\n13\n8.20\n1\n2\n3\nTRUE\nFALSE\n\nThis layout explicitly allows us to see which state transitions are observed and which are censored. Patient 13 starts in state 1, as do all patients. The first observation at time \\(t = 3.92\\) is a transition from \\(1 \\rightarrow 2\\), illness - this observation implies a censored observation for the transition \\(1 \\rightarrow 3\\). Then we transition from state 2 back to state 1 and never observe the terminal state.\n\\[\\begin{aligned}\np(X_{i=13,t_{0:3}}|Q) &= Pr(X_{t_1} = 2|X_{t_0} = 1)Pr(X_{t>t_1} = 3)Pr(X_{t_2} = 1|X_{t_1} = 2)Pr(X_{t>t_2} = 3) \\\\\n&=f_{12}(t_1-t_0)S_{13}(t_1-t_0)f_{21}(t_2-t_1)S_{23}(t_2-t_1) \\\\\n&=q_{12}\\exp(-q_{12}(t_1-t_0))\\exp(-(t_1-t_0)q_{13})q_{21}\\exp(-q_{21}(t_2-t_1))\\exp(-(t_2-t_1)q_{23}) \\\\\n&= q_{12}q_{21}\\exp(-q_{12}(t_1-t_0) -(t_1-t_0)q_{13} -q_{21}(t_2-t_1) -(t_2-t_1)q_{23}) \\\\\n&= q_{12}q_{21}\\exp(-(q_{12} + q_{13})(t_1-t_0) - (q_{21} + q_{23})(t_2-t_1))\n\\end{aligned}\\]\nThe final line shows that the likelihood consists of the hazards of the observed transitions \\(1 \\rightarrow 2\\) and \\(2 \\rightarrow 1\\), multiplied by the exponential of the sum of the off-diagonal elements of the rate matrix multiplied by the total time those states have been observed.\nThis can be generalised:\n\\[p(X_{i,t_{0:3}}|Q) = \\prod_{j=1}^3\\prod_{k=1,k\\neq j}^3q_{jk}^{n_{ijk}}\\exp(-\\tau_{ij}q_j)\\]\nWhere \\(q_j = \\sum_{j=1,j\\neq k}^3q_{jk}\\), \\(\\tau_{ij}\\) is the time in the \\(j^{\\text{th}}\\) state for patient \\(i\\) and \\(n_{ijk}\\) is a matrix with a count of the number of transitions from state \\(j\\) to state \\(k\\). We can then calculate the likelihood for all patients:\n\\[\\begin{aligned}\np(\\textbf{X}_t|Q) &= \\prod_{i=1}^N\\prod_{j=1}^3\\prod_{k=1,k\\neq j}^3q_{jk}^{n_{ijk}}\\exp(-\\tau_{ij}q_j)\\\\\n&= \\prod_{j=1}^3\\prod_{k=1,k\\neq j}^3q_{jk}^{n_{1jk} + \\dots + n_{Njk}}\\exp(-\\sum_{i=1}^N\\tau_{ij}q_j)\\\\\n&= \\prod_{j=1}^3\\prod_{k=1,k\\neq j}^3q_{jk}^{n_{jk}}\\exp(-\\tau_{j}q_j)\\\\\n\\end{aligned}\\]\nWhere, \\(n_{jk}\\) is the sum of all \\(N\\) matrices representing the transitions \\(j \\rightarrow k\\) and \\(\\tau_j\\) is the total time spent in state \\(j\\) for all patients.\nFor reasons of numerical stability, the log-likelihood is used,\n\\[\\begin{aligned}\n\\log p(\\textbf{X}_t|Q) &= \\sum_{j=1}^3\\sum_{k=1,k\\neq j}^3\\left(n_{jk}\\log(q_{jk})-\\tau_jq_j\\right).\n\\end{aligned}\\]\nParameter Inference\nThe posterior distribution of the free parameters in the rate matrix is determined using a random walk Metropolis algorithm. The prior distribution for the rate parameters are independent Gamma distributions parameterised with shape, \\(\\alpha\\) and rate, \\(\\beta\\) such that the mean is \\(\\alpha/\\beta\\)\n\\[\\lambda_i \\sim \\text{Gamma}\\left(3, \\frac{3}{0.01}\\right), \\quad i = 1,\\dots,12.\\]\nThe rate parameters are transformed to the log scale and the proposal distribution is a symmetric Normal distribution centered at the value of the previously accepted parameter at iteration \\(k-1\\)\n\\[\\log\\lambda_k \\sim \\text{MVN}(\\log\\lambda_{k-1}, I_{4}\\delta), \\quad \\delta = 0.01.\\]\nwhere \\(\\delta\\) is a tuning parameter of the MCMC and \\(I_{4}\\) represents the 4 dimensional identity matrix.\nCalculate the total-time in each state.\n\nstate\ntotal_time\n1\n2284.0808\n2\n189.7724\n3\n0.0000\n\nCalculate the matrix of observed transitions. Note that no transitions occur from state 3, the absorbing state.\n\n      1   2  3\n[1,]  0 107 21\n[2,] 38   0 43\n[3,]  0   0  0\n\nProgram the log-likelihood, prior and proposal distribution.\n\n\n## rate_matrix is the constructed rate-matrix\n## tau is a vector containing total time in each state\n## observed_transitions is a matrix with all observed transitions\n## with the state from in the rows and state to in the columns.\nlog_likelihood <- function(rate_matrix, tau, observed_transitions) {\n  n_states <- nrow(rate_matrix)\n  qi <- - diag(rate_matrix)\n  sum(log(rate_matrix - diag(diag(rate_matrix))) * observed_transitions, na.rm = TRUE) - (n_states - 1) * sum(qi * tau)\n}\n\nlog_prior <- function(lambda) {\n  sum(dgamma(lambda, shape = 3, scale = 3 / 0.1, log = TRUE))\n}\n\nproposal <- function(lambda) {\n  lambda * exp(rnorm(4, sd = 0.01))\n}\n\nlog_posterior <- function(lambda) {\n    Q <- build_rate_matrix(lambda)\n    log_likelihood(Q, tau, observed_transitions) + log_prior(lambda)\n  }\n\n\n\nWe are ready to sample from the posterior distribution of the rate parameters using the Metropolis algorithm. We run four chains in parallel using furrr as explained in my previous post on efficient MCMC in R.\n\n\niters <- jonnylaw::metropolis(\n  lambda,\n  log_posterior,\n  proposal,\n  m = 1e5, \n  chains = 4, \n  parallel = TRUE\n)\n\n\n\nThe posterior diagnostics are plotted below.\n\n\n\nThe plot below shows posterior distribution of the mean time in each state, \\(1 / q_i\\). The blue points are the actual observed values of each patient.\n\n\n\nMost applications of multi-state survival models have patient attributes associated with each patient. This can be used to inform the next transition and the time to the next transition. I will consider this in a future post.\nIn addition, fully observed processes are rare. If the exact time of a state transition is not known or multiple state transitions can happen between observations then the multi-state survival model is partially observed. A continuous time Hidden Markov Model can be used in this case.\n\n\n\n",
    "preview": "posts/2020-04-19-multi-state-survival-models/multi-state-survival-models_files/figure-html5/multi-state-1.png",
    "last_modified": "2021-04-06T09:52:24+01:00",
    "input_file": {},
    "preview_width": 769,
    "preview_height": 216
  },
  {
    "path": "posts/2020-03-17-tidy-tuesday-the-office/",
    "title": "Tidy Tuesday: The Office",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2020-03-17",
    "categories": [
      "tidy-tuesday",
      "R",
      "Bayesian"
    ],
    "contents": "\nFirst we download the ratings for each office episode using the tidytuesdayR package.\n\n\noffice <- tidytuesdayR::tt_load(x = 2020, 12)\n\n\n\n    Downloading file 1 of 1: `office_ratings.csv`\n\nepisode_ratings <- office$office_ratings\n\n\n\nWe can use glimpse from the tibble package to see the column types and some example data from the head of the table.\n\n\nglimpse(episode_ratings)\n\n\nRows: 188\nColumns: 6\n$ season      <dbl> 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ episode     <dbl> 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,…\n$ title       <chr> \"Pilot\", \"Diversity Day\", \"Health Care\", \"The Al…\n$ imdb_rating <dbl> 7.6, 8.3, 7.9, 8.1, 8.4, 7.8, 8.7, 8.2, 8.4, 8.4…\n$ total_votes <dbl> 3706, 3566, 2983, 2886, 3179, 2852, 3213, 2736, …\n$ air_date    <date> 2005-03-24, 2005-03-29, 2005-04-05, 2005-04-12,…\n\nFirstly, I will plot the distribution of ratings by season. We can see that the ratings dropped after season 8 when Steve Carell left the show.\n\n\nepisode_ratings %>%\n  group_by(season) %>%\n  tidybayes::median_qi(imdb_rating, .width = c(0.6, 0.9)) %>%\n  ggplot(aes(x = imdb_rating, xmin = .lower, xmax = .upper, y = as.factor(season))) +\n  tidybayes::geom_pointinterval() +\n  labs(\n    title = \"Office (US) IMDB Ratings by Season\",\n    subtitle = \"The point interval shows the median, 60% and 90% intervals.\",\n    y = \"Season\",\n    x = \"IMDB Rating\"\n  )\n\n\n\n\nUsing the schrute package we can get every line of dialogue and additional episode information including the writers and the directors.\n\n\nscript <- schrute::theoffice\n\n\n\nNext, I wanted to see the number of episodes per writer.\n\n\nwriters <- script %>% \n  mutate_at(vars(season, episode), as.numeric) %>% \n  inner_join(episode_ratings, by = c(\"season\", \"episode\", \"imdb_rating\")) %>% \n  separate(writer, into = paste0(\"writer\", 1:3), sep = \";\") %>% \n  select(season, episode, episode_name, imdb_rating, contains(\"writer\")) %>% \n  distinct() %>% \n  pivot_longer(contains(\"writer\"), names_to = \"name\", values_to = \"writer\") %>% \n  drop_na() %>% \n  mutate(true = TRUE) %>% \n  pivot_wider(names_from = writer, values_from = true) %>% \n  mutate_if(is.logical, ~ !is.na(.)) %>% \n  group_by(season, episode, episode_name, imdb_rating) %>% \n  summarise_if(is.logical, sum)\n\nepisodes_per_writer <- writers %>%\n  ungroup() %>%\n  select(-c(1:4)) %>%\n  summarise_all(sum) %>%\n  gather(key = writer, value = total_episodes)\n\nepisodes_per_writer %>% \n  mutate(writer = forcats::fct_reorder(writer, total_episodes)) %>%\n  ggplot(aes(x = total_episodes, y = writer)) +\n  geom_col() +\n  labs(title = \"Number of episodes by writer\", \n       y = \"Writer\", x = \"Total Episodes\")\n\n\n\n\nNext, I would like to see which writers produce the highest rated episodes. Determining the writer who writes the best episodes is challenging since writers often collaborate and some writers have only written a handful of episodes. Initially I plotted the IMDB rating for each episode written by each writer regardless of who they collaborated with.\nSteve Carell appears to have the highest median episode rating, however he has written only two episodes! It looks like Greg Daniels is the real MVP, with many consistently well received episodes.\n\n\nwriters_by_episode <- writers %>% \n  ungroup() %>% \n  pivot_longer(-c(1:4), names_to = \"writer\", values_to = \"written\") %>% \n  filter(written == 1) \n\nwriters_by_episode %>% \n  mutate(writer = forcats::fct_reorder(writer, imdb_rating)) %>%\n  ggplot(aes(x = imdb_rating, y = writer)) +\n  geom_point()\n\n\n\n# ggsave(\"ratings_by_writer.png\", width = 5, height = 8)\n\n\n\nA Model for Writers\nWe could fit a hierarchical model to the rating by writer using the BRMS package which uses Stan to perform full Bayesian inference for hierarchical distributional models. The hierarchical model allows the ratings to be shared across writers, hence writers with a small number of episode credits will be pulled towards the overall mean rating of all episodes. The model specification can be written as\n\\[\\begin{aligned}\n\\mathbf{Y} &\\sim \\textrm{Beta}(\\mu\\phi, (1 - \\mu)\\phi), \\\\\n\\mu &= \\mathbf{X}\\beta + \\mathbf{Z}\\nu, \\\\\np(\\beta) &= \\mathcal{N}(0, 5^2) \\\\\np(\\nu) &= \\mathcal{N}(0, \\sigma^2) \\\\\np(\\sigma) &= \\frac{1}{2}t(3, 0, 10) \\\\\np(\\phi) &= \\textrm{Gamma}(0.01, 0.01).\n\\end{aligned}\\]\nWhere \\(Y\\) represents the scaled IMDB rating (\\(Y = \\textrm{IMDB Rating} / 10\\)). The parameters we wish to estimate include \\(\\beta\\), which is the latent population-level effect and \\(\\nu\\) which is the latent group-level effect. The Beta distribution has support in \\([0, 1]\\) so the ratings are scaled by dividing by ten. We can recover the true scale by multiplying by 10 when considering the posterior fitted values. The response Beta distribution is parameterised such that the mean is \\(\\mu\\) and the variance is \\(\\operatorname{Var}(Y) = \\mu(1-\\mu)/(1+\\phi)\\) so the variance of the response decreases as \\(\\phi\\) increases, \\(\\phi\\) is known as a precision parameter for the Beta distribution. The design matrix \\(X\\) contains the intercept representing the overall mean IMDB rating. The design matrix \\(Z\\) is a matrix containing ones.\nThere are 40 group level effects, drawn from a Normal distribution with standard deviation \\(\\sigma\\). \\(\\sigma\\) has a half student-t prior which controls the regularisation of the group level effects. Effectively the level of regularisation for the group level effects is learned from the data. The Stan wiki provides a good reference to the literature on prior choices.\nFirst rescale the rating to be between zero and one.\n\n\nmodel_data <- writers_by_episode %>% \n  mutate(rating = imdb_rating / 10)\n\n\n\nTo define and fit the model use the brm function. The formula rating ~ (1 | writer) specified that we want a group-level effect for each writer. The population level intercept is included by default. The response family is specified to be the Beta distribution and the prior distribution for the population intercept is specified as a Normal distribution with mean 0 and standard deviation 5. The other priors are default and specified above.\n\n\nfit <- brm(rating ~ 1 + (1 | writer), family = Beta, data = model_data, prior = set_prior(\"normal(0, 5)\", class = \"Intercept\"))\n\n\n\nThe model is fit using Hamiltonian Monte Carlo, the sampling code is written in C++. If you are familiar with Stan, you can extract the Stan code from the model using stancode(fit).\nNext we plot the posterior fitted values using the tidybayes package and overlay the actual episode ratings using points. Those with fewer writing credits have a larger posterior credible interval.\n\n\nadd_fitted_draws(newdata = model_data, model = fit) %>%\n  mutate(fitted_rating = .value * 10) %>%\n  median_qi(fitted_rating, .width = c(.95, .8, .5)) %>%\n  ungroup() %>%\n  mutate(writer = forcats::fct_reorder(writer, fitted_rating)) %>%\n  ggplot(aes(y = writer, x = fitted_rating)) +\n  geom_interval(aes(xmin = .lower, xmax = .upper)) +\n  geom_point(aes(x = imdb_rating), data = writers_by_episode) +\n  scale_color_brewer() +\n  labs(\n    title = \"Posterior Fitted Values for IMDB Rating Ordered by Posterior Mean\",\n    subtitle = \"Actual episode ratings are plotted as points\",\n    x = \"IMDB Rating\",\n    y = \"\"\n  ) +\n  theme(legend.position = \"none\") +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n# ggsave(\"brms_ratings_by_writer.png\", width = 7, height = 8)\n\n\n\nTweet\nNow use the rtweet library to post the plots directly to Twitter without leaving R. You must connect to the Twitter API using OAuth as described in this vignette, I set the Twitter application keys in my .Renviron file which is never committed to public version control (this can be easily edited using usethis::edit_r_environ()).\n\n\nlibrary(rtweet)\n\ntoken <- create_token(\n  app = \"rstats\",\n  consumer_key = Sys.getenv(\"TWITTER_API_KEY\"), \n  consumer_secret = Sys.getenv(\"TWITTER_API_SECRET_KEY\"),\n  access_token = Sys.getenv(\"TWITTER_ACCESS_TOKEN\"),\n  access_secret = Sys.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\"))\n\n\n\nThe function post_tweet can be used to post a new tweet.\n\n\npost_tweet(\n  status = \"This week's #TidyTuesday data is from the US Office TV series. I've fit a simple Bayesian hierarchical model using Stan and BRMS to determine which writers produce the highest rated episodes. Full code: https://jonnylaw.rocks/blog/tidy-tuesday-the-office/ #rstats\",\n  media = c(\"brms_ratings_by_writer.png\", \"ratings_by_writer.png\"),\n  token = token\n)\n\n\n\n\n\n\n",
    "preview": "posts/2020-03-17-tidy-tuesday-the-office/tidy-tuesday-the-office_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-08-16T16:08:40+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-03-10-tidy-tuesday-us-tuition-data/",
    "title": "Tidy Tuesday: US Tuition Data",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2020-03-10",
    "categories": [
      "tidy-tuesday",
      "R"
    ],
    "contents": "\n\n\ntuesdata <- tidytuesdayR::tt_load(2020, week = 11)\n\n    Downloading file 1 of 5: `diversity_school.csv`\n    Downloading file 2 of 5: `historical_tuition.csv`\n    Downloading file 3 of 5: `salary_potential.csv`\n    Downloading file 4 of 5: `tuition_cost.csv`\n    Downloading file 5 of 5: `tuition_income.csv`\n\nThis weeks data consists of tuition costs, salary potential and diversity information of US colleges. This includes 2 year colleges which offer associate degrees, certificates and diplomas and 4 year colleges which offer bachelors and masters degrees. These are further split by private institutions, public and for profit. Additionally, Universities in the US charge different tuition fees for in-state or out-of-state students. Also, the ticket price is not always reflective of the students costs. The fees can be wholly or partially subsidised by scholarships and financial aid.\nThe first question which I wanted to answer is which universities have the highest tuition cost and what type of institution are they.\n\n\ntuition_cost <- tuesdata$tuition_cost\n\ntuition_cost %>% \n  top_n(30, wt = in_state_tuition) %>%\n  mutate(name = forcats::fct_reorder(name, in_state_tuition)) %>%\n  ggplot(aes(x = name, y = in_state_tuition, fill = type)) +\n  geom_col() +\n  coord_flip() +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  labs(title = \"Top 30 most expensive colleges\") +\n  ylab(\"College\") +\n  xlab(\"In State Tuition\")\n\n\ntidybayes can be used to plot the distribution of in state costs and out of state costs.\n\n\ntuition_cost %>% \n  group_by(type) %>% \n  pivot_longer(c(\"out_of_state_total\", \"in_state_total\"), names_to = \"tuition_type\", values_to = \"tuition\") %>% \n  ggplot(aes(x = tuition, y = tuition_type)) +\n  stat_halfeyeh() +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  labs(title = \"\")\n\n\nHistorical Tuition Data\nAnother dataset contains the historical tuition values in adjusted US dollars. We can see that private and public tuitions have doubled for four year courses since 1985. It’s quite a lot more expensive to attend college in the US now than it was 35 years ago!\n\n\ntuesdata$historical_tuition %>% \n  mutate(year = substr(year, 1, 4) %>% as.numeric()) %>% \n  ggplot(aes(x = year, y = tuition_cost, colour = tuition_type)) +\n  geom_line() +\n  facet_wrap(~type, ncol = 3) +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  labs(title = \"Tuition cost in 2016/17 dollars\")\n\n\nState tuition map\nThe package urbnmapr allows us to plot US states and overlay information at a state level.\n\n\ntuition_cost <- tuesdata$tuition_cost\n\nstates_sf <- get_urbn_map(\"states\", sf = TRUE)\n\ntuition_by_state <- states_sf %>% \n  rename(state_code = state_abbv) %>% \n  inner_join(tuition_cost, by = \"state_code\") %>% \n  filter(degree_length == \"4 Year\") %>% \n  group_by(state_code) %>% \n  summarise_if(is.numeric, list(~mean(.), ~median(.)))\n\ntuition_by_state %>% \n  ggplot() +\n  geom_sf(color = \"#ffffff\", aes(fill = out_of_state_tuition_median)) +\n  scale_fill_gradient2(labels = scales::dollar_format()) +\n  coord_sf(datum = NA)\n\n\nMost cost effective Universities\nTo quantify the most cost effective university to attend, divide the mid career pay by the total tuition paid for a 4-year degree (bachelors or masters degree).\n\n\nsalary <- tuesdata$salary_potential\n\nsalary %>% \n  left_join(tuesdata$tuition_cost %>% filter(degree_length == \"4 Year\")) %>% \n  pivot_longer(c(\"out_of_state_total\", \"in_state_total\"), names_to = \"tuition_type\", values_to = \"tuition\") %>% \n  mutate(ratio = mid_career_pay / tuition,\n         name_cost = paste(name, scales::dollar(tuition))) %>%\n  ggplot(aes(x = ratio, y = tuition)) +\n  geom_point() +\n  facet_wrap(~tuition_type, ncol = 1) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  xlab(\"Mid Career Earnings / Tuition Fee\") +\n  ylab(\"Tuition\") +\n  labs(title = \"Yearly Tuition Costs and Mid Career Earnings\")\n\n\nNet cost by income bracket\nUsing tidybayes again to view the distribution of the net-costs by income bracket. The data has a peak at zero for those receiveing full scholarships. Interesting they seem most prevelant for those with an income over £110,000!\n\n\ntuesdata$tuition_income %>% \n  ggplot(aes(y = income_lvl, x = net_cost)) +\n  stat_halfeyeh() +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  labs(title = \"Distribution of tuition cost paid by income level\")\n\n\nDiversity of top 20 Forbes Colleges\nTo create a chart of diversity such as that on the priceonomics blog I took the top 20 colleges according to Forbes from wikipedia. I copied the table to an R tribble using data pasta.\n\n\ntop_unis <- tibble::tribble(\n                              ~name, ~ranking,\n                     \"Harvard University\", 1,\n                    \"Stanford University\", 2,\n                        \"Yale University\", 3,\n  \"Massachusetts Institute of Technology\", 4,\n                   \"Princeton University\", 5,\n             \"University of Pennsylvania\", 6,\n                       \"Brown University\", 7,\n     \"California Institute of Technology\", 8,\n                        \"Duke University\", 9,\n                      \"Dartmouth College\", 10,\n                     \"Cornell University\", 11,\n                         \"Pomona College\", 12,\n     \"University of California, Berkeley\", 13,\n                    \"Columbia University\", 14,\n                  \"Georgetown University\", 15,\n                  \"University of Chicago\", 16,\n                \"Northwestern University\", 17,\n               \"University of Notre Dame\", 18,\n       \"Rensselaer Polytechnic Institute\", 19,\n                 \"University of Michigan\", 20\n  )\n\nThe Herfindahl-Hirschman index is used as a measure of diversity at each college. The general formula is:\n\\[HHI = \\sum_{i=1}^Na^2_i\\]\nWhere \\(a_i\\) represents the proportion of the \\(i^{th}\\) race and \\(N = 5\\) consisting of the number of races under consideration: White, Black, Hispanic, Asian and other. So for a completely homogeneous population \\(a_1 = 1\\) and \\(a_{2:5} = 0\\), hence \\(HHI = 1^2 + 0^2 + 0^2 + 0^2 + 0^2 = 1\\). The following plot shows the Forbes top 20 univesities ordered by diversity according to the HHI.\n\n\ndiversity <- tuesdata$diversity_school\n\nracial_diversity <- diversity %>% \n  filter(!is.na(name)) %>% \n  filter(category %in% c(\"White\", \"Black\", \"Hispanic\", \"Asian\")) %>% \n  pivot_wider(names_from = category, values_from = enrollment) %>% \n  mutate(other = total_enrollment - White - Black - Hispanic - Asian) %>% \n  pivot_longer(4:8, names_to = \"category\", values_to = \"enrollment\") %>% \n  group_by(name) %>% \n  mutate(hhi = sum((enrollment / total_enrollment) ** 2))\n\nracial_diversity %>% \n  inner_join(top_unis) %>% \n  ungroup() %>% \n  mutate(\n    name_hhi = paste(ranking, name, round(hhi, 2)),\n    name_hhi = forcats::fct_reorder(name_hhi, desc(hhi))) %>% \n  filter(ranking <= 20) %>% \n  ggplot() +\n  geom_col(aes(y = name_hhi, x = enrollment, fill = category), position = \"fill\")\n\n\nThe next plot considers diversity as measured by HHI against potential earnings at mid career. This is the plot I chose to tweet for tidy tuesday.\n\n\nracial_diversity %>%\n  inner_join(salary, by = c(\"name\")) %>%\n  distinct(name, hhi, mid_career_pay) %>%\n  ggplot(aes(x = hhi, y = mid_career_pay, colour = name)) +\n  geom_point() +\n  gghighlight(mid_career_pay > 1.55e5 |\n                hhi > 0.9, use_direct_label = TRUE) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  labs(\n    title = \"Expected Pay and Racial Diversity at US Universities\",\n    subtitle = \"Racial Diversity is calculated using HHI\\nwhich ranges from 0.2 (most diverse) to 1 (least diverse).\") +\n    ylab(\"Mid Career Pay\") +\n    xlab(\"Herfindahl-Hirschman index\")\n\n\n# ggsave(\"diversity.png\")\n\nTweet\nNow use the rtweet library to post the plots directly to Twitter without leaving R. You must connect to the Twitter API using OAuth as described in this vignette, I set the Twitter application keys in my .Renviron file which is never committed to public version control (this can be easily edited using usethis::edit_r_environ()).\n\n\nlibrary(rtweet)\n\ntoken <- create_token(\n  app = \"rstats\",\n  consumer_key = Sys.getenv(\"TWITTER_API_KEY\"), \n  consumer_secret = Sys.getenv(\"TWITTER_API_SECRET_KEY\"),\n  access_token = Sys.getenv(\"TWITTER_ACCESS_TOKEN\"),\n  access_secret = Sys.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\"))\n\nThe function post_tweet can be used to post a new tweet.\n\n\npost_tweet(\n  status = \"This week's #TidyTuesday features diversity, tuition costs and expected salaries at US universities. It appears more diverse colleges tend to have higher earning graduates. #rstats\",\n  media = c(\"diversity.png\", \"carbon.png\"),\n  token = token\n)\n\n\n\n",
    "preview": "posts/2020-03-10-tidy-tuesday-us-tuition-data/distill-preview.png",
    "last_modified": "2020-07-30T08:18:13+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-03-04-harrier_league_data/",
    "title": "Releasing Harrier League Data",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2020-03-04",
    "categories": [
      "R"
    ],
    "contents": "\nThe North East Harrier League is a series of cross country running races in the North East of England taking place over the winter from September to March. Results are available online from 2012-13 season to the present season 2019-20. The results are available online in HTML format. I have downloaded and cleaned the data and it can be used for analysis or exploration. The data for senior men and women is available in a tabular format in my blog package - see the file which contains the parsing functions here to get an insight into what it takes to parse this kind of data.\nI used the following R packages to download, parse the HTML and clean the resulting data\nrvest is a web scraping library for R. The functions read_html, html_table were used to download the raw html and extract tables created using <table> tags. html_node can be used to find a specific tag, for instance the date of a fixture was often in a main header denoted by either <h1> or <h2>.\ndplyr provides a suite of functions which can be used to manipulate columns of a dataframe in a declarative, functional way.\nstringr was used to parse strings in the raw data and the HTML. Information such as division is included in the same table cell as the name. This can be extracted using str_extract(name, pattern = \"[1-3]\") the regex matches either 1, 2 or 3 and extracts the matching number.\nThe data can be accessed by installing my R package which contains a selection of R code relating to this blog.\n\n\n# install.packages(\"remotes\")\n# remotes::install_github(\"jonnylaw/jonnylaw\")\ndata(\"harrier_league_results\")\n\nDetermining the most difficult course\nAs a quick example of what can be done with the data I will consider the running time by course. The data can be split by male and female. However the men and women don’t compete over the same distance with the women completing two laps and the men completing three. Therefore we can plot the average time for a single lap of the course (obviously this doesn’t account for changing pace throughout the race). It appears that the hardest (or longest) course is Aykley Heads with the highest median race time.\n\n\n\n\n\n",
    "preview": "posts/2020-03-04-harrier_league_data/distill-preview.png",
    "last_modified": "2020-07-30T08:18:13+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-03-03-tidy_tuesday_nhl_data/",
    "title": "Tidy Tuesday: NHL Goalscorers",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2020-03-03",
    "categories": [
      "R",
      "tidy-tuesday"
    ],
    "contents": "\nFirst install the Tidy Tuesday R package.\n\n\n# install.packages(\"remotes\")\nremotes::install_github(\"thebioengineer/tidytuesdayR\")\n\nThe data for this Tuesday can be downloaded using tt_load.\n\n\ntuesdata <- tidytuesdayR::tt_load('2020-03-03')\n\n    Downloading file 1 of 3: `game_goals.csv`\n    Downloading file 2 of 3: `season_goals.csv`\n    Downloading file 3 of 3: `top_250.csv`\n\nUnfortunately this only grabbed one file - the top 250 goalscorers. First look at this file.\nTop Career Goalscorers\n\n\ntuesdata$top_250 %>% \n  top_n(30, wt = total_goals) %>% \n  mutate(player = forcats::fct_reorder(player, total_goals)) %>% \n  ggplot(aes(x = player, y = total_goals)) +\n  geom_col() +\n  coord_flip() \n\n\nAverage goals per season\n\n\nparse_end_year <- function(years) {\n  end_tens <- substr(years, 6, 7)\n  possible_end <- as.numeric(paste0(substr(years, 1, 2), end_tens))\n  start <- as.numeric(substr(years, 1, 4))\n  if (possible_end - start > 0) {\n    possible_end\n  } else {\n    as.numeric(paste0(20, end_tens))\n  }\n}\n\noptions(scipen = 99)\ntuesdata$top_250 %>%\n  rowwise() %>% \n  mutate(\n    yr_end = parse_end_year(years),\n    seasons = yr_end - yr_start,\n    average_goals_per_season = total_goals / seasons,\n    decade = cut(yr_start, breaks = seq(1920, 2020, by = 10), dig.lab = 10)\n  ) %>% \n  ungroup() %>% \n  # group_by(decade) %>% \n  mutate(player = forcats::fct_reorder(player, average_goals_per_season)) %>% \n  top_n(30, wt = average_goals_per_season) %>%\n  ggplot(aes(x = player, y = average_goals_per_season, fill = active)) +\n  geom_col() +\n  coord_flip()\n\n\n  # facet_wrap( ~ decade, scales = \"free_y\")\n\n\n\ngoals_per_season <- tuesdata$top_250 %>%\n  rowwise() %>%\n  mutate(\n    yr_end = parse_end_year(years),\n    seasons = yr_end - yr_start,\n    average_goals_per_season = total_goals / seasons,\n    decade = cut(\n      yr_start,\n      breaks = seq(1920, 2020, by = 10),\n      dig.lab = 10\n    )\n  ) %>%\n  ungroup()\n\nggplot() +\n  geom_point(data = goals_per_season,\n             aes(x = seasons, y = average_goals_per_season)) +\n  ggrepel::geom_label_repel(data = top_n(goals_per_season, 10, total_goals), aes(x = seasons, y = average_goals_per_season, label = paste(player, total_goals), fill = active)) +\n  xlab(\"Total Seasons in NHL\") +\n  ylab(\"Average Goals per Season\") +\n  theme_minimal() +\n  labs(title = tools::toTitleCase(\"average goals per season and total number of seasons \\nfor the top 250 NHL Goal Scorers\"), subtitle = \"Top 10 all-time goal scorers are labelled\") +\n  theme(legend.position = c(0.9, 0.9), legend.title = element_blank())\n\n\n# ggsave(\"goals_per_season.png\")\n\nGame Goals\n\n\ngame_goals <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-03/game_goals.csv')\n\n\n\nhighlighted_players <- tuesdata$top_250 %>% \n  filter(player %in% c(\"Alex Ovechkin\", \"Wayne Gretzky\")) %>% \n  select(player, yr_start)\n\ncumulative_goals <- game_goals %>% \n  mutate(age_years = as.numeric(substr(age, 1, 2))) %>% \n  group_by(player, age_years) %>% \n  summarise(goals_per_season = sum(goals)) %>% \n  arrange(player, age_years) %>% \n  inner_join(tuesdata$top_250, by = \"player\") %>% \n  mutate(cumulative_goals = cumsum(goals_per_season))\n\nggplot(cumulative_goals, aes(x = age_years, y = cumulative_goals)) +\n  geom_line(data = cumulative_goals,\n            aes(x = age_years, y = cumulative_goals, group = player),\n            alpha = 0.2) +\n  geom_line(\n    data = cumulative_goals %>% inner_join(highlighted_players, by = \"player\"),\n    aes(x = age_years, y = cumulative_goals, colour = player)\n  ) +\n  geom_label_repel(\n    data =\n      cumulative_goals %>%\n      inner_join(highlighted_players, by = \"player\") %>% \n      filter(age_years == 30),\n    aes(\n      x = age_years,\n      y = cumulative_goals,\n      label = player,\n      colour = player\n    )\n  ) +\n  xlab(\"Player Age\") +\n  ylab(\"Total Goals\") +\n  theme_minimal() +\n  labs(title = \"Cumulative Goals in the NHL by Age\", subtitle = \"Data from 42 of the all time 250 NHL scorers who started their career since game-level \\ndata became available in 1979/80 season.\") +\n  theme(legend.position = \"none\")\n\n\n# ggsave(\"cumulative_goals.png\")\n\nPost some results to Twitter\nNow use the twitteR library to post the plots directly to Twitter without leaving R. You must connect to the Twitter API using OAuth as described in the README for twitteR, I set the Twitter application keys in my .Renviron file which is never committed to public version control (this can be easily edited using usethis::edit_r_environ()).\n\n\nlibrary(twitteR)\noauth <- setup_twitter_oauth(\n  Sys.getenv(\"TWITTER_API_KEY\"),\n  Sys.getenv(\"TWITTER_API_SECRET_KEY\"),\n  access_token = Sys.getenv(\"TWITTER_ACCESS_TOKEN\"),\n  access_secret = Sys.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\")\n)\n\nWith twitteR we can get the most recent #tidytuesday posts\n\n\nsearchTwitter('#tidytuesday', n = 10)\n\nUnfortunately the twitteR API (I think people tend to use rtweet now!) is a little outdated and requires tweets to be less than 140 characters (however this can be bypassed using bypassCharLimit = TRUE) and we can’t post multiple images in the same tweet using the mediaPath argument. So unfortunately I had to resort to posting my graphs manually!\n\n\nupdateStatus(\n  \"Can Alex Ovechkin overhaul Wayne Gretzky's all time NHL goal scoring record. It appears as if Wayne slowed down in the latter years of his career. #rstats #TidyTuesday\",\n  mediaPath = c(\"cumulative_goals.png\", \"goals_per_season.png\"),\n  bypassCharLimit = TRUE\n)\n\n\n\n",
    "preview": "posts/2020-03-03-tidy_tuesday_nhl_data/distill-preview.png",
    "last_modified": "2020-07-30T13:48:11+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-11-04-parsing-strava/",
    "title": "Analysing .fit files in R",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-11-04",
    "categories": [
      "R"
    ],
    "contents": "\nGarmin running watches output a file type called .fit, the developer SDK can be downloaded from the ANT website. There is also Python library named fitparse which has been written to parse .fit files. This blog post will show you how to use reticulate to parse a .fit file.\nFirst create a Python virtual environment, this is commonly used to store a projects’ package collection together to enable more straightforward reproducibility. A virtual environment also contains its own Python and the python package manager pip for installing and managing packages. reticulate has a function to create a virtual environment:\n\n\nvirtualenv_create(\"r-reticulate\")\n\nvirtualenv: r-reticulate\n\nuse_virtualenv(\"r-reticulate\")\n\nParsing\nThe virtual environment can be used to install the Python package fitparse\n\n\npy_install(\"fitparse\")\n\nThe library can be imported as an R object.\n\n\nfitparse <- reticulate::import(\"fitparse\")\n\nThen methods and classes defined in the fitparse Python libary can be accessed using the $ notation. Typing $ after fitparse (and hitting the TAB key) in the RStudio IDE gives a list of top-level methods and classes defined in the fitparse library.\n\n\n\n\n\nff <- fitparse$FitFile(fit_file)\n\nWe can use the get_messages method on the FitFile. This returns a generator, this is a special type of lazy list in Python.\n\n\ngenerator <- ff$get_messages(\"record\")\n\niterate is a function provided by the reticulate library which can be used to traverse a Python generator:\n\n\nactivity <- reticulate::iterate(ff$get_messages(\"record\"), function(x) x$get_values())\n\nThis evaluates the generator and applies the function get_values to retrieve the details associated with this activity. A list object is returned by R, the first element looks like this:\n\n\nactivity[[1]]\n\n$timestamp\n2017-03-16 17:22:08\n\n$position_lat\n[1] 655852851\n\n$position_long\n[1] -19374352\n\n$distance\n[1] 3.47\n\n$enhanced_altitude\n[1] 88.8\n\n$altitude\n[1] 2944\n\n$enhanced_speed\n[1] 3.471\n\n$speed\n[1] 3471\n\n$vertical_oscillation\n[1] 120\n\n$stance_time_percent\n[1] 34.25\n\n$stance_time\n[1] 259\n\n$heart_rate\n[1] 137\n\n$cadence\n[1] 79\n\n$activity_type\n[1] \"running\"\n\n$fractional_cadence\n[1] 0\n\nWe want to transform this list of lists into a dataframe. The most straightforward way is to extract the elements of interest using the map function from purrr:\n\n\n(activity_tibble <- activity %>%\n  purrr::map_dfr(function(x) tibble(\n    timestamp = readr::parse_datetime(as.character(x$timestamp)),\n    latitude = x$position_lat,\n    longitude = x$position_long,\n    elevation = x$enhanced_altitude,\n    heart_rate = x$heart_rate,\n    cadence = x$cadence\n    )))\n\n# A tibble: 611 x 6\n   timestamp           latitude longitude elevation heart_rate cadence\n   <dttm>                 <int>     <int>     <dbl>      <int>   <int>\n 1 2017-03-16 17:22:08   6.56e8 -19374352      88.8        137      79\n 2 2017-03-16 17:22:09   6.56e8 -19375987      90.2        138      79\n 3 2017-03-16 17:22:10   6.56e8 -19374810      81.2        138      80\n 4 2017-03-16 17:22:15   6.56e8 -19372657      39.2        139      78\n 5 2017-03-16 17:22:21   6.56e8 -19368861      54.2        140      79\n 6 2017-03-16 17:22:26   6.56e8 -19365257      54.6        140      80\n 7 2017-03-16 17:22:32   6.56e8 -19362723      65.4        138      79\n 8 2017-03-16 17:22:33   6.56e8 -19361314      69          138      79\n 9 2017-03-16 17:22:37   6.56e8 -19358357      98.8        135      79\n10 2017-03-16 17:22:44   6.56e8 -19354267      97          135      80\n# … with 601 more rows\n\nNotice that the latitude and longitude don’t look correct, it turns out they are in semicircles and can be converted to a recognisable coordinate system using the following function.\n\n\nsemicircle_to_degrees <- function(semicircle)\n  semicircle * (180 / 2**31)\n\nApplying the function to the latitude and longitude models.\n\n\nactivity_tibble <- activity_tibble %>% \n  mutate_at(vars(latitude, longitude), semicircle_to_degrees)\n\nThis is a very basic summary of the activity. We can derive the distance per timestep using the longitude, latitude and timestamp fields.\n\n\n(activity_tibble <- activity_tibble %>%\n  mutate(\n    time_diff_to_prev = as.numeric(difftime(timestamp, lag(timestamp, default = .$timestamp[1]))),\n    cumtime = cumsum(time_diff_to_prev),\n    dist_to_prev = c(0, sp::spDists(\n      x = as.matrix(.[, c(\"longitude\", \"latitude\")]),\n      longlat = TRUE,\n      segments = TRUE\n    )),\n    elevation_to_prev = elevation - lag(elevation),\n    distance = cumsum(dist_to_prev)\n    )) \n\n# A tibble: 611 x 11\n   timestamp           latitude longitude elevation heart_rate cadence\n   <dttm>                 <dbl>     <dbl>     <dbl>      <int>   <int>\n 1 2017-03-16 17:22:08     55.0     -1.62      88.8        137      79\n 2 2017-03-16 17:22:09     55.0     -1.62      90.2        138      79\n 3 2017-03-16 17:22:10     55.0     -1.62      81.2        138      80\n 4 2017-03-16 17:22:15     55.0     -1.62      39.2        139      78\n 5 2017-03-16 17:22:21     55.0     -1.62      54.2        140      79\n 6 2017-03-16 17:22:26     55.0     -1.62      54.6        140      80\n 7 2017-03-16 17:22:32     55.0     -1.62      65.4        138      79\n 8 2017-03-16 17:22:33     55.0     -1.62      69          138      79\n 9 2017-03-16 17:22:37     55.0     -1.62      98.8        135      79\n10 2017-03-16 17:22:44     55.0     -1.62      97          135      80\n# … with 601 more rows, and 5 more variables:\n#   time_diff_to_prev <dbl>, cumtime <dbl>, dist_to_prev <dbl>,\n#   elevation_to_prev <dbl>, distance <dbl>\n\nSummarising\nWe can calculate a high level summary of the activity similar to what you would find on Garmin connect.\n\ntotal_distance\nelapsed_time\nmoving_time\nelevation_gain\naverage_heart_rate\naverage_pace\n8.044634\n39.65 mins\n39M 39S\n380\n150\n00:04:55.72509\n\nWe can recreate plots commonly found on activity websites such as training peaks, Strava and Garmin Connect, for instance average speed for each 1km:\n\n\n\n\n\n\nAnalysing heart rate data\nWe can determine how hard the activity was for the athlete using heart rate data. Heart rate is an individual metric and differs between athletes running the same pace. To that end, we must compute the heart rate relative to the maximum heart rate or using heart rate reserve (taking into account both the maximum and resting heart rate). Using the max heart rate and resting heart rate, training zones can be determined. These zones are broad and for the convenience of the athlete (and coach) when performing workouts at a given intensity. This intensity should vary depending on the purpose of the workout (recovery, threshold, VO2 max intervals etc.).\nSuggested heart rate zones according to Pete Pfitzinger are:\nActive recovery: less than 76% MHR\nGeneral Aerobic: 70%-81% MHR\nTempo (Marathon pace): 81%-88% MHR\nLactate Threshold: 82%-92% MHR\nAnaerobic: 95%+\nFor my maximum heart rate of 189, the zones can be written as.\n\n\n\n\nzone\nheart_rate\none\n144\ntwo\n153\nthree\n166\nfour\n174\nfive\n189\n\nThen the time in zones can be plotted for the given activity.\n\n\n\nTraining impulse\nTRIMP can be used (TRaining IMPulse) to calculate a one-number summary of the activity difficulty, more information on TRIMP can be found here.\nThe most straightforward way to calculate TRIMP is calculating the total time in each zone by multiplying the zone number by the total minutes in the corresponding zone.\n\ntrimp_zone\n87.86667\n\nThis number is straightforward to calculate however it lacks nuance. For instance it remains the same if the athlete is at either the upper or lower end of the heart rate range for a given zone. To account for this TRIMP exp can be calculated:\n\\[\\textrm{TRIMP}^{\\textrm{exp}} = \\sum_{i=1}^T \\textrm{D}_i \\cdot \\textrm{HRr} \\cdot 0.64e^y\\]\nWhere, \\(\\textrm{D}_i\\) is the duration of a single measurement (typically one to five seconds on a Garmin watch), HRr is the heart rate reserve (maximum heart rate - resting heart rate), \\(y\\) is the percentage of heart rate reserve multiplied by 1.92 for men and 1.67 for women.\n\n\ntrimp_exp <- function(heartrate, time_seconds, max_hr, resting_hr, sex = \"Male\") {\n  heart_rate_reserve <- max_hr - resting_hr\n  hrr <- heartrate / heart_rate_reserve\n  constant <- if_else(sex == \"Male\", 1.92, 1.67)\n  sum((time_seconds / 60) * hrr * 0.64 * exp(constant * hrr))\n}\n\n\ntrimp_exp\n187.6727\n\nThese summaries can be used to calculate the overall training workload for an athlete to assist with planning and reviewing training plans. This is typically used in addition to training time and distance covered.\n\n\n",
    "preview": "posts/2019-11-04-parsing-strava/distill-preview.png",
    "last_modified": "2020-07-30T08:18:13+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-08-05-ad_r/",
    "title": "Forward Mode AD in R",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-08-05",
    "categories": [
      "R"
    ],
    "contents": "\nForward Mode Automatic Differentation\nAutomatic differentiation can be used to calculate the exact derivative of a function at a point using applications of the chain rule. Dual numbers provide a straightforward implementation in R using S3 generic methods. A dual number has a real component and a “dual” component which can be used to exactly calculate the expression and derivative at a specific value of \\(x\\). Consider the quadratic form \\(f(x) = 5x^2 + 3x + 10\\) with derivative \\(f^\\prime(x) = 10x + 3\\). The function and derivative can be evaluated at a value, say \\(x = 5\\) using the dual number \\(5 + \\varepsilon\\), the dual component \\(\\varepsilon\\) is considered small such that \\(\\varepsilon^2 = 0\\) then calculating \\(f(5 + \\varepsilon)\\):\n\\[\\begin{align}\nf(5 + \\varepsilon) &= 5(5 + \\varepsilon)^2 + 3(5 + \\varepsilon) + 10,\\\\\n&= 5(25 + 10\\varepsilon + \\varepsilon^2) + 15 + 3\\varepsilon + 10,\\\\\n&= 5\\varepsilon^2 + 53\\varepsilon + 150.\n\\end{align}\\]\nThen the coefficient of \\(\\varepsilon\\) is the derivative and the constant is the evaluation of the function, \\(f(5) = 150\\) and \\(f^\\prime(5) = 53\\).\nS3 Objects\nR has three systems for object oriented programming, S3, S4 and reference classes which can be learned about in the relevant chapter of Advanced R. Dual numbers can be implemented as an S3 class in R:\n\n\ndual <- function(real, eps) {\n  if (!is.numeric(real)) stop(\"real must be numeric\")\n  structure(list(real = real, eps = eps), class = \"dual\")\n} \nvar <- function(x) {\n  if (!is.numeric(x)) stop(\"x must be numeric\")\n  dual(x, 1)\n}\nconst <- function(x) {\n  if (!is.numeric(x)) stop(\"x must be numeric\")\n  dual(x, 0)\n}\n\n\n\nvar represents a variable which we want to differentiate, whereas const represents a constant.\nNext, primitive functions can be defined in terms of dual numbers which simultaneously evaluate the function and the derivative:\n\n\nplus <- function(x, y) \n  dual(x$real + y$real, x$eps + y$eps)\nminus <- function(x, y) \n  dual(x$real - y$real, x$eps - y$eps)\ntimes <- function(x, y) \n  dual(x$real * y$real, x$eps * y$real + y$eps * x$real)\ndivide <- function(x, y) \n  dual(\n      x$real / y$real,\n      (x$eps * y$real - x$real * y$eps) / (y$real * y$real)\n    )\n\n\n\nGroup generics can be used to implement the mathematics of dual numbers. Group generics included with base R include Math which includes special functions such such as abs and sqrt as well as trigonometric and hyperbolic functions. Ops which include the basic infix operations reqruired for arithmetic, +, -, *, / etc. For a full list of group generics associated with Math and Ops consult the R help by typing ?groupGeneric in the R console. In order to implement a group generic for the S3 class dual we implement Ops.dual:\n\n\nOps.dual <- function(x, y) {\n  switch(\n    .Generic,\n    `+` = plus(x, y),\n    `-` = minus(x, y),\n    `*` = times(x, y),\n    `/` = divide(x, y)\n  )\n}\n\n\n\nswitch is used to pattern match on the generic function being called within Ops by matching on .Generic. Implementing dual numbers in this way allows us to define a function using the in-built infix operators in a natural way. The function \\(f(x)\\) can be defined in terms of dual numbers as\n\n\nf <- function(x) \n  const(5) * x * x + const(3) * x + const(10)\n\n\n\nThen evaluated at \\(x = 5\\) using the constructor var which initialises a dual with \\(\\varepsilon = 1.0\\).\n\n\nf(var(5))\n\n\n$real\n[1] 150\n\n$eps\n[1] 53\n\nattr(,\"class\")\n[1] \"dual\"\n\nThe definition of f is cumbersome since we have to explicitly create the constants using the const constructor. The methods defined in Ops.dual can be extended to handle cases when a double is multiplied by a dual number to convert the double to a const and hence we can automatically differentiate any univariate function using forward mode automatic differentiation.\nWe can write a function which checks the arguments of plus, minus etc, then if the arguments aren’t explicitly dual number variables using the function var then they are converted to a dual constant using const. This function checks each argument (of a generic function of two arguments f) in turn to determine if they are doubles then promotes them to constants.\n\n\nlift_function <- function(f) {\n  function(x, y)\n    if (is.double(x)) {\n      f(const(x), y)\n    } else if (is.double(y)) {\n      f(x, const(y))\n    } else {\n      f(x, y)\n    }\n}\n\n\n\nThe ops can then be re-defined using the lift_function:\n\n\nOps.dual <- function(x, y) {\n  switch(\n    .Generic,\n    `+` = lift_function(plus)(x, y),\n    `-` = lift_function(minus)(x, y),\n    `*` = lift_function(times)(x, y),\n    `/` = lift_function(divide)(x, y)\n  )\n}\n\n\n\nThen f can be defined more naturally:\n\n\nf <- function(x) \n  5 * x * x + 3 * x + 10\n\n\n\nAnd the derivative calculated:\n\n\nf(var(5))\n\n\n$real\n[1] 150\n\n$eps\n[1] 53\n\nattr(,\"class\")\n[1] \"dual\"\n\nTesting using Hedgehog\nHedgehog is a package which utilises testthat to implement property based testing in R. Property based testing can be used to check a wide range of inputs to a function and determine if the code outputs the expected value. In standard unit testing the state before the test is defined by programmer and typically does not change - if we were to consider a test for the derivative of the quadratic function defined above then we might write a test which evaluates the function at \\(x = 5\\). This verifies we are correct for \\(x = 5\\), but what about \\(x = 0\\) or another value. With property based testing, we define a random generator for the input and the test checks hundreds of potential values for failure.\nThe input to this property based test is a, a number between \\(-100\\) and \\(100\\). The usual testthat syntax is then used to evaluate the gradient using forward mode AD and comparing it to the exact derivative calculated by hand.\n\n\ntest_that(\"Derivative of 5x^2 + 3x + 10\",\n          forall(list(a = gen.c(gen.element(\n            -100:100\n          ))),\n          function(a)\n            expect_equal(object = f(var(a))$eps, expected = 10 * a + 3)))\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-06T09:28:22+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-07-31-hmc/",
    "title": "Hamiltonian Monte Carlo in R",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-07-31",
    "categories": [
      "R",
      "Bayesian"
    ],
    "contents": "\nIntroduction\nDetermining the posterior distribution for the parameters of a real-world Bayesian model inevitably requires calculating high-dimensional integrals. Often these are tedious or impossible to calculate by hand. Markov chain Monte Carlo (MCMC) algorithms are popular approaches, samplers such as the Gibbs sampler can be used to sample from models with conditionally conjugate specifications and the Metropolis-Hastings algorithm can be used when the conditionally conjugate form is not present.\nThere are downsides to these established methods, Gibbs sampling puts a restrictive form on the prior distribution. Although Metropolis-Hastings (MH) allows any prior distribution from which the probability density function can be evaluaten, the proposal distribution is often a multivariate Normal distribution with mean corresponding to the previous value of the parameters. This proposal results in random walk behaviour and although the MH algorithm is guaranteed to converge it can take a long time to get satisfactory draws from the posterior distribution.\nThe gradient of the un-normalised log-posterior distribution can be used to explore the posterior distribution more efficiently. Hamiltonian Monte Carlo (HMC) is an MCMC method which utilises a discretisation of Hamilton’s equations in order to model a physical system where the parameters are represented by the position of a particle in \\(\\theta \\in \\mathbb{R^d}\\). In order to implement HMC, the posterior distribution is augmented with a momentum vector, \\(\\phi\\), which is used to propose updates to the position which can be far away from the initial position.\nHMC in R\nHamilton’s equations are discretised and a “leapfrog” update is used. These leapfrog steps can be written as:\n\n\nleapfrog_step <- function(gradient, step_size, position, momentum, d) {\n  momentum1 <- momentum + gradient(position) * 0.5 * step_size\n  position1 <- position + step_size * momentum1\n  momentum2 <- momentum1 + gradient(position1) * 0.5 * step_size\n\n  matrix(c(position1, momentum2), ncol = d*2)\n}\n\n\n\nleapfrogs <- function(gradient, step_size, l, position, momentum, d) {\n  for (i in 1:l) {\n    pos_mom <- leapfrog_step(gradient, step_size, position, momentum, d)\n    position <- pos_mom[seq_len(d)]\n    momentum <- pos_mom[-seq_len(d)]\n  }\n  pos_mom\n}\n\nThe log-acceptance can be written as:\n\n\nlog_acceptance <- function(propPosition,\n                           propMomentum,\n                           position,\n                           momentum,\n                           log_posterior) {\n  log_posterior(propPosition) + sum(dnorm(propMomentum, log = T)) - \n    log_posterior(position) - sum(dnorm(momentum, log = T))\n}\n\nIn order to propose a new set of parameters a random momentum vector is drawn from the Normal distribution and used in the leapfrog steps. To ensure detailed balance and that the stationary distribution of the Markov chain is equivalent to the target distribution, a metropolis step is used accepting the newly proposed parameters with log-probability equal to the log_acceptance defined above. To implement this in R, we compare a log uniform random number to the log acceptance criteria. A single HMC step can be written as:\n\n\nhmc_step <- function(log_posterior, gradient, step_size, l, position) {\n  d <- length(position)\n  momentum <- rnorm(d)\n  pos_mom <- leapfrogs(gradient, step_size, l, position, momentum, d)\n  propPosition <- pos_mom[seq_len(d)]\n  propMomentum <- pos_mom[-seq_len(d)]\n  a <- log_acceptance(propPosition, propMomentum, position, momentum, log_posterior)\n  if (log(runif(1)) < a) {\n    propPosition\n  } else {\n    position\n  }\n}\n\nThe HMC algorithm can be written as\n\n\nhmc <- function(log_posterior, gradient, step_size, l, initP, m) {\n  out <- matrix(NA_real_, nrow = m, ncol = length(initP))\n  out[1, ] <- initP\n  for (i in 2:m) {\n    out[i, ] <- hmc_step(log_posterior, gradient, step_size, l, out[i-1,])\n  }\n  out\n}\n\nAn Example Model: Bivariate Normal Model\nThe same bivariate Normal model from a previous post implementing the Metropolis algorithm is used. See that post for details of deriving the log-likelihood and choice of prior distributions for the parameters.\n\n\n\n\n\n\nFrom the previous post, the log-posterior distribution is the sum of the log-prior and the log-likelihood. The log-likelihood is given by:\n\\[\\log p(y|\\mu, \\Sigma) = \\sum_{j=1}^2\\left(-\\frac{N}{2}\\log(2\\pi\\sigma_{j}^2) - \\frac{1}{2\\sigma_{j}^2}\\sum_{i=1}^N(y_{ij}-\\mu_j)^2\\right)\\] Where \\(\\Sigma = \\operatorname{diag}(\\sigma_1, \\sigma_2)\\), the prior distributions are chosen to be:\n\\[\\begin{align}\np(\\mu_j) &= \\mathcal{N}(0, 3), \\\\\np(\\sigma_j) &= \\textrm{Gamma}(3, 3), \\quad j = 1, 2.\n\\end{align}\\]\nThe log-pdf of these distributions are:\n\\[\\begin{align}\n\\log p(\\mu_j) &= -\\frac{1}{2}\\log(18\\pi)-\\frac{\\mu_j^2}{18} \\\\\n\\log p(\\sigma_j) &= \\alpha\\log(\\beta)-\\log(\\Gamma(\\alpha)) + (\\alpha-1)\\log(\\sigma_j)-\\beta \\sigma_j\n\\end{align}\\]\nThe gradient of the log-posterior with respect to each of the paramters can be written as:\n\\[\\begin{align}\n\\frac{\\partial \\ell}{\\partial \\mu_j} &= \\frac{1}{\\sigma_j^2}\\sum_{i=1}^N(y_{ij}-\\mu_j) - \\frac{\\mu_j}{9}, \\\\\n\\frac{\\partial \\ell}{\\partial \\sigma_j} &= -\\frac{N}{\\sigma_j} +  \\frac{1}{\\sigma_j^3}\\sum_{i=1}^N(y_{ij}-\\mu_j)^2 + \\frac{2}{\\sigma_j}-3, \\quad j = 1, 2.\n\\end{align}\\]\nIn R the gradient can be programmed by hand:\n\n\ngradient <- function(ys) {\n  function(theta) {\n    mu <- c(theta[1], theta[3])\n    sigma <- c(theta[2], theta[4])\n    n <- nrow(ys)\n    c(1/sigma[1]^2*sum(ys[,1] - mu[1]) - mu[1]/9,\n      -n/sigma[1] + sum((ys[,1] - mu[1])^2) / sigma[1]^3 + 2/sigma[1] - 3,\n      1/sigma[2]^2*sum(ys[,2] - mu[2]) - mu[2]/9,\n      -n/sigma[2] + sum((ys[,2] - mu[2])^2) / sigma[2]^3 + 2/sigma[2] - 3)\n  }\n}\n\nTo ensure the value of the gradient is correct we can compare it to a numerical approximation of the gradient using the https://cran.r-project.org/web/packages/numDeriv/numDeriv.pdf package:\n\n\napprox_gradient <- function(xs, theta) {\n    grad(log_posterior(xs), theta)\n}\n\ncompare_gradient <- function(theta, tol) {\n  abs(gradient(xs)(theta) - approx_gradient(xs, theta)) < tol\n}\n\ncompare_gradient(theta, 1e-3)\n\n[1] TRUE TRUE TRUE TRUE\n\nIt appears the calculated derivative is correct. Next, HMC works best when the leapfrog proposal can propose unconstrained values of the parameters which lie on the whole real line. A transform function is defined for the parameters, \\(\\theta\\) which calculates the exponential of the standard deviation parameters. The log-posterior is calculated using the transformed values, the appropriate transformation and inverse transformation functions can be written as:\n\n\ntransform <- function(theta) {\n  c(theta[1], exp(theta[2]), theta[3], exp(theta[4]))\n}\n\ninv_transform <- function(theta) {\n  c(theta[1], log(theta[2]), theta[3], log(theta[4]))\n}\n\nThe leapfrog step proposal is calculated using the unconstrained parameters, hence the derivative of the log-jacobian of the transformation is required to be added to the value of the gradient of the log-density. Then the derivative of the log-jacobian is calculated to get the value of the gradient corresponding to the unconstrained parameters in the leapfrog step.\n\n\nlog_jacobian <- function(theta) {\n  c(0, theta[2], 0, theta[4])\n}\n\nderiv_log_jacobian <- function(theta) {\n  c(0, 1, 0, 1)\n}\n\nThe derivative of the log-jacobian contributes the value 1 to each of the partial derivatives \\(\\frac{\\partial \\ell}{\\partial \\sigma_j}, j = 1, 2.\\)\n\n\n# evaluate the log-posterior on the appropriate scale, using the transform function\nbounded_log_posterior <- function(xs) {\n  function(theta) {\n    log_posterior(xs)(transform(theta)) + sum(log_jacobian(theta))\n  }\n}\n\nbounded_gradient <- function(xs) {\n  function(theta) {\n    gradient(xs)(transform(theta)) + deriv_log_jacobian(theta)\n  }\n}\n\nThe HMC algorithm can be run in parallel using the furrr package as described in my post about the Metropolis algorithm. First the hmc function is used in another function which returns a dataframe called hmc_df.\n\n\nhmc_df <- function(log_posterior, gradient, step_size, l, initP, m, parameter_names) {\n  mat <- hmc(log_posterior, gradient, step_size, l, initP, m)\n  colnames(mat) <- parameter_names\n  as.data.frame(mat) %>% \n    mutate(iteration = row_number())\n}\n\nThen the function is used in future_map_dfr:\n\n\nfuture::plan(future::multiprocess)\nstart_time <- Sys.time()\nout_hmc <-\n  furrr::future_map_dfr(\n    .x = 1:2,\n    .f = function(x)\n      hmc_df(\n        bounded_log_posterior(xs),\n        bounded_gradient(xs),\n        0.01,\n        4,\n        inv_transform(theta),\n        10000,\n        c(\"mu1\", \"sigma1\", \"mu2\", \"sigma2\")\n      ),\n    .id = \"chain\"\n  )\nend_time <- Sys.time()\nhmc_time <- end_time - start_time\n\nThe traceplots of both chains are plotted below.\n\n\n\nEvaluating the efficiency\nIn order to compare between Markov chain Monte Carlo algorithms the amount of information from each correlated sample can be measured. This is termed the effective sample size, corresponding to the number of effectively independent draws from the posterior distribution. The code below calculates the ESS of each parameter in the chain using ess_bulk from the R interface to stan rstan library. The ESS per second can be calculated which is a measure of the efficiency of the sampler\n\n\nout_hmc %>% \n  summarise_at(2:5, rstan::ess_bulk) %>% \n  mutate_all(~ . / as.numeric(hmc_time))\n\n       mu1   sigma1      mu2  sigma2\n1 6607.128 1654.819 592.0212 2422.24\n\nThis could be compared to a similar method, such as the Metropolis algorithm. To determine which algorithm is the most efficient for sampling from the posterior distribution of the bivariate Normal model.\n\n\nproposal <- function(x) {\n  z = rnorm(4, sd = 0.05)\n  c(x[1] + z[1], x[2] * exp(z[2]),\n    x[3] + z[3], x[4] * exp(z[4]))\n}\nnames(theta) <- c(\"mu1\", \"sigma1\", \"mu2\", \"sigma2\")\n\nstart_time <- Sys.time()\nout_met <- metropolis(theta, log_posterior(xs), proposal, 1e4, chains = 2, parallel = TRUE)\nend_time <- Sys.time()\nmetropolis_time <- end_time - start_time\n\nThe ESS/s can be computed for the Metropolis algorithm\n\n\nout_met %>% \n  summarise_at(2:5, rstan::ess_bulk) %>% \n  mutate_all(~ . / as.numeric(metropolis_time))\n\n# A tibble: 1 x 4\n  accepted   mu1 sigma1   mu2\n     <dbl> <dbl>  <dbl> <dbl>\n1   13823. 1755.  1025.  334.\n\n\n\n",
    "preview": "posts/2019-07-31-hmc/distill-preview.png",
    "last_modified": "2020-07-30T16:48:08+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-06-14-bayesian-linear-regression/",
    "title": "Bayesian Linear Regression with Gibbs Sampling in R",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-06-14",
    "categories": [
      "R",
      "Bayesian"
    ],
    "contents": "\nLinear regression models are commonly used to explain relationships between predictor variables and outcome variables. The data consists of pairs of independent observations \\((y_i, x_i)\\) where \\(y_i \\in \\mathbb{R}\\) represents the outcome variable of the \\(i^\\text{th}\\) observation and \\(x_i \\in \\mathbb{R}^m\\) represents the predictors (or covariates) of the \\(i^\\text{th}\\) observation. The specification for this model is:\n\\[y_i = \\alpha + x_i^T\\beta + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2).\\]\nThe parameters of the model include the intercept (or overall mean) \\(\\alpha \\in \\mathbb{R}\\), the coefficients of the predictor variables, \\(\\beta \\in \\mathbb{R}^m\\) is a \\(m \\times 1\\) column vector and the standard deviation of the un-modelled noise, \\(\\sigma \\in \\mathbb{R}\\).\nThe Model as a Data Generating Process\nIn order to manufacture a deeper understand of linear regression it is useful to explore the model as a data generating process. This allows us to understand when linear regression is applicable, how to effectively perform parameter inference and how to assess the model fit. If the model is suitable for the application, then synthetic data from the model with appropriately chosen parameters should be indistinguishable from real observed data. The parameters used to generate the simulated data are known and hence inference algorithms should be able to reliably recover these parameters using the simulated data.\nFirst consider a simple linear regression (a regression where there is only one predictor variable) which links height to weight. We assume that height will be of adults and measured in cm. This is a continuous variable and we might think that this could be modelled using a Normal distribution with a mean of \\(150\\) and a standard deviation of \\(20\\). Let’s simulate some values:\n\n\nheights <- rnorm(100, mean = 150, sd = 20)\nqplot(heights, geom = \"histogram\")\n\n\nWe have simulated 100 heights and plotted them on a histogram. The tallest adults are 200cm and the smallest are 100cm. Now that we have our heights, it remains to choose a suitable value for the parameter \\(\\alpha\\) which will be the intercept and the coefficient \\(\\beta\\) which will be multiplied by height to determine the weight in kilograms. In addition, a value of the unmodelled noise \\(\\sigma\\) must be chosen, this seems reasonable since we know that other factors apart from height determine an individuals weight.\n\n\nalpha <- 60\nbeta <- 0.3\nsigma <- 5\nweights <- purrr::map_dbl(heights, ~ rnorm(1, mean = alpha + beta * ., sd = sigma))\nqplot(weights, geom = \"histogram\")\n\n\nFor every height, we have simulated an associated weight using purrrs map_dbl. We can plot the height against the weight and see that there is a generally increasing trend, this is expected since our chosen value of the coefficient \\(\\beta = 0.5\\).\n\n\nqplot(heights, weights)\n\n\nWhen performing an applied analysis in a business context, it might be tempting to stop here after plotting the relationship between height and weight. However these heights and weights are only a sample of a population - we wish to make statements which pertain to the entire population. If we consider the sample representative of the population then a properly fitted statistical model will allow us to make statements about the population which this sample is drawn from. As an example of a common business problem, this could include sales of a product - we wish to make statements about future sales which we can’t possibly have seen and hence a statistical model is important.\nFitting the model\nA parametric model is described by a distribution \\(p(y|\\theta)\\) where \\(y\\) represents the observed data and \\(\\theta\\) represents the parameters. These parameters are unknown, but represent properties of the model. The distribution of the observed data is controlled by the values of the parameters, \\(\\theta\\). The goal of Bayesian inference is to learn which values of the parameters are consistent with the observed data. The parameters are unknown and can’t be determined precisely, however the more data collected the more accurate the posterior inferences can be.\nIn the Bayesian paradigm, the parameters also have a distribution. Before the data is observed, this is referred to as the prior distribution \\(p(\\theta)\\) which can incorporate the hypothesis of the analyst. The goal is to determine the posterior distribution of the parameters given the observed data, this can be achieved using Bayes theorem:\n\\(p(\\theta|y) = \\frac{p(\\theta)p(y|\\theta)}{\\int_\\theta p(\\theta)p(y|\\theta)d\\theta}\\)\nThe likelihood for linear regression with \\(n\\) univariate observations, \\(\\textbf{y} = y_1,\\dots,y_n\\) is written as\n\\[p(\\textbf{y}|\\psi) = \\prod_{i=1}^n\\mathcal{N}(\\alpha + x_i^T\\beta, \\tau),\\]\nnote that the likelihood is parameterised in terms of the precision \\(\\tau = \\frac{1}{\\sigma^2}\\). Standard prior distributions for simple linear regression are chosen to be\n\\[\\begin{align*}\np(\\tau) &= \\textrm{Gamma}(\\alpha_\\sigma, \\beta_\\sigma), \\\\\np(\\alpha) &= \\mathcal{N}(\\mu_\\alpha, \\sigma^2_\\alpha), \\\\\np(\\beta) &= \\mathcal{N}(\\mu_\\beta, \\sigma^2_\\beta).\n\\end{align*}\\]\nGibbs Sampling\nGibbs sampling works by alternately sampling from the conditional conjugate distribution. It can often be faster for models which are specified using the conjugate structure, however the choice of prior distribution is not flexible (but the parameterisation is). The algebra below is not required to implement a Gibbs sampling algorithm as there are probabilistic programming languages such as BUGS and JAGS which work out the required maths.\nUsing the likelihood and priors from the section above we can derive the conditionally conjugate posterior distributions:\n\\[\\begin{align*}\np(\\tau|\\textbf{y}, \\textbf{x}, \\beta, \\alpha) &= p(\\tau)\\prod_{i=1}^np(y_i|\\psi), \\\\\n&= \\textrm{Gamma}(\\tau|\\alpha_\\sigma, \\beta_\\sigma)\\prod_{i=1}^n\\mathcal{N}(y_i|\\alpha + x_i^T\\beta, \\sigma^2), \\\\\n&\\propto \\tau^{\\alpha_\\tau-1}e^{-\\beta_\\tau\\tau}\\tau^{\\frac{n}{2}}\\exp\\left\\{-\\frac{\\tau}{2}\\sum_{i=1}^n(y_i-\\alpha - x_i^T\\beta)^2\\right\\}, \\\\\n&= \\tau^{\\alpha_\\tau-1 + \\frac{n}{2}}\\exp\\left\\{-\\beta_\\tau\\tau-\\frac{\\tau}{2}\\sum_{i=1}^n(y_i-\\alpha - x_i^T\\beta)^2\\right\\},\\\\\n&=\\textrm{Gamma}\\left(\\alpha_\\tau+\\frac{n}{2}, \\beta_\\tau +\\frac{1}{2}\\sum_{i=1}^n(y_i-\\alpha - x_i^T\\beta)^2\\right).\n\\end{align*}\\]\n\\[\\begin{align*}\np(\\alpha|\\textbf{y}, \\textbf{x}, \\beta, \\tau) &= \\mathcal{N}(\\alpha|\\mu_\\alpha, \\tau_\\alpha)\\prod_{i=1}^n\\mathcal{N}(y_i|\\alpha + x_i^T \\beta, \\tau), \\\\\n&\\propto \\tau^{\\frac{1}{2}}_\\alpha\\exp\\left\\{-\\frac{\\tau_\\alpha}{2}(\\alpha-\\mu_\\alpha)^2\\right\\}\\tau^\\frac{n}{2}\\exp\\left\\{-\\frac{\\tau}{2}\\sum_{i=1}^n(y_i-\\alpha-x_i^T \\beta)^2\\right\\}, \\\\\n&= \\exp\\left\\{-\\frac{\\tau_\\alpha}{2}(\\alpha-\\mu_\\alpha)^2-\\frac{\\tau}{2}\\sum_{i=1}^n(y_i-\\alpha-x_i^T \\beta)^2\\right\\}, \\\\\n&= \\exp \\left\\{ -\\frac{1}{2}\\left(\\alpha^2(\\tau_\\alpha + n\\tau) + \\alpha(-2\\tau_\\alpha\\mu_\\alpha - 2\\tau\\sum_{i=1}^n (y_i - x_i^T \\beta)) \\right) + C \\right\\}, \\\\\n&= \\mathcal{N}\\left((\\tau_\\alpha + n\\tau)^{-1}\\left(\\tau_\\alpha + \\tau\\sum_{i=1}^n (y_i - x_i^T \\beta)\\right), \\tau_\\alpha + n\\tau\\right).\n\\end{align*}\\]\n\\[\\begin{align*}\np(\\beta|\\textbf{y}, \\textbf{x}, \\alpha, \\tau) &= \\mathcal{N}(\\beta|\\mu_\\beta, \\tau_\\beta)\\prod_{i=1}^n\\mathcal{N}(y_i|\\alpha + x_i^T \\beta, \\tau), \\\\\n&\\propto \\tau^{\\frac{1}{2}}_\\beta\\exp\\left\\{-\\frac{\\tau_\\beta}{2}(\\beta-\\mu_\\beta)^2\\right\\}\\tau^\\frac{n}{2}\\exp\\left\\{-\\frac{\\tau}{2}\\sum_{i=1}^n(y_i-\\alpha-x_i^T \\beta)^2\\right\\}, \\\\\n&= \\exp\\left\\{-\\frac{\\tau_\\beta}{2}(\\beta-\\mu_\\beta)^2-\\frac{\\tau}{2}\\sum_{i=1}^n(y_i-\\alpha-x_i^T \\beta)^2\\right\\}, \\\\\n&= \\exp \\left\\{ -\\frac{1}{2}\\left(\\beta^2(\\tau_\\beta + \\tau\\sum_{i=1}^nx_i^2) + \\beta(-2\\tau_\\beta\\mu_\\beta - 2\\tau\\sum_{i=1}^n (y_i - \\alpha) x_i) \\right) + C \\right\\}, \\\\\n&= \\mathcal{N}\\left((\\tau_\\beta + \\sum_{i=1}^nx_i^2\\tau)^{-1}\\left(\\tau_\\beta + \\tau\\sum_{i=1}^n (y_i - \\alpha )x_i\\right), \\tau_\\beta + \\tau\\sum_{i=1}^nx_i^2 \\right).\n\\end{align*}\\]\nThis allows us to construct a Gibbs Sampler for the linear regression model by alternating sampling from the precision, \\(\\tau\\) given the latest value of the coefficient vector \\(\\beta\\) and vice versa. The functions to sample from the conditional posterior distributions are written in R as:\n\n\nsample_tau <- function(ys, alpha, beta, alpha0, beta0) {\n  rgamma(1,\n    shape = alpha0 + nrow(ys) / 2,\n    rate = beta0 + 0.5 * sum((ys$y - (alpha + as.matrix(ys$x) %*% beta))^2)\n  )\n}\n\nsample_alpha <- function(ys, beta, tau, mu0, tau0) {\n  prec <- tau0 + tau * nrow(ys)\n  mean <- (tau0 + tau * sum(ys$y - as.matrix(ys$x) %*% beta)) / prec\n  rnorm(1, mean = mean, sd = 1 / sqrt(prec))\n}\n\nsample_beta <- function(ys, alpha, tau, mu0, tau0) {\n  prec <- tau0 + tau * sum(ys$x * ys$x)\n  mean <- (tau0 + tau * sum((ys$y - alpha) * ys$x)) / prec\n  rnorm(1, mean = mean, sd = 1 / sqrt(prec))\n}\n\nThen a function which loops through each conditional distribution in turn is defined using the three functions defined above. Each conditional distribution is dependent on the parameter draw made immediately above.\n\n\ngibbs_sample <- function(ys,\n                         tau0,\n                         alpha0,\n                         beta0,\n                         m,\n                         alpha_tau,\n                         beta_tau,\n                         mu_alpha,\n                         tau_alpha,\n                         mu_beta,\n                         tau_beta) {\n  tau <- numeric(m)\n  alpha <- numeric(m)\n  beta <- numeric(m)\n  tau[1] <- tau0\n  alpha[1] <- alpha0\n  beta[1] <- beta0\n  \n  for (i in 2:m) {\n    tau[i] <-\n      sample_tau(ys, alpha[i - 1], beta[i - 1], alpha_tau, beta_tau)\n    alpha[i] <-\n      sample_alpha(ys, beta[i - 1], tau[i], mu_alpha, tau_alpha)\n    beta[i] <- sample_beta(ys, alpha[i], tau[i], mu_beta, tau_beta)\n  }\n  \n  tibble(iteration = seq_len(m),\n         tau,\n         alpha,\n         beta)\n}\n\n\n\nys <- tibble(y = weights, \n             x = heights)\n\n\n\nplan(multiprocess)\niters <- future_map_dfr(\n  .x = 1:2,\n  .f = function(x) gibbs_sample(\n      ys,\n      tau0 = 0.5,\n      alpha0 = 60,\n      beta0 = 0.3,\n      m = 1e4,\n      alpha_tau = 3,\n      beta_tau = 2,\n      mu_alpha = 0,\n      tau_alpha = 0.01,\n      mu_beta = 0,\n      tau_beta = 0.01\n    ),\n  .id = \"chain\"\n)\n\n\n\n\nMaking the Markov chain more efficient\nIn order to get this chain to mix better, the predictor (the height, \\(x\\)) can be centered by subtracting the mean. This will result in the intercept being higher than when using the untransformed data, since the outcome variable (the weight, \\(y\\)) is not transformed. In order to recover the value of the parameter\n\n\ngibbs_sample_centered <- function(ys,\n                         tau0,\n                         alpha0,\n                         beta0,\n                         m,\n                         alpha_tau,\n                         beta_tau,\n                         mu_alpha,\n                         tau_alpha,\n                         mu_beta,\n                         tau_beta) {\n  tau <- numeric(m)\n  alpha <- numeric(m)\n  beta <- numeric(m)\n  tau[1] <- tau0\n  alpha[1] <- alpha0\n  beta[1] <- beta0\n  \n  mean_x = mean(ys$x)\n  ys$x = ys$x - mean_x\n\n  for (i in 2:m) {\n    tau[i] <- sample_tau(ys, alpha[i - 1], beta[i - 1], alpha_tau, beta_tau)\n    alpha[i] <- sample_alpha(ys, beta[i - 1], tau[i], mu_alpha, tau_alpha)\n    beta[i] <- sample_beta(ys, alpha[i], tau[i], mu_beta, tau_beta)\n  }\n\n  tibble(\n    iteration = seq_len(m),\n    tau,\n    alpha = alpha - mean_x * beta,\n    beta\n  )\n}\n\n\n\niters_centered <- future_map_dfr(\n  .x = 1:2,\n  .f = function(x) gibbs_sample_centered(\n      ys,\n      tau0 = 0.5,\n      alpha0 = 60,\n      beta0 = 0.3,\n      m = 1e4,\n      alpha_tau = 3,\n      beta_tau = 2,\n      mu_alpha = 0,\n      tau_alpha = 0.01,\n      mu_beta = 0,\n      tau_beta = 0.01\n    ),\n  .id = \"chain\"\n)\n\n\n\niters_centered %>% \n  filter(iteration > 1000) %>% \n  gather(key = \"parameter\", value, -chain, -iteration) %>%\n  plot_diagnostics_sim(actual_values)\n\n\nThe draws from the Gibbs sampling algorithm are draws from the posterior distribution which can be used to produce summaries required for inference using the linear model. Posterior fitted values, ie. a straight line, can be plotted by sampling pairs of values (\\(\\alpha, \\beta\\)) from the MCMC output and plotting them using the equation of a straight line (\\(y = \\alpha + \\beta x\\)). This gives an indication of the uncertainty in the parameter estimates.\n\n\n\n\n\n",
    "preview": "posts/2019-06-14-bayesian-linear-regression/distill-preview.png",
    "last_modified": "2020-07-30T11:46:56+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-04-16-multi-armed-bandits/",
    "title": "Multi-armed Bandits in Scala",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-04-16",
    "categories": [
      "Scala"
    ],
    "contents": "\nSetting up the Environment\nThis post uses Almond in order to run Scala code in a Jupyter notebook. See my previous post to learn how to setup Jupyter, Ammonite and Almond. That post examined using the Scala libraries EvilPlot (including inline plotting in the Jupyter notebook) and Rainier for Bayesian inference in a simple linear model.\nThe imports required for this post are:\n\nimport coursier.MavenRepository\n\ninterp.repositories() ++= Seq(MavenRepository(\n  \"http://dl.bintray.com/cibotech/public\"\n))\n\nimport $ivy.`com.stripe::rainier-core:0.2.2`\nimport $ivy.`com.stripe::rainier-plot:0.2.2`\nimport $ivy.`org.scalanlp::breeze:0.13.2`\n\nimport breeze.stats.distributions._\nimport breeze.linalg._\nimport almond.interpreter.api._\nThe full working notebook can be found here.\nA Multi-armed Bandit\nA multi-armed bandit is an analogy taken from the one-armed bandit slot machines where a lever is pulled and the player has an unknown probability of a prize. A multi-armed bandit is a generalisation, whereby the player is faced with multiple one-armed bandits each of which could have different rewards. The problem is to determine the best bandit to play. One way to determine this is to randomly pull levers to get information on the payout for each bandit. Assuming the probability of payout is constant in time, then after a period of exploration the player will be able to know which bandits pay the most.\nEpsilon Greedy Method\nOne strategy to maximise the expected long-term reward from a bandit is to choose the bandit with the largest long-term reward a fraction of the time and the rest of the time choose a bandit uniformly at random in order to continually explore the space of actions. At each time step, the reward for a given action can be calculated and the long-term reward can be calculated as the function:\n\\[Q_{t+1}(A) = Q_t(A) + \\frac{R_t(A) - Q_t(A)}{N_t(A)}\\] where \\(A\\) is the current action, \\(Q_t(A)\\) is the long-term reward at time \\(t\\) for action \\(A\\), \\(R_t(A)\\) is the instantaneous reward for action \\(A\\) at time \\(t\\) and \\(N_t(A)\\) is the total number of times action \\(A\\) has been performed by time step \\(t\\). Before writing the algorithm for the epsilon greedy algorithm, first we define a few helper functions.\nThe first is to sample a value uniformly a random from a selection of values.\n\ndef sample(selection: Vector[Int]): Rand[Int] = {\n    for {\n        i <- Multinomial(DenseVector.ones[Double](selection.size))\n    } yield selection(i)\n}\nIf there are multiple actions with the same long-term reward then the next action should be selected randomly from all the actions which maximise the long term reward.\n\ndef maxActionWithTies(longTermReward: Vector[Double]): Rand[Int] = {\n    val maxReward = longTermReward.max\n    val rewards = longTermReward.zipWithIndex.\n          filter { case (r, a) => r == maxReward }.map(_._2)\n    if (rewards.size > 1) {\n        sample(rewards)        \n    } else {\n        Rand.always(rewards.head)\n    }\n}\nThen a single step of the epsilon greedy algorithm can be written\n\ncase class BanditState(\n    reward: Array[Double],\n    longTermReward: Vector[Double],\n    actions: Map[Int, Int]\n)\ndef banditStep(\n    epsilon: Double,\n    reward: Int => Rand[Double],\n    selectAction: (Map[Int, Int], Vector[Double]) => Rand[Int])(s: BanditState): Rand[BanditState] = {\n    for {\n        nextAction <- selectAction(s.actions, s.longTermReward)\n        newReward <- reward(nextAction)\n        prevCount = s.actions.get(nextAction).get\n        nextCount  = prevCount + 1\n        newLongTermReward = s.longTermReward(nextAction) + (newReward - s.longTermReward(nextAction)) / nextCount\n    } yield BanditState(s.reward :+ newReward, \n                s.longTermReward.updated(nextAction, newLongTermReward),\n                s.actions.updated(nextAction, nextCount))\n}\nFirstly, we define a BanditState which contains all of the rewards \\(R_t(A)\\) for each time step, a list of length equal to the number of actions containing the long-term reward for each action. actions represents \\(N_t(A)\\) using a map from the index of the action to the count of actions. The algorithm proceeds by sampling a uniform random number, if this number is less than the chosen value of epsilon, then a random action is sampled from a Multinomial distribution with equal probabilities, otherwise the algorithm selects the action which currently has the highest long-term reward. The values are updated according to the formula above.\nTo run this algorithm for a pre-determined number of steps, realise that it is recursive and completely determined by the count and long-term reward at the previous time step. Hence it can be implemented as a Markov chain.\n\ndef buildActions(actions: Int): Map[Int, Int] = {\n    (0 until actions).map(a => a -> 0).toMap\n}\n\ndef epsilonGreedy(\n    epsilon: Double, \n    actions: Int, \n    reward: Int => Rand[Double],\n    n: Int): BanditState = {\n    \n    val initState = BanditState(Array(0.0), Vector.fill(10)(0.0), buildActions(actions))\n    MarkovChain(initState)(banditStep(epsilon, reward, selectGreedy(epsilon))).steps.drop(n-1).next\n}\nWe can assume that the rewards for each of the ten actions is Normally distributed and define a suitable reward function\n\nval qs = Gaussian(0, 1).sample(10)\n\n// The reward is selected from a N(q(A_t), 1)\ndef r(qa: Seq[Double])(action: Int): Rand[Double] = \n    Gaussian(qa(action), 1)\n    \n//  qs: IndexedSeq[Double] = Vector(\n//    -1.1319170735731177,\n//    0.5392647196381599,\n//    0.7127636875526561,\n//    0.8765526115252499,\n//    -0.9555744042626685,\n//    -0.2723645491439034,\n//    0.10029206857194808,\n//    0.3758538986470721,\n//    1.9412629812694995,\n//    1.0620845496569054\n//)\nThen the algorithm can be run for a single multi-armed bandit\n\nval oneBandit = epsilonGreedy(0.5, 10, r(qs), 1000)\nThe distribution of actions at the end of 1,000 steps with epsilon = 0.5 and number of actions 10 is:\n\n\naction_distribution <- read_csv(here::here(\"notebooks/data/action_distribution.csv\"), \n                                col_names = c(\"action\", \"count\"))\n\naction_distribution %>% \n  ggplot(aes(x = action, y = count)) +\n  geom_col()\n\n\nThe mean reward at action 8 was highest at approximately 1.95, the epsilon-greedy algorithm prefers to take action 8.\nMultiple Multi-armed Bandits\nNow to see the behaviour of a typical multi-armed bandit with a constant reward function, calculate the average reward for n = 2,000 10-arm bandits each with 1,000 steps.\n\nVector.fill(2000)(DenseVector(bandit(0.1, 10, r(qs), 1000).reward)).\n  reduce(_ + _).\n  map(_ / 2000)\nThe average reward for each time step can then be plotted, this can be used to evaluate different choices of epsilon. Different values of epsilon can be compared and the average reward can be calculated\n\nval data = List(0.0, 0.1, 0.5).map ( eps => averageReward(2000, 1000, eps))\n\n\naverage_reward <- read_csv(here::here(\"notebooks/data/average_reward.csv\"))\n\naverage_reward %>% \n  ggplot(aes(x = step, y = reward, colour = as.factor(epsilon))) +\n  geom_line() +\n  geom_label(data = filter(average_reward, step == 1000), \n            aes(label = epsilon, x = 950, y = reward), hjust = .5) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Average reward by step for various values of epsilon\")\n\n\n\n\n",
    "preview": "posts/2019-04-16-multi-armed-bandits/distill-preview.png",
    "last_modified": "2020-07-30T13:45:12+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-04-15-scala-and-jupyter-notebook-with-almond/",
    "title": "Scala and Jupyter Notebook with Almond",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-04-15",
    "categories": [
      "Scala"
    ],
    "contents": "\nTypically, when programming with Scala I use a combination of ensime in emacs, sbt and the Scala repl. However, sometimes when working on a new project which requires a lot of data exploration and graphics it is sometimes more useful to have a notebook where figures are rendered inline with descriptions of why each figure has been generated and what it shows for future reference. Jupyter notebooks have long been the standard in Python (although I prefer rmarkdown and knitr when using R).\nJupyter notebooks can be initialised with many different kernels to serve a wide array of users. Recently there has been a release which combines the power of the Ammonite scala repl which empowers users to write small Scala scripts where dependencies can be stored in the same script file and are fetched using coursier without the need for a large SBT project. Ammonite has many more features besides this, however scripting is one of my favourites. It also allows us to write self-contained Jupyter notebooks with dependencies by utilising Ammonite as the kernel of the Jupyter notebook using Almond.\nIn this blog, I will show you how to use Almond to fit a linear regression using the probabilistic programming language, Rainier.\nSetup of Almond\nInstall Jupyter Notebook using pip\n\npython3 -m pip install --upgrade pip\npython3 -m pip install jupyter\nInstall Ammonite\n\nmkdir -p ~/.ammonite && curl -L -o ~/.ammonite/predef.sc https://git.io/vHaKQ\nInstall Almond https://almond.sh/docs/quick-start-install\nRun jupyter notebook by running jupyter notebook from a terminal and create a new document in the web interface selecting the “Scala” kernel\nScala library dependencies\nAmmonite lets you import dependencies directly from Maven central using a special import syntax, for example to import the latest version of the Rainier core library simply type:\n\nimport $ivy.`com.stripe::rainier-core:0.2.2`\nThen all imports from the Rainier library should be available. Additionally, we want to be able to use a plotting library Evilplot which does not have a standard resolver. Luckily Ammonite makes adding new resolvers straightforward, simply add a new block which points to the Maven repository of cibotech. Note that this is not an especially common operation - since most OSS Scala libraries are stored in the Maven central repository.\n\nimport coursier.MavenRepository\n\ninterp.repositories() ++= Seq(MavenRepository(\n  \"http://dl.bintray.com/cibotech/public\"\n))\nThen the plotting library can be imported, ensure this is in a new block.\n\nimport $ivy.`com.stripe::rainier-plot:0.2.2`\nBuilding a model using rainier\nThe model under consideration is straightforward, a simple linear regression with unknown slope and intercept:\n\\[y_i = \\alpha + \\beta x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\]\nIn order to perform inference to determine the posterior distribution of the unknown parameters, \\(\\psi = \\{\\alpha, \\beta, \\sigma\\}\\) on this model using Rainier first we simulate some data from the model:\n\nimport com.stripe.rainier.core._\nimport com.stripe.rainier.sampler._\n\nval (alpha, beta, sigma) =  (-1.5, 2.0, 0.5)\n\nval lm = for {\n  x <- Normal(0, 1).param\n  y <- Normal(alpha + beta * x, sigma).param\n} yield (x, y)\n\nimplicit val s = RNG.default\nval sims = lm.sample(100)\nThe code above uses rainiers sampling-based monad in order to simulate standard Normal data representing the covariates, \\(x_i, i = 1,\\dots,100\\) and the dependent variable \\(y_i\\). 100 \\((x, y)\\) pairs are simulated from the model with the selected parameter values. Now it might be of interest to plot the data using the Evilplot plotting library. Here we write out the data to a csv and use ggplot in R\n\n\nlm_sims <- read_csv(here::here(\"notebooks/data/lm_sims.csv\"))\nlm_sims %>% \n  ggplot(aes(x, y)) +\n  geom_point()\n\n\nThe code required to sample from the posterior distribution is similar to that required to simulate the model:\n\nimport com.stripe.rainier.compute._\n\ndef linearModel(data: Seq[(Double, Double)]): RandomVariable[Map[String, Real]] = for {\n    alpha <- Normal(0, 5).param\n    beta <- Normal(0, 5).param\n    sigma <- LogNormal(2, 2).param\n    _ <- Predictor[Double].from { x =>\n      Normal(alpha + beta * x, sigma)\n    }\n    .fit(data)\n  } yield Map(\"alpha\" -> alpha, \"beta\" -> beta, \"sigma\" -> sigma)\nFirst prior distributions are chosen for the static parameters, then the function Predictor is used to specify the likelihood for the linear regression as the Normal distribution. The data consists of a sequence of tuples. Finally to sample values from the posterior using Hamiltonian Monte Carlo with 5 leapfrog steps and auto-tuning of the leapfrog step-size using dual averaging.\n\nval iters = linearModel(sims).sample(HMC(5), 5000, 100000, 100)\n\n\niters <- read_csv(here::here(\"notebooks/data/lm_params.csv\"))\niters %>% \n  mutate(iteration = row_number()) %>% \n  gather(key = Parameter, value, -iteration) %>% \n  ggplot() +\n    geom_line(ggplot2::aes(x = iteration, y = value), alpha = 0.5) +\n    facet_wrap(~Parameter, scales = \"free_y\", strip.position = \"right\")\n\n\nThe full notebook can be viewed on Github here.\n\n\n",
    "preview": "posts/2019-04-15-scala-and-jupyter-notebook-with-almond/distill-preview.png",
    "last_modified": "2020-07-30T11:46:44+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-02-25-rejection_sampling/",
    "title": "Bayesian Inference using rejection sampling",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-02-25",
    "categories": [
      "R",
      "Bayesian"
    ],
    "contents": "\nCoin Flip Model\nAs an example, consider a (possibly biased) coin flip experiment. The parameter of interest is the probability of heads \\(p_h\\). A Beta distribution is chosen for the prior of \\(p_h\\), \\(p(p_h) = \\mathcal{B}(\\alpha, \\beta)\\). The Beta distribution has support between 0 and 1, which is appropriate for a probability. The likelihood of a coin flip is Bernoulli, however the coin should be flipped several times in order to learn the parameter \\(p_h\\). The distribution for \\(n\\) independent Bernoulli trials is the Binomial distribution, hence the likelihood can be written as \\(\\textrm{Bin}(Y;n,p_h)\\). The coin is flipped \\(n = 10\\) times and the results are displayed below:\n\\[\\begin{equation*}\n  \\label{eq:4}\n\\{H, H, T, H, H, H, T, T, H, T\\}  \n\\end{equation*}\\]\nThen \\(Y = 6\\), and it remains to determine the posterior distribution of the parameter \\(p_h\\) representing the probability of obtaining heads. Applying Bayes theorem:\n\\[\\begin{align*}\n  p(p_h|Y) &= \\frac{p(p_h)\\textrm{Bin}(Y;n,p_h)}{\\int_0^1 p(Y|p_h) dp_h} \\\\\n           &\\propto p_h^{\\alpha - 1}(1-p_h)^{\\beta-1}{n \\choose Y}p_h^Y(1-p_h)^{n-Y} \\\\\n           &= p_h^{Y + \\alpha - 1}(1 - p_h)^{n-Y + \\beta - 1}\n\\end{align*}\\]\nThe posterior distribution is a Beta distribution, \\(\\mathcal{B}(Y + \\alpha, n - Y + \\beta)\\).\n\n\nY <- 6\nn <- 10\n\nalpha <- 3\nbeta <- 3\n\nprior <- function(x) dbeta(x, alpha, beta)\nposterior <- function(x) dbeta(x, alpha + Y, n - Y + beta)\n\nggplot(data = data.frame(x = 0), mapping = aes(x = x)) +\n  stat_function(fun = prior, aes(colour = \"Prior\")) + xlim(0, 1) +\n  stat_function(fun = posterior, aes(colour = \"Posterior\")) +\n  xlab(\"p_h\") +\n  ylab(\"Density\") +\n  theme(text = element_text(size = 20), legend.title = element_blank(), legend.position = c(0.2, 0.9), legend.text = element_text(size = rel(1.3)))\n\n\nRejection Sampler\nThe rejection sampler is an algorithm which produces exact samples from the target distribution. Consider a problem where it is straightforward to evaluate the posterior density \\(p(\\cdot)\\) up to a constant. The rejection sampler algorithm proceeds as follows; start by sampling a value from a proposal distribution \\(\\psi^\\star \\sim q(\\cdot)\\), then accept the proposed value with probability \\(p(\\psi^\\star)/Mq(\\psi)\\), where \\(M\\) is an upper bound on \\(p/q\\). The algorithm below shows a single step of the rejection sampler algorithm which returns a single sample from the target distribution \\(p(\\cdot)\\).\nPropose \\(\\psi^\\star \\sim q(\\cdot)\\)\nContinuously sample \\(u \\sim U[0, 1]\\) and check the condition in step 3.\nIf \\(u < \\frac{p(\\psi^\\star)}{Mq(\\psi)}\\), set \\(\\psi^\\star\\) as a sample from \\(p\\)\nRepeat 1-3 until enough samples are attained\nThe figure below shows the empirical posterior distribution found for the coin flip experiment overlaid with the analytic posterior distribution. This algorithm performs well for low-dimensional problems, but finding the upper bound \\(M\\) can be challenging. The rejection algorithm does not work well in higher dimensions as many proposed moves are rejected. Adding extra dimensions to the problem results in the exponential increase in volume, this is known as the curse of dimensionality. More sophisticated algorithms are required for high dimensional target distributions.\n\n\n# Perform one rejection step\nrejection_sample <- function(prop, propPdf, log_density) {\n  u <- runif(1)\n  y <- prop(1)\n\n  if (log(u) < log_density(y) - propPdf(y)) {\n    y\n  } else {\n    rejection_sample(prop, propPdf, log_density)\n  }\n}\n\nlog_density <- function(alpha, beta, Y, n) {\n  function(theta) {\n    dbeta(theta, alpha, beta, log = T) + dbinom(Y, n, theta, log = T)\n  }\n}\n\nsamples <- 1000\nlden <- log_density(alpha, beta, Y, n)\nrejection_samples <- replicate(samples, rejection_sample(runif, function(x) dunif(x, log = T), lden))\n\nggplot(tibble(rejection_samples)) +\n  geom_histogram(aes(x = rejection_samples, y = ..density..), alpha = 0.4) +\n  stat_function(\n    fun =  posterior, aes(colour = \"Analytic posterior\")\n  ) +\n  theme(legend.position = \"none\") +\n  xlab(\"p_h\")\n\n\nFigure 1: Empirical Posterior distribution for the coin flip problem using 1,000 samples from the rejection sampler with a Uniform(0, 1) proposal distribution and M = 1. The analytic posterior distribution is plotted as a solid red line.\n\n\n\n\n\n",
    "preview": "posts/2019-02-25-rejection_sampling/distill-preview.png",
    "last_modified": "2020-07-30T11:46:40+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-02-25-sampling/",
    "title": "Sampling from a distribution with a known CDF",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-02-25",
    "categories": [
      "R"
    ],
    "contents": "\nA distribution with an inverse cumulative distribution function (CDF) can be sampled from using just samples from \\(U[0, 1]\\). The inverse CDF (sometimes called the quantile function) is the value of \\(x\\) such that \\(F_X(x) = Pr(X \\leq x) = p\\). Consider a that a transformation \\(g: [0, 1] \\rightarrow \\mathbb{R}\\), exists which takes a value sampled from the standard uniform distribution \\(u \\sim U[0, 1]\\) and returns a value distributed according to the target distribution. Then the inverse CDF can be written as:\n\\[Pr(g(U) \\leq x) = Pr(U \\leq g^{-1}(x)) = g^{-1}(x)\\]\nSince the CDF of the uniform distribution over the interval \\([0, 1]\\) is:\n\\[\\begin{align*}\n  F_U(u) =\n  \\begin{cases}\n    0 & u < 0 \\\\\n    u & u \\in [0, 1) \\\\\n    1 & u \\geq 1\n  \\end{cases}\n\\end{align*}\\]\nThen \\(F_x^{-1}(X) = g(x)\\) as required. The algorithm below summarises the inverse sampling procedure.\nSample \\(u \\sim U[0, 1]\\)\nEvaluate \\(x = F^{-1}(u)\\)\nReturn \\(x\\)\nMost statistical packages will expose the quantile function for common distributions making it practical to use inverse sampling. The figure below shows a histogram of 1,000 simulated values from a \\(\\textrm{Gamma}(3, 4)\\) distribution using the inverse CDF method, the analytical density is plotted in red.\nThe figure below shows samples from \\(\\textrm{Gamma}(3, 4)\\) using the inverse CDF method plotted with the analytical PDF.\n\n\ninverse_cdf_sample <- function(inv_cdf) {\n  u <- runif(1)\n  inv_cdf(u)\n}\n\ninv_cdf <- function(x) qgamma(p = x, shape = 3, rate = 4)\ngamma_samples <- replicate(1000, inverse_cdf_sample(inv_cdf))\n\nggplot(tibble(gamma_samples)) +\n  geom_histogram(aes(x = gamma_samples, y = ..density..), alpha = 0.4) +\n  stat_function(\n    fun = function(x) dgamma(x, shape = 3, rate = 4),\n    aes(colour = \"Gamma Density\")\n  ) +\n  theme(\n    text = element_text(size = 12), legend.title = element_blank(),\n    legend.text = element_text(size = rel(1.0)), legend.position = c(0.8, 0.8)\n  ) +\n  ylab(\"density\") +\n  xlab(\"value\")\n\n\n\n\n",
    "preview": "posts/2019-02-25-sampling/distill-preview.png",
    "last_modified": "2020-07-30T11:46:41+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-02-22-national_xc/",
    "title": "A Statistical Model for Finishing Positions at the National Cross Country",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-02-22",
    "categories": [
      "R"
    ],
    "contents": "\nArea Results\nDownload the results from Power of ten for northern, midlands, southern and national.\n\n\n\nA linear model\n\n\n\nThe goal is to fit a model, where the outcome is the position at the national and the input is the position at the northern XC. This then allows us to determine the quality of the field at each XC and determine what position you are likely to finish in the National this season given a result in the area championships. A simple linear model has each observation (runner) considered independent with normally distributed errors.\n\\[Y_i = \\beta^T x_i + \\varepsilon_i, \\quad \\mathcal{N}(0, \\sigma^2).\\]\nThree separate models are fit, one for each area championship under consideration. To construct the dataset for each of the models we join the results together from the area and nationals in 2018 by name and exclude those who didn’t participate in both. We fit the model using least squares and see that the coefficient associated with the finishing position in the northerns is 2.23 and the intercept is 95. This means that given your finishing position in the Northern XC, just add 95 and multiple by 2.23 to get an approximate finishing position in the 2018 National XC.\n\nterm\nestimate\nstd.error\nstatistic\np.value\n(Intercept)\n95.0\n25.3\n3.8\n0\nposition_northern\n2.2\n0.1\n17.0\n0\n\nNext we plot a Normal Q-Q plot to check the residuals are Normally distributed. If the residuals (the errors, \\(Y_i - \\varepsilon_i, i = 1,\\dots,N\\)) are Normally distributed the conclusions drawn from the model are valid. The Q-Q plots look reasonable (except maybe the midlands (centre)) with only a few outliers identified by R.\n\n\n\nFigure 1: Normal Q-Q plots (Left) Northern. (Centre) Midlands. (Right) Southerns.\n\n\n\nNext we can plot the actual values and the “line of best fit”. This is the regression line given by the data, we can see this generally captures the relationship quite well.\n\n\n\nLooking at the line of best fit, the data appears to be linear, however exceptional performances in both competitions are not accurately modelled.\nThe coefficients for the midlands and the southerns simple linear regression are as follows:\n\nterm\nestimate\nstd.error\nstatistic\np.value\n(Intercept)\n190.8\n45.5\n4.2\n0\nposition_midlands\n3.6\n0.4\n8.9\n0\nterm\nestimate\nstd.error\nstatistic\np.value\n(Intercept)\n89.8\n17.2\n5.2\n0\nposition_southern\n1.9\n0.1\n26.2\n0\n\nPredict 2019 National placing using the linear model\nNow we’d like to use this data in order to predict a performance in 2019. I ran in the Northern XC, finished 289th. The linear model predicts a mean finishing position of 742, given by the equation:\n\\[\\textrm{finish_nationals} = 95 + 2.239 * 289\\]\n\n\n       1 \n742.0754 \n\nMy actual finishing position was 904.\n\n\n",
    "preview": "posts/2019-02-22-national_xc/distill-preview.png",
    "last_modified": "2020-07-30T13:47:03+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-02-11-metropolis_r/",
    "title": "Efficient Markov chain Monte Carlo in R with Rcpp",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-02-11",
    "categories": [
      "R",
      "Bayesian"
    ],
    "contents": "\nBivariate Normal Model\nThis post considers how to implement a simple Metropolis scheme to determine the parameter posterior distribution of a bivariate Normal distribution. The implementation is generic, using higher-order-functions and hence can be re-used with new algorithms by specifying the un-normalised log-posterior density and a proposal distribution for the parameters. The built-in parallel package is used fit multiple chains in parallel, finally the Metropolis algorithm is reimplemented in C++ using Rcpp which seemlessly integrates with R.\n\n\nbivariate_normal <- function(theta, n) {\n  mu1 <- theta[1]\n  sigma1 <- theta[2]\n  mu2 <- theta[3]\n  sigma2 <- theta[4]\n  x <- rnorm(n / 2, mean = mu1, sd = sigma1)\n  y <- rnorm(n / 2, mean = mu2, sd = sigma2)\n  tibble(x, y)\n}\n\ntheta <- c(5.0, 0.5, 2.0, 1.5)\nsims <- bivariate_normal(theta, 1000)\nxs <- as.matrix(sims)\nggplot(sims, aes(x, y)) + \n  geom_point()\n\n\nThe random variables in the model can be written as:\n\\[p(y,\\mu, \\Sigma) = p(\\mu)p(\\Sigma)\\prod_{i=1}^N\\mathcal{N}(y_i;\\mu, \\Sigma)\\]\nThe covariance matrix is diagonal, hence the log-likelihood can be written as the sum of two univariate normal distributions:\n\\[\\log p(y|\\mu, \\Sigma) = \\sum_{j=1}^2\\left(-\\frac{N}{2}\\log(2\\pi\\sigma_j^2) - \\frac{1}{2\\sigma_j^2}\\sum_{i=1}^N(y_{ij}-\\mu_j)^2\\right)\\]\n\n\nlog_likelihood <- function(xs, theta) {\n  apply(xs, 1, function(x) dnorm(x[1], mean = theta[1], sd = theta[2], log = T) + \n          dnorm(x[2], mean = theta[3], sd = theta[4], log = T)) %>% sum()\n}\n\nThe prior distributions are chosen to be:\n\\[\\begin{align}\np(\\mu_j) &= \\mathcal{N}(0, 3), \\\\\np(\\Sigma_{jj}) &= \\textrm{Gamma}(3, 3), \\quad j = 1, 2.\n\\end{align}\\]\n\n\nlog_prior <- function(theta) {\n  dnorm(theta[1], log = T) + \n    dnorm(theta[3], log = T) + \n    dgamma(theta[2], shape = 3.0, rate = 3.0, log = T) + \n    dgamma(theta[4], shape = 3.0, rate = 3.0, log = T)\n}\nlog_posterior <- function(xs) \n  function(theta) \n    log_likelihood(xs, theta) + log_prior(theta)\n\nMetropolis-Hastings algorithm\nA Metropolis-Hastings algorithm can be used to determine the posterior distribution of the parameters, \\(theta = \\{\\mu, \\Sigma\\}\\). The Metropolis algorithm constructs a Markov chain whose stationary distribution corresponds to the target posterior distribution, \\(p(\\theta|y)\\). In order to construct the Markov chain with this property, a carefully chosen tansition function \\(P(\\theta^\\prime|\\theta)\\) is used. In order to prove the Metropolis algorithm has the target distribution as its stationary distribution, the existence and uniqueness of the stationary distribution must be determined. A transition function which satisfies detailed balance is chosen which is a sufficient condition for the existence of the stationary distribution:\n\\[P(\\theta^\\prime|\\theta)p(\\theta|y) = P(\\theta|\\theta^\\prime)p(\\theta^\\prime|y)\\]\nThe Markov chain proceeds by proposing a new value of the parameters, \\(\\theta^\\prime\\) from a distribution which can be easily simulated from (typically a Normal distribution centred at the previously accepted value of the parameter, \\(\\theta\\)), \\(q(\\theta^\\prime|\\theta)\\). The transition function is the product of the proposal distribution and the acceptance ratio. The acceptance ration which satisfies detailed balance is called the Metropolis choice:\n\\[A = \\operatorname{min}\\left(1, \\frac{p(\\theta^\\prime|y)q(\\theta|\\theta^\\prime)}{p(\\theta|y)q(\\theta^\\prime|\\theta)}\\right).\\]\nR Implementation\nFirst of a single step of the Metropolis algorithm is implementated. This is a higher order function, since two of the arguments are functions themselves. The function log_posterior is a function from parameters to log-likelihood and the proposal is a symmetric proposal distribution for the parameters, a function from parameters to parameters. The final argument, theta represents the parameters.\n\n\nmetropolis_step <- function(theta, log_posterior, proposal) {\n  propTheta <- proposal(theta)\n  a <- log_posterior(propTheta) - log_posterior(theta)\n  u <- runif(1)\n  if (log(u) < a) {\n    propTheta\n  } else {\n    theta\n  }\n}\n\nNext the step function can be used in a for loop to generate m samples, each dependent on the previous step. An matrix containing \\(m\\) rows is initialised to contain each iteration of the Metropolis algorithm.\n\n\nmetropolis <- function(theta, log_posterior, proposal, m) {\n  out = matrix(NA_real_, nrow = m, ncol = length(theta))\n  out[1, ] = theta\n  for (i in 2:m) {\n    out[i, ] <- metropolis_step(out[i-1, ], log_posterior, proposal)\n  }\n  out\n}\n\nThe strictly positive variance parameters are proposed on the log-scale:\n\n\nproposal <- function(x) {\n  z = rnorm(4, sd = 0.05)\n  c(x[1] + z[1], x[2] * exp(z[2]),\n    x[3] + z[3], x[4] * exp(z[4]))\n}\n\nFinally, all the components are there to sample from the posterior distribution of the parameters. The mean of the sampled posterior distribution should coincide with the parameters used to simulate the data. In the figure below the actual values used to simulate the data are plotted with dashed lines.\n\n\nout = metropolis(theta, log_posterior(xs), proposal, 10000)\n\n\n\n\nParallel Chains in R\nTypically, multiple chains are run in parallel, a straightforward way to do this in R is to use a parallel map from the furrr package. First we create a new function which alters the metropolis function to return a dataframe:\n\n\nmetropolis_df <- function(theta, log_posterior, proposal, m, parameter_names) {\n  function(x) {\n    mat <- metropolis(theta, log_posterior, proposal, m)\n    colnames(mat) <- parameter_names\n    as.data.frame(mat)\n  }\n}\n\nThen future_map_dfr is used which performs the function .f for each element of .x. It then rowbinds into a dataframe. This is explicit in the function name, the suffix _dfr meaning a dataframe is the return type and is created by rowbinding the results. The id of each function run is provided by the .id column and takes on the values of .x.\n\n\nplan(multiprocess)\nmh_samples <- future_map_dfr(\n  .x = 1:2,\n  .f = metropolis_df(theta, log_posterior(xs), proposal, 10000, actual_values$parameter),\n  .id = \"chain\"\n)\n\nThe figure below shows the trace plots and marginal densities from 10,000 draws of the parallel Metropolis hastings algorithm.\n\n\n\nRcpp implementation\nR has a straightforward interface to C++, the Metropolis-Hastings algorithm can be re-implemented using C++. C++ is a statically typed imperative language, hopefully the effort of reimplementing in C++ will result in a significant speed-up. The log_posterior and proposal functions are run many times to calculate the Markov chain. Let’s first implement these two functions using C++:\n\n\n#include <Rcpp.h>\nusing namespace Rcpp;\n// [[Rcpp::plugins(cpp11)]]\n\n// [[Rcpp::export]]\ndouble logDensity(NumericMatrix ys, NumericVector p) {\n  double ll = 0;\n  int n = ys.nrow();\n  for (int i = 0; i < n; i++) {\n    ll += R::dnorm(ys(i, 0), p(0), p(1), true) + R::dnorm(ys(i, 1), p(2), p(3), true);\n  }\n  return ll;\n}\n\n// [[Rcpp::export]]\nNumericVector proposalCpp(NumericVector p, double delta) {\n  int d = p.size();\n  NumericVector z(d);\n  NumericVector propP(d);\n  for (int i = 0; i < d; i++) {\n    propP(i) = p(i);\n    z(i) = R::rnorm(0, delta);\n  }\n  propP(0) += z(0);\n  propP(1) *= exp(z(1));\n  propP(2) += z(2);\n  propP(3) *= exp(z(3));\n  return propP;\n}\n\nThese functions can then be used in the Metropolis algorithm written using R, as we can see from the below code chunk the C++ function appears as if it was an R function.\n\n\nout_cpp <- metropolis(theta, function(p) logDensity(xs, p), function(p) proposalCpp(p, 0.05), 10000)\n\n\n\n\nPerformance Improvements\nBut what about the performance, the relative speedup can be calculated using the bench package. The plot below shows the absolute timings of the R implementation and the Rcpp implementation.\n\n\ntimings <-\n  bench::mark(\n    R = metropolis(theta, log_posterior(xs), proposal, 100),\n    Rcpp = metropolis(theta, function(p)\n      logDensity(xs, p),\n      function(p)\n        proposalCpp(p, 0.05), 100),\n    iterations = 500,\n    check = FALSE\n  )\n\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2019-02-11-metropolis_r/distill-preview.png",
    "last_modified": "2020-07-30T11:56:45+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2017-10-26-harrier-league-cross-country/",
    "title": "Harrier League Cross Country",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2017-10-26",
    "categories": [
      "R"
    ],
    "contents": "\nThe Harrier League is a cross country running league with seven fixtures across the North East of England in the 2017/18 season across the winter months from September ’17 until March ’18.\nThe Harrier League is unique to other cross country fixtures because the senior runners are divided up into slow, medium and fast packs. In the senior men’s race, the slow runners start first followed 2 minutes 30 seconds later by the medium pack runners, then a further 2 minutes 30 seconds by the fast pack runners.\nIn order to progress to the fast pack, runners must first run in the slow pack and finish in the top 10% of finishers, this entitles them to run from the medium pack. If they then finish in the top 10% from the medium pack, then they can run from the fast pack.\nTeams are split into three divisions and final team positions are calculated within each division by ordering the first six runners (four in the senior women) by race time. This is the total time elapsed from when the slow pack starts to when the runner crosses the finish line. Let’s answer one interesting question, what would the results be from the past weekends fixture without the handicaps, i.e. if everyone started from scratch.\nThe code for this analysis was written using R and the tidyverse. The full code can be found on Github. Let me know if you’d like me to answer other questions about the harrier league.\nSenior Womens\nWe will look at the results from the previous race at Druridge Bay. In the senior women’s race there are four counters, any team with less than four runners will be removed from the final results. Now, we can calculate the position of each runner within each division by the actual running time of each runner.\nFirst we need to download the relevant information, since the results are available as an HTML table, the package htmltab can be used to select the table containing the individual results\n\n\n\nThese results are then cleaned up, removing any guest runners and extracting the division of each runner from the club field.\n\n\n\nNow we write a function which calculates the results for one division. This can be re-used for the men’s results by accepting the number of counters for each team as an argument to the function (4 women to count, 6 men to count). We remove any clubs who fielded incomplete teams and select the top counters ordered by the position supplied in the raw_results dataframe. We then calculate the total points for each time, by summing the positions (lower is better) and concatenate the names of the counters into a single string.\n\n\n\nNow, we are ready to calculate the results given positions. But instead of using the race time (actual time + handicap time), we use the actual time to determine individual positions within each of the three divisions. We then use the previously defined function get_results_one_division() to calculate the final score for each team in each division:\n\n\n\nHere are the results for the women’s division one, two and three:\n\n(#tab:final_results_division_one)Senior Women’s Division One\nposition\nclub\ntotal_points\ncounters\n1\nTyne Bridge Harriers\n43\nAlison Dargie (3), Louise Rodgers (5), Heather Dorman (18), Mairi Clancy (17)\n2\nNorth Shields Poly\n63\nRachel Mcintyre (15), Joanna Fletcher (26), Alison Smith (10), Katherine Davis (12)\n3\nTynedale Harriers\n65\nElizabeth Earle (24), Steph Scott (28), Kirstie Anderson (6), Jo Sutton (7)\n4\nGateshead Harriers\n66\nJemma Louise Bell (13), Sarah Holmes (36), Sarah Hill (8), Elizabeth Raven (9)\n5\nMorpeth Harriers & AC\n71\nEmma Holt (1), Jane Hodgson (2), Alison Brown (11), Sue Smith (57)\n6\nSouth Shields Harriers\n89\nLinda Bone (28), Rachel Breheny (16), Fran Dembele (26), Claire O’Callaghan (19)\n7\nAlnwick Harriers\n93\nJohanna Gascoigne-Owens (4), Lisa Baston (33), Alice Tetley-Paul (34), Karen Kelly (22)\n8\nGosforth Harriers\n111\nLouise Watson (20), Angela Richardson (31), Sylvie Moffat (21), Susan Driscoll (39)\n9\nDurham City Harriers\n117\nKelly Bentley (25), Gemma Soulsby (14), Rachel Terry (40), Rachel Bentley (38)\n10\nElvet Striders\n186\nSusan Davis (41), Rachelle Mason (44), Sarah Davies (50), Rachael Bullock (51)\n\n\n(#tab:division_two_women)Senior Women’s Division Two\nposition\nclub\ntotal_points\ncounters\n1\nHeaton Harriers\n31\nNina Cameron (8), Louise Johnson (10), Ellen Roberts (2), Janine Routledge (11)\n2\nJesmond Joggers\n49\nEmma Glover (4), Laura Cheetham (5), Jo King (18), Kate Black (22)\n3\nCrook AC\n53\nKirstin Farquhar (6), Helene Pratt (32), Elizabeth Wood (12), Joanne Raine (3)\n4\nSunderland Harriers\n88\nLinda Mudford (24), Lauren Flaxen (26), Vikki Cotton (9), Peter Richardson (29)\n5\nElswick Harriers\n89\nElspeth Aspinall (7), Catherine Lee (13), Graham Leslie (33), Jill Bennett (36)\n6\nClaremont RR\n94\nMandy Herworth (20), Sarah Kerr (25), Nicki O’Brien (14), Jane Evans (35)\n6\nLow Fell RC\n94\nEmma Sproat (16), Claire Diamond-Howe (17), Cheryl Parkin (19), William Doidge (42)\n8\nBirtley AC\n121\nChloe Price (1), Clare Atkinson (31), Shannel Curtis (37), Stephanie Young (52)\n9\nBlackhill\n152\nLouise Priestley (30), Caroline Slane (34), Kerry Anderson (40), Juliet Robinson (48)\n10\nWallsend Harriers\n154\nDonna Thompson (15), Emily James (28), Julie Williams (47), Julie Collinson (64)\n\n\n(#tab:division_three_women)Senior Women’s Division Three\nposition\nclub\ntotal_points\ncounters\n1\nSaltwell Harriers\n46\nChristie Waddington (4), Gemma Bradley (3), Sarah Garrett (12), Nicola Whitman (27)\n2\nSunderland Strollers\n70\nWendy Chapman (2), Clare Baharie (14), Hasina Khanom (16), Michelle Houghton (38)\n3\nPonteland Runners\n72\nElaine Stroud (15), Fiona Nicholson (18), Alison Guadagno (19), Laura Choake (20)\n4\nBlyth RC\n76\nClaire Calverley (8), Holly Johnson (21), Lisa Scorer (23), Sandra Watson (24)\n5\nBlaydon Harriers\n88\nClaire Collinson (10), Liane Brown (25), Joanne Ramshaw (26), Izzi Jackson (27)\n6\nDerwent Valley Running Club\n93\nLindsay Smith (5), Dawn Cooper (13), Eleanor Shotton (22), Bernadette Salmon (53)\n7\nJarrow & Hebburn AC\n94\nRachel Jameson (9), Elaine Leslie (1), Heather Robinson (7), Julia Barnshaw (77)\n8\nDerwentside AC\n171\nEmma Armstrong (31), Fiona Gilchrist (40), Claire Wilkie (44), Janet Brooks (56)\n9\nWashington Running Club\n174\nKatherine Conway (30), Sarah Turnbull (42), Radina Tahtadzhieva (47), Nicole Clarke (55)\n10\nAshington Hirst\n192\nLee Elder (34), Kate Mclean (41), Laura Jobson (58), Lindsay Freeman (59)\n11\nDerwent Valley Trail Runners\n226\nDawn Metcalfe (17), Susie Thompson (37), Vicky Hopper (72), Kathleen Boyle (100)\n12\nNewcastle Frontrunners\n309\nHeather Witham (52), Gillian Dodds (70), Grace Lewis (89), Jo Morrissey (98)\n13\nStocksfield Striders\n353\nRachael Maitland (49), Holly Kelleher (71), Louise Newton (116), Jane Buckingham (117)\n\nSenior Men\nIn the senior men’s race there are six counters, any team with less than six runners will be removed from the final results. Now, we can calculate the position of each runner within each division by the actual running time of each runner.\n\n\n\n\n(#tab:division_one_men)Senior Men’s Division One\nposition\nclub\ntotal_points\ncounters\n1\nTyne Bridge Harriers\n89\nPaul O’Mara (9), James Dunce (2), Tom Charlton (4), Alasdair Blain (26), Tony Carter (21), Paul Turnbull (27)\n2\nDurham City Harriers\n111\nAlexander Cook (22), Jonathan Wilkinson (8), Francisco Martinez-Sevilla (16), Michael Wade (17), David Cross (36), Robin Linten (12)\n3\nMorpeth Harriers & AC\n129\nRobert Balmbra (1), Thomas Innes (23), Tony Lewis (37), Richard Castledine (39), Alistair Douglass (14), Mark Snowball (15)\n4\nSunderland Harriers\n149\nSean Mackie (31), Paul Blakey (10), Andrew Powell (3), Robert Walker (20), Steven Duffy (33), Paul Merrison (52)\n5\nGateshead Harriers\n159\nMatthew Linsley (43), Ross Christie (13), Conrad Franks (7), Steven Asquith (31), Kevin Connolly (18), Peter Grimoldby (47)\n6\nHeaton Harriers\n188\nMatt Hetherington (19), Sam Thorpe (30), James Meader (24), James Mckenzie (11), Mark Oliver (50), Ian Norman (54)\n7\nElvet Striders\n255\nStephen Jackson (5), Jack Lee (57), Jason Harding (34), Michael Mason (25), Phil Ray (66), Scott Watson (68)\n8\nBirtley AC\n348\nAdrian Bailes (6), Peter Farnie (40), Karl Oakes (79), Peter Gill (44), Mark Hornsby (87), Trevor Crewe (92)\n9\nNorth Shields Poly\n374\nPaul West (29), Michael Parkinson (27), Michael Gibson (78), Paul Davies (49), William Powis (95), Richard Hanley (96)\n10\nWallsend Harriers\n445\nSimon Lyon (35), David Diston (76), Jack Armstrong (57), Brian Hetherington (59), Keith Odonnell (108), Paul James (110)\n\n\n(#tab:division_two_men)Senior Men’s Division Two\nposition\nclub\ntotal_points\ncounters\n1\nJarrow & Hebburn AC\n90\nAndy Burn (1), Jonny Evans (3), Kevin Emmett (20), Jack Brown (25), Jonathan Gilroy (14), Conal Tuffnell (27)\n2\nAlnwick Harriers\n122\nDan Turnbull (5), Adam Fletcher (22), Steve Carragher (10), Ian Simon (34), Dominic Harris (21), Philip Hemsley (30)\n3\nGosforth Harriers\n132\nTom Coates (18), Maurice Bourke (31), Andrew Heppell (15), Jonny Stephenson (24), James McCreesh (13), Neil Ramsay (31)\n4\nElswick Harriers\n176\nIain Hardy (40), Lee Bennett (7), John Bell (43), Mark Turnbull (19), Kevin Richardson (11), David Armstrong (56)\n5\nSaltwell Harriers\n181\nGraham Stephenson (28), Iain Armstrong (12), Fred Smith (44), Jim Thompson (16), Matt O’Brien (16), Peter Mullarkey (65)\n6\nSouth Shields Harriers\n214\nLuke Adams (2), Jeff Mcgurty (40), Paul Owen (45), Neil Turner (46), Stephen Mackin (53), Mark Wilson (28)\n7\nBlyth RC\n219\nGraeme Stewart (36), Tony Horsley (38), Gary Jones (8), Graham Wood (51), Calum Storey (22), Paul Whalley (64)\n8\nSunderland Strollers\n253\nCallum Thom (9), Luke Mccormack (49), Ritchie Gerry (26), Paul Dunlop (66), Michael Dixon (68), Ken Maynard (35)\n9\nSedgefield Harriers\n311\nJames Oldfield (4), David Bentley (42), Mark Raine (48), David Walker (56), Mil Walton (77), Chris Lines (84)\n10\nBlackhill\n319\nJordan Bell (6), Ian Young (55), Gary Dixon (60), Jonathan Richards (68), Daryl Priestley (79), Michael Mcdonald (51)\n11\nCrook AC\n847\nPaul Brennan (90), Geoff Hewitson (98), Mark Nichol (148), Gerry Hehir (168), Lloyd Ashby (169), Paul Wragg (174)\n\n\n(#tab:division_three_men)Senior Men’s Division Three\nposition\nclub\ntotal_points\ncounters\n1\nLow Fell RC\n77\nGavin Thompson (13), David France (3), Ian Marriott (23), Fai Ng (25), Paul Harrison (9), Stephen Magrath (4)\n2\nHoughton AC\n80\nJuma Tatah (14), Stephen Johnson (21), Lee Dover (2), Adam Middleton (16), Tom Whelan (17), Thomas Grey (10)\n3\nBlaydon Harriers\n140\nDavid Garner (11), Isaac Dunn (5), James Ramshaw (30), James Dias (7), Jamie Boswell (41), Liam Friel (46)\n4\nDerwentside AC\n201\nGraham Marshall (18), Mark Davinson (29), David Reay (34), John S Donneky (8), Steven Dickson (27), Chris Lowes (85)\n5\nJesmond Joggers\n267\nJosh Freed (19), Angus Miller (33), Stuart Harper (43), Gregory Stamp (55), Tim Mcgahey (58), John Farr (59)\n6\nPonteland Runners\n268\nJohn Mcgargill (47), James Leiper (15), Chris Kenyon (56), Tim Allsop (20), Matthew Levison (62), Matty Bell (68)\n7\nAshington Hirst\n271\nKurt Heron (1), Paul White (6), Iain Singer (52), Martin Thompson (59), Philip Battista (74), Nic Crofts (79)\n8\nWashington Running Club\n302\nPeter Setterfield (24), Tim Jones (28), Craig Smith (50), Nick Butchart (51), Carl Smith (66), David Bannan (83)\n9\nClaremont RR\n391\nRoberto Marzo (22), Anthony Liddle (39), Danny Edwards (70), Duncan Scott (72), David Devennie (93), Dean O’Brien (95)\n10\nDerwent Valley Trail Runners\n445\nStephen Heseltine (38), Ian Hutchinson (49), Andrew Nesbit (76), Jordan Babak (91), Tony Curry (93), Dan English (98)\n11\nDerwent Valley Running Club\n553\nMark Marchant (35), Simon Woolley (79), Peter Storey (100), Nick Belcher (105), John Kirby (113), Steve Wade (121)\n12\nNewcastle Frontrunners\n587\nRussell Dickinson Deane (32), Phillip Hall (67), Mark Sutherland (75), Curtis Allen (123), Ken Hodson (144), Allen Dickinson Deane (146)\n\n\n\n",
    "preview": {},
    "last_modified": "2020-07-30T11:43:29+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-04-23-BreezeMcmc/",
    "title": "MCMC with Scala Breeze",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2017-04-23",
    "categories": [
      "Scala",
      "Bayesian"
    ],
    "contents": "\nBivariate Gaussian Model\nScala Breeze is a numerical computing library, which also provides facilities for statistical computing. For instance, implementations of distributions and Markov chain Monte Carlo (MCMC), which can be used for solving the integrals required in Bayesian modelling. In this post, I am going to simulate data from a bivariate Gaussian model and use the Scala Breeze library to recover the mean and the variance of the bivariate Gaussian distribution.\nThe model can be written as\n\\[ \\begin{pmatrix}X_1 \\\\ X_2\\end{pmatrix} \\sim \\textrm{MVN}\n  \\begin{pmatrix}\n    \\begin{pmatrix}\\mu_1 \\\\ \\mu_2 \\end{pmatrix}, \n    \\begin{pmatrix} \\sigma & 0 \\\\ 0 & \\sigma \\end{pmatrix} \n  \\end{pmatrix} \\]\nThe model has three parameters, the mean of each variable and the variance which is shared. \\(X_1\\) and \\(X_2\\) are independent and hence can be simulated from separate univariate Gaussian distributions:\nimport breeze.stats.distributions._\nimport breeze.linalg._\n\ncase class Parameters(mu: DenseVector[Double], sigma: Double)\n\ndef model(params: Parameters) = \n  MultivariateGaussian(params.mu, diag(DenseVector.fill(2)(params.sigma)))\nA simulation from the bivariate Gaussian model is plotted below, the mean for x is 2.0, the mean for y is 3.0 and the variance for each dimension is 0.5.\nval p = Parameters(DenseVector(2.0, 3.0), 0.5)\nval data = model(p).sample(100)\n\n\n\nIt is simple to write a function to calculate the log-likelihood of this model:\ndef likelihood(points: Seq[DenseVector[Double]])(p: Parameters) =\n    points.map { point => \n      MultivariateGaussian(p.mu, diag(DenseVector.fill(2)(p.sigma))).logPdf(point)\n    }.reduce((x, y) => x + y)\nWe take a sequence of observations, called points, since we know each point is simulated independently from the same distribution, then we simple map over the sequence of points the likelihood using the supplied value of the Parameters. The reduce operation then applies a pairwise function to each element of the list, in this case addition to get the value of the log-likelihood.\nFor a full Bayesian inference, we must specify a prior distribution on the parameters, let’s choose a multivariate Gaussian distribution on the mean and a Gamma distribution for the precision (the inverse of the variance, \\(\\tau = 1/\\sigma^2\\)). The Gamma distribution in Breeze is parameterised in terms of shape and scale, the mean of the Gamma distribution with shape \\(k = 1/2\\) and scale \\(\\theta = 2\\) is \\(k\\theta = 1 = 1/\\sigma^2\\):\ndef prior(p: Parameters) = {\n  MultivariateGaussian(DenseVector(2.0, 3.0), diag(DenseVector.fill(2)(3.0))).logPdf(p.mu) +\n    Gamma(shape = 0.5, scale = 2.0).logPdf(1/(p.sigma * p.sigma))\n}\nThe posterior distribution is proportional to the prior times the likelihood:\n\\[p(\\theta | x) \\propto p(x | \\theta) p(\\theta)\\]\nWe can define the un-normalised log-posterior in Scala\ndef logMeasure = (p: Parameters) => likelihood(data)(p) + prior(p)\nThe computational challenge for Bayesian inference is to determine the normalising constant for the posterior distribution. The full Bayes’ theorem is specified as:\n\\[p(\\theta | x) = \\frac{p(x | \\theta) p(\\theta)}{\\int_\\theta p(x|\\theta)p(\\theta)d\\theta}\\]\nAs we can see from the full equation, the normalising constant is an integral. This integral is typically intractable for complex problems. However, we can construct a Markov chain with a stationary distribution equal to the posterior.\nThe Markov chain Monte Carlo method we will be using is a Metropolis-Hastings algorithm with a symmetric random walk proposal. First, we propose a new value of the parameters, \\(\\theta^*\\) from the parameter proposal distribution, then we accept them with probability \\(\\min(1, A)\\), where \\(A\\) is:\n\\[A = \\frac{p(x|\\theta^*)p(\\theta^*)}{p(x|\\theta)p(\\theta)}\\]\nSo if the likelihood multiplied by the prior is larger at the proposed value of the parameters than the previous value, we always accept, otherwise, we may reject. In this way, we can explore the parameter space. In a well tuned sampler, the algorithm will not accept every proposed value of the parameters, otherwise we are NOT exploring the whole of the parameter posterior, just areas of high posterior density. In this case we can increase the variance of the proposal distribution to get the acceptance rate down to approximately 30-40%. A random walk proposal function can be written in Scala\nimport breeze.numerics.exp\n\ndef propose(scale: Double)(p: Parameters) = \n  for {\n    innov <- MultivariateGaussian(DenseVector.fill(3)(0.0), diag(DenseVector.fill(3)(scale)))\n    mu = p.mu + innov(0 to 1)\n    sigma = p.sigma * exp(innov(2))\n  } yield Parameters(mu, sigma)\nHere, the value of sigma is proposed on the log-scale, since sigma is expected to be positive. Now, we have all we need to build the sampler using breeze:\nMarkovChain.metropolis(p, propose(0.05))(logMeasure)\n\n\n\nThe full code required to run the MCMC in Breeze can be found in this gist. Note that Breeze is a required dependency.\n\n\n\n",
    "preview": "posts/2017-04-23-BreezeMcmc/BreezeMcmc_files/figure-html5/bivariate-normal-plot-1.png",
    "last_modified": "2021-06-22T10:37:27+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2017-02-21-AkkaClient/",
    "title": "An Akka HTTP Client with JSON Parsing",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2017-02-21",
    "categories": [
      "Scala"
    ],
    "contents": "\nThere are many sources of open data on the web, freely accessible via an Application Programming Interface (API) made available over the web. A common interchange format for these APIs is Javascript Object Notation (JSON) which is human readable and predictable, however is not in the correct format for analysis. The data needs to be parsed from the JSON string and made available as an object we can work with. This blog post considers a simple Akka Http client to read data from the Urban Observatory in Newcastle. If you just want to read the code, see this Gist.\nExploring the API\nThe Urban Observatory consists of a grid of sensors around the North East, measuring traffic, pollution and weather. The focus of this post will be getting sensor data from a single sensor, N05171T, a traffic sensor located near the Metro Centre on Hollinside road. The metadata from this sensor can be found by querying an API endpoint: http://uoweb1.ncl.ac.uk/api/v1/sensor.json?api_key=&sensor_name=N05171T. Note that this requires authentication, using an API key. An API key can be requested using this form.\nThe result of this query is:\n\n{\n    \"type\": \"Traffic\",\n    \"geom\": {\n        \"type\": \"LineString\",\n        \"coordinates\": [\n            [\n                -1.674433347,\n                54.959041883\n            ],\n            [\n                -1.673947928,\n                54.959553799\n            ]\n        ]\n    },\n    \"active\": \"True\",\n    \"latest\": \"2017-02-21T08:23:06\",\n    \"base_height\": null,\n    \"sensor_height\": null,\n    \"name\": \"N05171T\",\n    \"source\": {\n        \"web_display_name\": \"NE Travel Data API (Third Party)\",\n        \"third_party\": true,\n        \"db_name\": \"Scoot Netravel Api\",\n        \"document\": \"\",\n        \"fancy_name\": \"NE Travel Data API\"\n    }\n}\n \n\nWe can see a bit of information about the sensor, including its location, time of latest reading and whether the sensor is active.\nIn order to retrieve the actual data from the sensor, we query the url http://uoweb1.ncl.ac.uk/api/v1/sensor/data/raw.json with the following required fields:\napi_key your API key here\nsensor_name N05171T\nstart_time 20170201\nend_time 20170202\nThis returns the following:\n\n{\n    \"type\": \"Traffic\",\n    \"geom\": {\n        \"type\": \"LineString\",\n        \"coordinates\": [\n            [\n                -1.674433347,\n                54.959041883\n            ],\n            [\n                -1.673947928,\n                54.959553799\n            ]\n        ]\n    },\n    \"active\": \"True\",\n    \"data\": {\n        \"Congestion\": {\n            \"data\": {\n                \"2017-02-01 01:36:46\": 0.0\n            },\n            \"meta\": {\n                \"name\": \"Congestion\",\n                \"theme\": \"Traffic\",\n                \"units\": \"%\"\n            }\n        },\n        \"Traffic Flow\": {\n            \"data\": {\n                \"2017-02-01 01:36:46\": 4.0\n            },\n            \"meta\": {\n                \"name\": \"Traffic Flow\",\n                \"theme\": \"Traffic\",\n                \"units\": \"Passenger Car Units\"\n            }\n        },\n        \"Average Speed\": {\n            \"data\": {\n                \"2017-02-01 13:21:46\": 35.0\n            },\n            \"meta\": {\n                \"name\": \"Average Speed\",\n                \"theme\": \"Traffic\",\n                \"units\": \"KmPH\"\n            }\n        }\n    },\n    \"latest\": \"2017-02-21T08:33:06\",\n    \"base_height\": null,\n    \"sensor_height\": null,\n    \"name\": \"N05171T\",\n    \"source\": {\n        \"web_display_name\": \"NE Travel Data API (Third Party)\",\n        \"third_party\": true,\n        \"db_name\": \"Scoot Netravel Api\",\n        \"document\": \"\",\n        \"fancy_name\": \"NE Travel Data API\"\n    }\n}\n \n\nAll but one of the readings have been stripped for each data object to emphasize the structure of the JSON returned by the API. We can see that sensor N05171T records traffic flow, congestion, and average speed. We can provide a further (optional) field to the Urban Observatory API to limit the results to return only one these. Let’s consider the only the average speed, measures in kmph. This is an unusual unit for the UK, as road speed is measured in miles per hour.\nParsing the JSON in Scala\nThere are many JSON parsing libraries in Scala, including my favourite Circe which is a Typelevel project, providing generic parsers for case classes without additional boilerplate. However, Spray JSON and Akka HTTP work well together, so that is what we will be using today. In order to complete this tutorial, you will need the Akka HTTP and Spray JSON dependencies in your build.sbt file.\nlibraryDependencies ++=  Seq(\n  \"com.typesafe.akka\" %% \"akka-stream\" % \"2.4.17\",\n  \"com.typesafe.akka\" %% \"akka-http\" % \"10.0.3\",\n  \"com.typesafe.akka\" %% \"akka-http-spray-json\" % \"10.0.0\")\nFirstly, we describe the data we are interested in, in a collection of case classes representing the JSON data:\ncase class Sensor(name: String, data: SensorData)\ncase class SensorData(averageSpeed: AverageSpeed)\ncase class AverageSpeed(meta: Meta, data: Map[String, Double])\ncase class Meta(units: String, theme: String, name: String)\nWe extract the sensor name, and associated data, without bothering with the additional top-level fields. The sensor data field contains only average speed, by appending &variable=average speed to the end of the HTTP Get request.\nNow we have a domain model for the sensor data, we must provide a way for Spray JSON to parse the JSON to the case classes:\nimport spray.json._\nimport akka.http.scaladsl.marshallers.sprayjson.SprayJsonSupport\n\ntrait Protocols extends SprayJsonSupport with DefaultJsonProtocol {\n  implicit val metaFormat: RootJsonFormat[Meta] = \n    jsonFormat(Meta.apply, \"units\", \"theme\", \"name\")\n   implicit val averagespeedFormat: RootJsonFormat[AverageSpeed] = \n    jsonFormat(AverageSpeed.apply, \"meta\", \"data\")\n   implicit val sensorDataFormat: RootJsonFormat[SensorData] = \n    jsonFormat(SensorData.apply, \"Average Speed\")\n  implicit val sensorFormat: RootJsonFormat[Sensor] = \n    jsonFormat(Sensor.apply, \"name\", \"data\")\n}\nThis trait can be mixed in when the time comes to parse the JSON data. Let’s first test the JSON parsing by directly reading in the JSON we get when running the request in a web browser:\nobject TestJson extends App with Protocols {\n  val json_string = scala.io.Source.fromFile(\"data/traffic_sensor.json\").getLines.mkString\n\n  json_string.\n    parseJson.\n    convertTo[List[Sensor]].\n    foreach(println)\n}\nThere’s a bit going on here, first we have a file which contains the JSON String, this could have been pasted in directly to Scala. Then the string is parsed, this is possible since the TestJson object has the JSON Protocols trait we defined earlier mixed in using with.\nMaking an API Request using Akka HTTP\nIn order to make an API request using Akka HTTP, we utilise the high-level client API based on Scala futures:\nimport akka.actor.ActorSystem\nimport akka.http.scaladsl.marshallers.sprayjson.SprayJsonSupport\nimport akka.http.scaladsl.model._\nimport akka.http.scaladsl.Http\nimport HttpMethods._\nimport akka.stream.ActorMaterializer\nimport Uri.Query\nimport scala.concurrent.Future\n\nimplicit val system = ActorSystem()\nimplicit val materializer = ActorMaterializer()\nimplicit val executionContext = system.dispatcher\n\nval uri = Uri(\"http://uoweb1.ncl.ac.uk/api/v1/sensor/data/raw.json\")\nval api_key = // your api key here\n\n  val query: Query = Query(\"api_key\" -> api_key,\n    \"sensor_name\" -> \"N05171T\",\n    \"start_time\" -> \"20170201\",\n    \"end_time\" -> \"20170202\",\n    \"variable\" -> \"average speed\")\n\nval res: Future[HttpResponse] = \n  Http().singleRequest(HttpRequest(GET, uri = uri.withQuery(query)))\nWe have created a Query object, which is just a sequence of (String, Sting), denoted using the nicer arrow syntax. The request is initialised simply as a singleRequest which retuns a Future containing the HttpResponse. The HttpResponse object contains the status (200 OK, 404 Not Found etc.), and importantly the body content, called HttpEntity, the HttpEntity in this case is simply the JSON. In order to verify we are able to make a requst to the Urban Observatory, we can match on the result of the future using andThen:\nres andThen {\n    case Success(response) => println(response)\n    case Failure(ex) => println(ex)\n  } onComplete {\n    _ => system.terminate()\n  }\nandThen expects a function from a Try, an algebraic data type (ADT) which can be either Success or Failure. When the future completes, we shutdown the Actor System required by Akka HTTP. When you run this minimal example, you should receive a response similar to:\nHttpResponse(200 OK,List(Date: Tue, 21 Feb 2017 09:27:24 GMT, Server: Apache/2.4.7 (Ubuntu), Vary\n: Cookie, X-Frame-Options: SAMEORIGIN),HttpEntity.Chunked(application/json),HttpProtocol(HTTP/1.1\n))\nThe server has returned 200 OK, some headers and some response data. In order to access the response data, we must convert it to a string, first we access the entity field of the HttpResponse, the get the results as a ByteString which can finally be parsed to a String:\nval resp = response.\n  entity.\n  dataBytes.\n  map(_.utf8String)\nThis is an Akka Stream, containing the response from the server. In order to parse it into the sensor data, we map over the string using the JSON parsing function we have previously seen:\nresp.\n  map(_.parseJson.convertTo[List[Sensor]]).\n  runForeach(println)\nThis should print the parsed data to the console. There are a variety of other Akka Sinks which can be used to consume the HttpEntity, they can be found in the Akka docs overview of built in stages.\nA complete working example is available in this Gist.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-22T10:37:25+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-01-04-FailureInFunctionalProgramming/",
    "title": "Using Monads for Handling Failures and Exceptions",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2017-01-04",
    "categories": [
      "Scala"
    ],
    "contents": "\nIn this post I will give a practical introduction to some useful structures for handling failure in functional programming.\nReferential Transparency\nOne of the most important properties of functional programming is referential transparency and programming with pure functions. This means we can substitute a pure function with its result, for intance if we have the function def f = 1 + 2, we can replace every occurence of f with 3 and the final evaluation will remain unchanged\nThis simple idea can lead to difficulties when considering functions which involve side effects, such as reading from external sources or generating random numbers. One example of a side effect is an exception, an imperative programmer might write a function to calculate a square root as:\ndef unsafe_sqrt(a: Double): Double = {\n  if (a > 0) math.sqrt(a)\n  else throw new Exception(\"Can't calculate square root of negative number\")\n}\nThis compiles fine, however if we wrote this function for an end user and they didn’t look at the implementation they might not know the function can possibly return an exception.\nTry\nIn order to make it clear that a function can fail, we can return a Try:\ndef try_sqrt(a: Double): Try[Double] = {\n  if (a > 0) Success(math.sqrt(a))\n  else Failure(throw new Exception(\"Can't calculate square root of negative number\"))\n}\nNow, if someone were to use this function they would be forced to deal with the Try return type and understand that the function can return an exception. Try is actually an algebraic datatype (ADT), an illustrative implementation is:\nsealed trait Try[+A]\ncase class Success[+A](a: A) extends Try[A]\ncase class Failure[+A](exception: Throwable) extends Try[A]\nThis means that a Try can either be a Success or Failure. Learn more about Try in Daniel Westheide’s excellent Neophyte’s Guide to Scala.\nOption\nAnother simple structure to represent computations which may fail is Option, this is an algebraic datatype:\nsealed trait Option[+A]\ncase class Some[+A](a: A) extends Option[A]\ncase class None extends Option[Nothing]\nIn this case, option can either contain a value using the constructor Some, or can represent the absense of a value using None. This provides less information on failure that Try, but nevertheless is sometimes useful. We can re-write the sqrt function to return an optional value\ndef option_sqrt(a: Double): Option[Double] = {\n  if (a > 0) Some(math.sqrt(a))\n  else None\n}\nNow, when provide an incorrect argument to the function, we get None as the result.\nChaining Computations\nIn real world functional codebases we compose programs from many small functions. Let’s consider the problem of how to apply def sqrt(a: Double): Option[Double] twice. A naive attempt would be to simply compose the functions as we would for the Scala math library:\ndef sqrt_twice = unsafe_sqrt _ compose unsafe_sqrt _\nThe _ represents partial application of unsafe_sqrt, if we try to compose the option_sqrt function as in this example we will get a type mismatch. One application of option_sqrt returns a typle Option[Double], but we need the type Double. Luckily Option has a function defined on it for composing operations like this:\ndef flatMap[A, B](a: Option[A])(f: A => Option[B]): Option[B]\nWe can now use flatMap to compose option_sqrt:\ndef sqrt_twice_option(x: Double): Option[Double] = \n  option_sqrt(x) flatMap option_sqrt\nNow we can calculate sqrt_twice_option(81) = Some(3.0).\nWe can compose try_sqrt in the same way:\ndef sqrt_twice_try(x: Double): Try[Double] = \n  try_sqrt(81) flatMap try_sqrt\nNow, what if we want to compose option_sqrt and try_sqrt. This is not an easy problem in general, however the Scala standard library implements a toOption method on Try values. Hence we can just convert the output of try_sqrt to an Option, however we lose the text from the exception upon failure, which could illuminating in the event of a failure. Let’s consider a more general way to compose the two.\nNested Maps\nOption and Try are both monads (strictly Try is not a proper monad), which means they are equipped with two methods which satisfy the monad laws. The two methods defined for all monads are:\ntrait Monad[A, M[_]] {\n  def flatMap[B](f: A => M[B]): M[B]\n  def pure(a: A): M[A]\n}\nWe can define all the functions on Try and Option using these two functions, for instance map:\ndef map[B](f: A => B): M[B] = this.flatMap(a => pure(f(a)))\nNow, we can use the map function to compose option_sqrt and try_sqrt:\ndef sqrt_twice(a: Double): Try[Option[Double]] = try_sqrt(a) map option_sqrt\nHowever, what if we want to apply another function to a value returned by this function:\ndef f(a: Double) = a + 1\nsqrt_twice(81) map (_.map(f))\n// Success(Some(4.0))\nWe get the correct value, but we have to apply map twice, this seems cumbersome. There is a better way!\nMonad Transformers\nThe functional programming library cats, short for category, has some built in types for dealing with nesting in a more elegent way. The type OptionT[F[_], A] can be used instead of F[Option[A]], our F[_] type in this case is Try[A]\nimport cats.implicits._\nimport cats.data.OptionT\n\ndef sqrt_twice_trans(a: Double): OptionT[Try, Double] = \n  OptionT.fromOption[Try](option_sqrt(a)) flatMap (b => OptionT.liftF(try_sqrt(b))\nOptionT provides the function fromOption to transform the result of the option_sqrt function into the OptionT monad. The function liftF is used to lift any monad, in this case Try into the OptionT monad. This compiles and we if we now try to apply the function def f(a: Double) = a + 1 to the result of this function we only need a single call to map. This is because OptionT is also a monad:\nsqrt_twice_trans(81) map f\n// OptionT(Success(Some(4.0)))\nThis may seem like quite a lot of effort to remove a call to map, but removing unecessary duplication can help with readability of code, and enable bugs to be spotted earlier. The code has been assembled in a Github gist.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-22T10:37:24+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-12-13-SeasonalDlm/",
    "title": "Seasonal DLM",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2016-12-13",
    "categories": [
      "Bayesian",
      "Scala"
    ],
    "contents": "\nThe Seasonal DLM\nI introduced the class of state space models called DLMs in a previous post covering the Kalman Filter. The seasonal DLM is similar to the first order DLM, however it incorporates a deterministic transformation to the state, in order to capture cyclic trends. Remember a general DLM can be written as:\n\\[\\begin{align}\ny_t &= F_t x_t + \\nu_t, \\qquad \\mathcal{N}(0, V_t), \\\\\nx_t &= G_t x_{t-1} + \\omega_t, \\quad \\mathcal{N}(0, W_t).\n\\end{align}\\]\nIn the fourier-form seasonal DLM, the state is transformed by a block diagonal matrix, \\(G_t\\), containing rotation matrices. The rotation matrix is given by:\n\\[R = \\begin{pmatrix}\n\\cos(\\omega) & -\\sin(\\omega) \\\\\n\\sin(\\omega) & \\cos(\\omega)\n\\end{pmatrix},\\] where \\(\\omega\\) is the frequency of the seasonality. In practice, it is easier to specify the period of the seasonality, \\(T\\), which is related to the frequency: \\(\\omega = 2 \\pi/T\\). This means if we have data measured at hourly intervals and we believe the process has a daily cycle, then we will set \\(T = 24\\).\nIn order to model higher harmonics of the seasonality, we combine rotation matrices together into a block diagonal matrix, to form the \\(G\\) matrix, for instance with 3 harmonics the system matrix is:\n\\[G = \\begin{pmatrix}\n\\cos(\\omega) & -\\sin(\\omega) & 0 & 0 & 0 & 0\\\\\n\\sin(\\omega) & \\cos(\\omega) & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\cos(2\\omega) & -\\sin(2\\omega) & 0& 0 \\\\\n0 & 0 & \\sin(2\\omega) & \\cos(2\\omega) & 0 & 0 \\\\\n0 & 0 & 0 & 0 & \\cos(3\\omega) & -\\sin(3\\omega) \\\\\n0 & 0 & 0 & 0 & \\sin(3\\omega) & \\cos(3\\omega) \\\\\n\\end{pmatrix}.\\]\nIn this case, the latent state, \\(x_t\\) is six-dimensional. This means system evolution variance-covariance matrix, \\(W_t\\) is a six by six matrix.\nFirst we should make a case class in Scala representing a DLM. A case class in Scala is a class with a default apply method used to construct instances of the class. The case class also has getter methods which can be used to access the values of the class. The case class will be for a DLM with constant \\(F\\) and \\(G\\) matrices.\nimport breeze.linalg._\n\ncase class Dlm(f: DenseMatrix[Double], g: DenseMatrix[Double])\nNote that, for univariate observations \\(F\\) will be a row vector, the recommended way to specify row vectors using Breeze is to use a matrix with a single row. The DLM also has associated parameters which we will assume are constant and write as a case class.\ncase class Parameters(v: Double, w: DenseMatrix[Double], m0: DenseVector[Double], c0: DenseMatrix[Double])\nThen we can add a method to simulate forward given a value of the Parameters, firstly we will write a single step of the simulation:\ncase class Data(time: Double, observation: Double, state: DenseVector[Double])\n\ndef simStep(p: Parameters, model: Dlm): Data => Rand[Data] = d => {\n    for {\n      w <- MultivariateGaussian(DenseVector.zeros(p.w.cols), p.w)\n      x1 = model.g * d.state + w\n      v <- Gaussian(0, p.v)\n      y = model.f.toDenseVector dot x1 + v\n    } yield Data(d.time + 1.0, y, x1)\n  }\nThis function contains a for comprehension, the <- symbol represents either a map or a flatMap. Since the w and v are Rand[Double] values, we need to access the value inside of the Rand and perform a function on it, the for comprehension desugars to:\nMultivariateGaussian(DenseVector.zeros(p.w.cols), p.w).\n  flatMap(w => {\n    val x1 = model.g * d.state + w\n    Gaussian(0, p.v).map(v => {\n      val y = model.f.toDenseVector dot x1 + v\n      Data(d.time + 1.0, y, x1)\n    })}\n    )\nThe desugared chain of flatMap and map evaluates to the same result, but the for-comprehension is more readable. This syntax provides a clean and elegent way to work within the context of a Monad, Rand is an example of a Monad representing a distribution.\nNow, we can use the MarkovChain breeze object to simulate the full series:\ndef simMarkov(p: Parameters, model: Dlm): Rand[Process[Data]] = {\n  for {\n    x0 <- MultivariateGaussian(p.m0, p.c0)\n    y0 <- Gaussian(model.f.toDenseVector dot x0, p.v)\n    init = Data(0.0, y0, x0)\n  } yield MarkovChain(init)(simStep(p, model))\n}\nNote that the Markov chain is a Rand[Process[Double]], this is not the most elegant and I’m open to suggestions on a better way to formulate this. Once we have defined a Markov chain, we must draw from it which returns a Process[Double], the initial draw is to sample from the initial state distribution \\(\\mathcal{N}(m_0, C_0)\\). Next, we can sample from the Process and plot it. A simulation from the seasonal DLM, with parameters \\(V = 3\\) and \\(W = I_6\\), is given below:\n\n\n\n\n\n\n",
    "preview": "posts/2016-12-13-SeasonalDlm/SeasonalDlm_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-06-22T10:37:23+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2016-12-12-KalmanFilter/",
    "title": "The Kalman Filter in Scala",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2016-12-12",
    "categories": [
      "Scala",
      "Bayesian"
    ],
    "contents": "\nA Dynamic Linear Model (DLM) is a special type of state space model, where the state and observation equations are Normally distributed and linear. A general DLM can be written as follows:\n\\[\\begin{aligned} y_t &= F_t x_t + \\nu_t,  &\\nu_t &\\sim \\mathcal{N}(0, V_t) \\\\\nx_t &= G_tx_{t-1} + \\omega_t &\\omega_t &\\sim \\mathcal{N}(0, W_t), \\end{aligned}\\]\n\\(y_t\\) represents the observation of the process at time \\(t\\), \\(x_t\\) is the value of the unobserved state at time \\(t\\). The observation error \\(\\nu_t\\) and the system error \\(\\omega_t\\) are independent and identically distributed Normal random variables. \\(F_t\\) is the observation matrix which transforms the state space to the observation, \\(G_t\\) is the state transition matrix.\nForward Simulating from the DLM\nA first order polynomial DLM with constant \\(V\\) and \\(W\\), and \\(F_t = 1\\), \\(G_t = 1\\). can be simulated in scala as follows:\nimport breeze.stats.distributions.Gaussian\n\ncase class Data(time: Time, observation: Observation, state: Option[State])\ncase class Parameters(v: Double, w: Double, m0: Double, c0: Double)\n\ndef simulate(p: Parameters): Stream[Data] = {\nval stateSpace = Stream.iterate(Gaussian(p.m0, sqrt(p.c0)).draw)(x => Gaussian(x, sqrt(p.w)).draw)\n  stateSpace.zipWithIndex map { case (x, t) =>\n    Data(t, x + Gaussian(0, sqrt(p.v)).draw, Some(x)) \n  }\n}\n\nval p = Parameters(3.0, 0.5, 0.0, 10.0)\n// simulate 16 different realisations of 100 observations, representing 16 stations\nval data = (1 to 16) map (id => (id, simulate(p).take(100).toVector))\nWe use the built in streaming library’s iterate function to specify the evolution of the latent state. The initial state \\(x_0\\) is a sample drawn from a normal distribution with mean \\(m_0\\) and variance \\(C_0\\), \\(x_t \\sim \\mathcal{N}(x_0 ; m_0, C_0)\\). The Breeze numerical computing library provides many statistical and mathematical functions is used. Subsequent states are generated by adding \\(\\mathcal{N}(0, W)\\) to the previous state:\n\\[x_t = x_{t-1} + \\omega, \\qquad \\omega \\sim \\mathcal{N}(0, V)\\]\nWe then construct a Stream of Data objects. The Data object has a timestamped observation and an optional state space. We construct the observation at time \\(t\\) by simply adding the observation noise to the state space \\(y_t \\sim \\mathcal{N}(y_t | x_t, V)\\). The state is optional because we can’t observe the state of real data, only simulated data will have a known state.\nA graph of the data from four “stations”, produced using ggplot2 in R is shown in the figure below.\n\n\n\nAn Aside on Referential Transparency\nNote that the function simulate is not referentially transparent, meaning the function will return a different Stream of data each time we run it. Referential transparency is important in functional programming, to allow us to easily reason about complex programs. The Breeze library implements another object for stateful random number generation, the Process object. The MarkovChain object can be used to construct a process without drawing explicitly from the distribution until we run the program. Firstly define a single step of the random walk:\nimport breeze.stats.distributions._\n\ndef step_rw(p: Parameters): Double => Rand[Double] = \n  x => Gaussian(x, p.w)\nIf step_rw is supplied with a set of Parameters, it returns a function from the current state, which is assumed to be materialised, to the next state, which is a Rand[Double]. The actual random number isn’t generated until we sample from distribution represented by Rand. Next, we can construct a MarkovChain using the transition kernel stepRw:\nval random_walk: Process[Double] = MarkovChain(0.0)(step_rw(p))\nrandom_walk.\n  steps.\n  take(100)\nThe Kalman Filter\nThe Kalman Filter can be used to determine the posterior distribution of the state space given the current observation and the \\(p(x_t|D_{t})\\), where \\(D_t = \\{Y_t, D_{t-1}\\}\\) and \\(D_0\\) is the initial information, comprising of the parameters mode parameters \\(W_0\\) and \\(V_0\\) and the initial state \\(x_0 \\sim N(m_0, C_0)\\). The full treatment of the Kalman Filter can be found in the excellent Bayesian Forecasting for Dynamic Models by West and Harrison.\nI will present the filtering equations for the simple model in this post. Suppose we start with the posterior distribution of \\(x_{t-1} \\sim N(m_{t-1}, C_{t-1})\\). The first thing we need to do is advance the state, the equation to advance the state is a simple Markov transition \\(x_t = x_{t-1} + \\omega_t\\). We simply add the system variance, the system variance is drawn from a Normal distribution with zero mean and variance \\(W\\). The sum of two Normal distributions is Normal with the mean and variance added, hence the prior for \\((x_t | D_{t-1}) \\sim N(m_{t-1}, C_{t-1} + W)\\).\nNext we need to construct the observation, using the observation equation which is commonly called the one-step forecast for the series, \\(y_t = x_t + \\nu_t\\), since \\(\\nu_t\\) is Normally distributed with zero mean and variance \\(V\\), the distribution of the one step forecast is, \\((y_t|D_{t-1}) \\sim N(m_{t-1}, C_{t-1} + W + V)\\).\nNow, we observe the true value of \\(y_t\\) and are able to construct the posterior distribution of \\((x_t | D_t) \\sim N(m_t, C_t)\\). \\(m_t = m_{t-1} + A_t e_t\\) and \\(C_t = A_tV\\). \\(A_t = \\frac{C_{t-1} + W}{ C_{t-1} + W + V}\\) is known as the regression coefficient, and \\(e_t = Y_t - m_{t-1}\\). This result can be shown using properties of the multivariate normal distribution, and is presented in full in Bayesian Forecasting for Dynamic Models by West and Harrison.\nWe now have the equations required to program up the Kalman Filter using Scala.\ncase class FilterState(data: Data, p: Parameters)\n\ndef filter(p: Parameters): (FilterState, Data) => FilterState = (s, d) => {\n  val r = s.p.c0 + p.w\n  val q = r + p.v\n  val e = d.observation - s.p.m0\n\n  // kalman gain\n  val k = r / q\n  val c1 = k * p.v\n  \n  // return the data with the expectation of the hidden state and the updated Parameters\n  FilterOut(Data(d.time, d.observation, Some(m1)), Parameters(p.v, p.w, m1, c1))\n} \nThe function filter simply takes in Parameters and one observation, represented by Data and returns the updated parameters required for the next step of the filter. Now we need to write a function which filters a sequence of Data, and returns a sequence consisting of the latent states, which we can do using the function scanLeft.\nA simplified function signature of scanLeft is given by: scanLeft[A](l: List[A], z: A)(f: (A, A) => A): List[A]. A list, l with elements of type A and an initial value, z also of type A if passed to a Function2 and accumulated into another list with elements of type A. The function f is applied to each element of the list pairwise, starting the the head of the lift and the zero element, z. Consider calculating the sum of a list of numbers:\nval numbers = List(1,2,3,4,5)\ndef sum(a: Int, b: Int): Int = a + b\n\nnumbers.scanLeft(0)(sum)\n// List(0, 1, 3, 6, 10, 15)\nThe first calculation is (0 + 1) = 1, which is then used as the first argument in the pairwise sum function, then the second calculation is (1 + 2) = 3, the result of which is used again in the next application of sum. Each intermediate step of the calculation is retained and appended to a list to be output when the list of number is exhausted, so we end up with a cumulative sum, List(0, 1, 3, 6, 10, 15). We can use scanLeft and the Function2, filter to calculate and retain the latent states in a DLM:\ndef filterSeries(data: Seq[Data], initParams: Parameters): Seq[FilterOut] = {\n  val initFilter = FilterState(data.head, params, 0.0) // initialise the filter\n\n  data.\n    scanLeft(initFilter)(filter(initParams)).\n    drop(1)\n}\nNow, we can apply the filter to all the stations simultaneously:\ndata.\n  groupBy{ case (id, _) => id }. //groups by id\n  map{ case (id, idAndData) =>\n  (id, idAndData map (x => x._2)) }. // changes into (id, data) pairs\n  map{ case (id, data) =>\n  (id, filterSeries(data.sortBy(_.time), p)) } // apply the filter to the sorted data\nWe can now plot the results of the filtering using R and ggplot2, overlaid with 95% prediction intervals.\n\n\nfiltered = read_csv(\n  here::here(\"notebooks/data/filteredDlm.csv\"),\n  c(\"stationId\", \"time\", \"observation\", \"stateMean\", \"m\", \"c\")\n)\n\n## calculate upper and lower 95% bounds of the state estimate\nfiltered %>%\n  select(-observation) %>%\n  inner_join(data, by = c(\"time\", \"stationId\")) %>%\n  filter(stationId %in% 1:4) %>%\n  mutate(upper = qnorm(p = 0.975, mean = m, sd = sqrt(c)), \n         lower = qnorm(p = 0.025, mean = m, sd = sqrt(c))) %>%\n  select(-c, -m, -observation) %>%\n  gather(key, value, -time, -stationId, -upper, -lower) %>%\n  ggplot(aes(x = time, y = value, linetype = key)) + \n  geom_line() +\n  geom_ribbon(aes(x = time, ymin = lower, ymax = upper), alpha = 0.3) +\n  facet_wrap(~stationId, scales = \"free\")\n\n\n\n\nThe full code is available in a notebook file and an ammonite script in the GitHub Repo associated with this blog.\n\n\n\n",
    "preview": "posts/2016-12-12-KalmanFilter/KalmanFilter_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-06-22T10:37:22+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2016-12-01-PracticalAkkaStreams/",
    "title": "Practical Introduction to Akka Streaming",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2016-12-01",
    "categories": [
      "Scala"
    ],
    "contents": "\nAkka Streaming is a streaming IO engine used to build high performance, fault tolerant and scalable streaming data services. In this post I will describe how you can implement some of the features included in Akka Streaming using only simple streams of integers and strings, although the true power of Akka streams only becomes apparent when we are consuming data from real sources such as Websockets, databases and files. Akka is available in Java and Scala, but I will be focusing on the Scala API in this post.\nBuilding a new SBT Project\nSimple Build Tool is the most used build tool of Scala developers, despite the name it is incredibly powerful with many advanced features. In this post, we will be using SBT to manage the dependency on Akka. Firstly we must specify the following directory structure:\n.\n├── build.sbt\n└── src\n    └── main\n        └── scala\n\n3 directories, 1 file\nThe file build.sbt will contain the information required by SBT to download the Akka Stream dependencies. The .scala source code will live in the scala directory.\nDependencies in SBT\nFirst, we need to to specify some library dependencies in the file build.sbt:\nname := \"Akka-Stream-Example\"\nversion := \"1.0\"\n\nscalaVersion := \"2.11.8\"\n\nlibraryDependencies += \"com.typesafe.akka\" %% \"akka-stream\" % \"2.4.14\"\nNow, in the terminal, navigate to the root directory of the project and run sbt. The dependencies will be downloaded automatically, and available to use in any source files.\nSource\nSource represents the start of an Akka stream, there are many methods for constructing streams from Source. For now, we will define a ticking stream of integers and investigate how we can transform and output this stream using Flows and Sinks respectively. Here is a Source which outputs a steady stream of 1s every second\nimport akka.actor.ActorSystem\nimport akka.stream.ActorMaterializer\nimport akka.stream.scaladsl._\n\nobject Streaming {\n  implicit val system = ActorSystem(\"Streaming\")\n  implicit val executor = system.dispatcher\n  implicit val materializer = ActorMaterializer()\n\n  val in = Source.tick(1.second, 1.second, 1)\n}\nFlow\nA Flow is a data processing stage. We will define a data flow which takes in an integer, then doubles it.\nval doubleFlow = Flow[Int].map(a => a * 2)\nThis Flow is reusable and can be joined on to any stream which emits an Int. The map function is an example of a higher-order function; a higher-order function accepts a function as an argument. The map function here is used to access the value held inside of the Flow, in this case an Int. The function passed as the argument to map is an anonymous (or lambda) function, it says we take the Int and multiply it by two, a type annotation is not needed on the value a as the compiler infers the type to be Int.\nSink\nA Sink is an endpoint to a stream, we can use it to print to the console, write to a database or another external service. However only when the stream is materialized is the side effect in the Sink performed. Let’s define a simple sink which prints each element out on a new line\nval out = Sink.foreach(println)\nPutting it all together\nNow we want to get our stream printing to the console, we must define a main method for the Streaming object connecting the Source to the Flow and finally to the Sink.\nimport akka.actor.ActorSystem\nimport akka.stream.ActorMaterializer\nimport akka.stream.scaladsl.{Sink, Flow, Source}\nimport scala.concurrent.duration._\n\nobject Streaming {\n  implicit val system = ActorSystem(\"Streaming\")\n  implicit val executor = system.dispatcher\n  implicit val materializer = ActorMaterializer()\n\n  val in = Source.tick(1.second, 1.second, 1)\n  val double_flow = Flow[Int].map(a => a * 2)\n  val print_sink = Sink.foreach(println)\n\n  def main(args: Array[String]) {\n      in.\n        via(double_flow).\n        take(10).\n        runWith(print_sink).\n        onComplete(_ => system.terminate)\n  }\n}\nYou can now run this code block using by executing sbt run from the terminal, in the project directory root (where build.sbt lives). We should get a stream of twos emitting once every second. The function take will limit the amount of twos printed to the console and once the stream is exhausted the Akka system is shutdown using the function onComplete.\nGraph DSL\nAkka Streaming provides a domain specific language (DSL) to express stream processing pipelines using a graph. Here is another way to define the main method using a RunnableGraph:\nval graph = RunnableGraph.fromGraph(GraphDSL.create() { implicit builder =>\n\n  in ~> double_flow ~> Flow[Int].take(10) ~> print_sink\n\n  ClosedShape\n})\n  \ngraph.run()\nThe graph DSL requires a bit of work, namely defining a runnable graph and specifying that the graph is closed. This is because we can define partial graphs (a graph which isn’t connected to a Source, Sink or both) which compose with other graphs, handy if you want to reuse a block of processing.\nIn order to specify a partial graph with no connections, we use FlowShape(inlet, outlet). Let’s define a partial graph which takes in one stream a integers, splits them on a condition, performs some processing then sends them out:\nval partial_graph = Flow.fromGraph(GraphDSL.create() { implicit builder =>\n    val broadcast = builder.add(Broadcast[Int](2))\n    val zip = builder.add(Zip[Int, Int]())\n\n    broadcast.out(0) ~> Flow[Int].filter(_ % 2 == 0) ~> Flow[Int].map(_ / 2) ~> zip.in0\n    broadcast.out(1) ~> Flow[Int].filter(_ % 2 != 0) ~> Flow[Int].map(_ * 2) ~> zip.in1\n\n    FlowShape(broadcast.in, zip.out)\n  })\nThis FlowShape is expecting a Source containing integers, in the first line of processing it checks if the items are even, then divides them by two. The second line of processing doubles all the even numbers. The two streams are then recombined using a zip. In order to materialize data through this processing stage, a Source of integers and a Sink must be connected.\nMerging a stream\nIf we have two streams containing the same datatype, then we can merge these two streams into one:\nLet’s reuse our stream of ones and merge it into the stream of twos we already have:\nval merge_graph = RunnableGraph.fromGraph(GraphDSL.create() { implicit builder =>\n  val merge = builder.add(Merge[Int](2))\n  \n  in ~> double_flow ~> merge ~> print_sink\n                in ~> merge\n  ClosedShape\n})\nWe add a Merge to the graph builder, Merge requires we specify the type of the stream elements we are merging and the number of stream sources we are merging. We then alter the stream processing flow to include a stream of ones and perform the merge.\nIn this case, if the stream of ones was publishing at a higher frequency than the other stream, we would have a stream with more ones than twos. ie:\n1 1 1 2 1 1 1 2 …\nZipping a Stream\nWe can zip a stream just as we can zip collections in Scala. This results in a tuple, which can have heterogeneous types. Zip requires that both streams have an element available, so if one stream is publishing at a quicker rate than the others there will be buffering of those elements.\nTo illustrate this, we will build a continuous stream of natural numbers using the unfold function:\nval naturalNumbers = Source.unfold(1)(a => Some(a + 1, a))\nunfold is the dual of fold and is an Anamorphism. unfold starts with a seed value and applies a function to produce the next value in the stream, the result of this function evaluation is sent to the next evaluation and so on. Using unfold is a simple way to define a stream which depends on the previous value.\nNow if we zip together a continuous source of ones which publishes every ten seconds, Zip will wait for both streams to have an element before publishing the tuple of both streams, guaranteeing order.\nval in = Source.tick(1.second, 10.seconds, 1)\n\nval zipStream = RunnableGraph.fromGraph(FlowGraph.create() { implicit builder =>\n    val zip = builder.add(Zip[Int, Int])\n    naturalNumbers ~> zip.in0\n    in ~> zip.in1\n    zip.out ~> out\n\n    ClosedShape\n  })\n\n  def main(args: Array[String]): Unit = {\n    zipStream.run(materializer)\n  }\nThe output from this stream is: (1,1) (2,1) (3,1) …\nSummary\nWe have covered a few ways to express simple streams using Akka Streaming. The real power of Akka streaming is when it is combined with file or connection handling. Streaming libraries can be used to process extremely large, or unbounded, data files using a bounded amount of computational power. This is useful when dealing with infinite sources of data, such as streaming data from Twitter.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-22T10:37:16+01:00",
    "input_file": {}
  }
]
