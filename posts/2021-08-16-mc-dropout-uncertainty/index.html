<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { color: #20794d; } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007ba5; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #007ba5; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #007ba5; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #20794d; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>

  <!--radix_placeholder_meta_tags-->
<title>Bayesian Statistics and Functional Programming: Uncertainty in Neural Networks</title>

<meta property="description" itemprop="description" content="Using MC Dropout to get probability intervals for neural network predictions."/>

<link rel="canonical" href="https://jonnylaw.rocks/posts/2021-08-16-mc-dropout-uncertainty/"/>

<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2021-08-16"/>
<meta property="article:created" itemprop="dateCreated" content="2021-08-16"/>
<meta name="article:author" content="Jonny Law"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content="Bayesian Statistics and Functional Programming: Uncertainty in Neural Networks"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="Using MC Dropout to get probability intervals for neural network predictions."/>
<meta property="og:url" content="https://jonnylaw.rocks/posts/2021-08-16-mc-dropout-uncertainty/"/>
<meta property="og:image" content="https://jonnylaw.rocks/posts/2021-08-16-mc-dropout-uncertainty/mc-dropout-uncertainty_files/figure-html5/unnamed-chunk-1-1.png"/>
<meta property="og:image:width" content="1248"/>
<meta property="og:image:height" content="768"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="Bayesian Statistics and Functional Programming"/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary_large_image"/>
<meta property="twitter:title" content="Bayesian Statistics and Functional Programming: Uncertainty in Neural Networks"/>
<meta property="twitter:description" content="Using MC Dropout to get probability intervals for neural network predictions."/>
<meta property="twitter:url" content="https://jonnylaw.rocks/posts/2021-08-16-mc-dropout-uncertainty/"/>
<meta property="twitter:image" content="https://jonnylaw.rocks/posts/2021-08-16-mc-dropout-uncertainty/mc-dropout-uncertainty_files/figure-html5/unnamed-chunk-1-1.png"/>
<meta property="twitter:image:width" content="1248"/>
<meta property="twitter:image:height" content="768"/>

<!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
<meta name="citation_title" content="Bayesian Statistics and Functional Programming: Uncertainty in Neural Networks"/>
<meta name="citation_fulltext_html_url" content="https://jonnylaw.rocks/posts/2021-08-16-mc-dropout-uncertainty/"/>
<meta name="citation_online_date" content="2021/08/16"/>
<meta name="citation_publication_date" content="2021/08/16"/>
<meta name="citation_author" content="Jonny Law"/>
<!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=What are bayesian neural network posteriors really like?;citation_publication_date=2021;citation_author=Pavel Izmailov;citation_author=Sharad Vikram;citation_author=Matthew D Hoffman;citation_author=Andrew Gordon Wilson"/>
  <meta name="citation_reference" content="citation_title=Dropout as a bayesian approximation: Representing model uncertainty in deep learning;citation_publication_date=2016;citation_publisher=PMLR;citation_author=Yarin Gal;citation_author=Zoubin Ghahramani"/>
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","bibliography","output","categories","citation_url","canonical_url"]}},"value":[{"type":"character","attributes":{},"value":["Uncertainty in Neural Networks"]},{"type":"character","attributes":{},"value":["Using MC Dropout to get probability intervals for neural network predictions."]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url"]}},"value":[{"type":"character","attributes":{},"value":["Jonny Law"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":[]}},"value":[]}]}]},{"type":"character","attributes":{},"value":["2021-08-16"]},{"type":"character","attributes":{},"value":["references.bib"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["Python","Deep Learning","Bayesian"]},{"type":"character","attributes":{},"value":["https://jonnylaw.rocks/posts/2021-08-16-mc-dropout-uncertainty/"]},{"type":"character","attributes":{},"value":["https://jonnylaw.rocks/posts/2021-08-16-mc-dropout-uncertainty/"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["mc-dropout-uncertainty_files/anchor-4.2.2/anchor.min.js","mc-dropout-uncertainty_files/bowser-1.9.3/bowser.min.js","mc-dropout-uncertainty_files/distill-2.2.21/template.v2.js","mc-dropout-uncertainty_files/figure-html5/unnamed-chunk-1-1.png","mc-dropout-uncertainty_files/figure-html5/unnamed-chunk-1-11.png","mc-dropout-uncertainty_files/figure-html5/unnamed-chunk-1-13.png","mc-dropout-uncertainty_files/figure-html5/unnamed-chunk-1-3.png","mc-dropout-uncertainty_files/figure-html5/unnamed-chunk-1-5.png","mc-dropout-uncertainty_files/figure-html5/unnamed-chunk-1-7.png","mc-dropout-uncertainty_files/figure-html5/unnamed-chunk-1-9.png","mc-dropout-uncertainty_files/header-attrs-2.8/header-attrs.js","mc-dropout-uncertainty_files/jquery-1.11.3/jquery.min.js","mc-dropout-uncertainty_files/popper-2.6.0/popper.min.js","mc-dropout-uncertainty_files/tippy-6.2.7/tippy-bundle.umd.min.js","mc-dropout-uncertainty_files/tippy-6.2.7/tippy-light-border.css","mc-dropout-uncertainty_files/tippy-6.2.7/tippy.css","mc-dropout-uncertainty_files/tippy-6.2.7/tippy.umd.min.js","mc-dropout-uncertainty_files/webcomponents-2.0.0/webcomponents.js","references.bib"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
<meta name="distill:offset" content="../.."/>

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      tolerance: 5,
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #0F2E3D;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
  min-width: 150px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative; min-height: 500px; }
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

/* adjust viewport for navbar height */
/* helps vertically center bootstrap (non-distill) content */
.min-vh-100 {
  min-height: calc(100vh - 100px) !important;
}

</style>

<script src="../../site_libs/jquery-1.11.3/jquery.min.js"></script>
<link href="../../site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet"/>
<link href="../../site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet"/>
<script src="../../site_libs/headroom-0.9.4/headroom.min.js"></script>
<script src="../../site_libs/autocomplete-0.37.1/autocomplete.min.js"></script>
<script src="../../site_libs/fuse-6.4.1/fuse.min.js"></script>

<script type="application/javascript">

function getMeta(metaName) {
  var metas = document.getElementsByTagName('meta');
  for (let i = 0; i < metas.length; i++) {
    if (metas[i].getAttribute('name') === metaName) {
      return metas[i].getAttribute('content');
    }
  }
  return '';
}

function offsetURL(url) {
  var offset = getMeta('distill:offset');
  return offset ? offset + '/' + url : url;
}

function createFuseIndex() {

  // create fuse index
  var options = {
    keys: [
      { name: 'title', weight: 20 },
      { name: 'categories', weight: 15 },
      { name: 'description', weight: 10 },
      { name: 'contents', weight: 5 },
    ],
    ignoreLocation: true,
    threshold: 0
  };
  var fuse = new window.Fuse([], options);

  // fetch the main search.json
  return fetch(offsetURL('search.json'))
    .then(function(response) {
      if (response.status == 200) {
        return response.json().then(function(json) {
          // index main articles
          json.articles.forEach(function(article) {
            fuse.add(article);
          });
          // download collections and index their articles
          return Promise.all(json.collections.map(function(collection) {
            return fetch(offsetURL(collection)).then(function(response) {
              if (response.status === 200) {
                return response.json().then(function(articles) {
                  articles.forEach(function(article) {
                    fuse.add(article);
                  });
                })
              } else {
                return Promise.reject(
                  new Error('Unexpected status from search index request: ' +
                            response.status)
                );
              }
            });
          })).then(function() {
            return fuse;
          });
        });

      } else {
        return Promise.reject(
          new Error('Unexpected status from search index request: ' +
                      response.status)
        );
      }
    });
}

window.document.addEventListener("DOMContentLoaded", function (event) {

  // get search element (bail if we don't have one)
  var searchEl = window.document.getElementById('distill-search');
  if (!searchEl)
    return;

  createFuseIndex()
    .then(function(fuse) {

      // make search box visible
      searchEl.classList.remove('hidden');

      // initialize autocomplete
      var options = {
        autoselect: true,
        hint: false,
        minLength: 2,
      };
      window.autocomplete(searchEl, options, [{
        source: function(query, callback) {
          const searchOptions = {
            isCaseSensitive: false,
            shouldSort: true,
            minMatchCharLength: 2,
            limit: 10,
          };
          var results = fuse.search(query, searchOptions);
          callback(results
            .map(function(result) { return result.item; })
          );
        },
        templates: {
          suggestion: function(suggestion) {
            var img = suggestion.preview && Object.keys(suggestion.preview).length > 0
              ? `<img src="${offsetURL(suggestion.preview)}"</img>`
              : '';
            var html = `
              <div class="search-item">
                <h3>${suggestion.title}</h3>
                <div class="search-item-description">
                  ${suggestion.description || ''}
                </div>
                <div class="search-item-preview">
                  ${img}
                </div>
              </div>
            `;
            return html;
          }
        }
      }]).on('autocomplete:selected', function(event, suggestion) {
        window.location.href = offsetURL(suggestion.path);
      });
      // remove inline display style on autocompleter (we want to
      // manage responsive display via css)
      $('.algolia-autocomplete').css("display", "");
    })
    .catch(function(error) {
      console.log(error);
    });

});

</script>

<style type="text/css">

.nav-search {
  font-size: x-small;
}

/* Algolioa Autocomplete */

.algolia-autocomplete {
  display: inline-block;
  margin-left: 10px;
  vertical-align: sub;
  background-color: white;
  color: black;
  padding: 6px;
  padding-top: 8px;
  padding-bottom: 0;
  border-radius: 6px;
  border: 1px #0F2E3D solid;
  width: 180px;
}


@media screen and (max-width: 768px) {
  .distill-site-nav .algolia-autocomplete {
    display: none;
    visibility: hidden;
  }
  .distill-site-nav.responsive .algolia-autocomplete {
    display: inline-block;
    visibility: visible;
  }
  .distill-site-nav.responsive .algolia-autocomplete .aa-dropdown-menu {
    margin-left: 0;
    width: 400px;
    max-height: 400px;
  }
}

.algolia-autocomplete .aa-input, .algolia-autocomplete .aa-hint {
  width: 90%;
  outline: none;
  border: none;
}

.algolia-autocomplete .aa-hint {
  color: #999;
}
.algolia-autocomplete .aa-dropdown-menu {
  width: 550px;
  max-height: 70vh;
  overflow-x: visible;
  overflow-y: scroll;
  padding: 5px;
  margin-top: 3px;
  margin-left: -150px;
  background-color: #fff;
  border-radius: 5px;
  border: 1px solid #999;
  border-top: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion {
  cursor: pointer;
  padding: 5px 4px;
  border-bottom: 1px solid #eee;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion:last-of-type {
  border-bottom: none;
  margin-bottom: 2px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item {
  overflow: hidden;
  font-size: 0.8em;
  line-height: 1.4em;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item h3 {
  font-size: 1rem;
  margin-block-start: 0;
  margin-block-end: 5px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-description {
  display: inline-block;
  overflow: hidden;
  height: 2.8em;
  width: 80%;
  margin-right: 4%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview {
  display: inline-block;
  width: 15%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img {
  height: 3em;
  width: auto;
  display: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img[src] {
  display: initial;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion.aa-cursor {
  background-color: #eee;
}
.algolia-autocomplete .aa-dropdown-menu .aa-suggestion em {
  font-weight: bold;
  font-style: normal;
}

</style>


<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

d-byline .byline {
  grid-template-columns: 2fr 2fr;
}

d-byline .byline h3 {
  margin-block-start: 1.5em;
}

d-byline .byline .authors-affiliations h3 {
  margin-block-start: 0.5em;
}

.authors-affiliations .orcid-id {
  width: 16px;
  height:16px;
  margin-left: 4px;
  margin-right: 4px;
  vertical-align: middle;
  padding-bottom: 2px;
}

d-title .dt-tags {
  margin-top: 1em;
  grid-column: text;
}

.dt-tags .dt-tag {
  text-decoration: none;
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0em 0.4em;
  margin-right: 0.5em;
  margin-bottom: 0.4em;
  font-size: 70%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

d-article table.gt_table td,
d-article table.gt_table th {
  border-bottom: none;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

d-article {
  padding-top: 2.5rem;
  padding-bottom: 30px;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article h2 {
  margin: 1rem 0 1.5rem 0;
}

d-article h3 {
  margin-top: 1.5rem;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

/* Tweak code blocks */

d-article div.sourceCode code,
d-article pre code {
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: auto;
}

d-article div.sourceCode {
  background-color: white;
}

d-article div.sourceCode pre {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

d-article pre {
  font-size: 12px;
  color: black;
  background: none;
  margin-top: 0;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

d-article pre a {
  border-bottom: none;
}

d-article pre a:hover {
  border-bottom: none;
  text-decoration: underline;
}

d-article details {
  grid-column: text;
  margin-bottom: 0.8em;
}

@media(min-width: 768px) {

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: visible !important;
}

d-article div.sourceCode pre {
  padding-left: 18px;
  font-size: 14px;
}

d-article pre {
  font-size: 14px;
}

}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for d-contents */

.d-contents {
  grid-column: text;
  color: rgba(0,0,0,0.8);
  font-size: 0.9em;
  padding-bottom: 1em;
  margin-bottom: 1em;
  padding-bottom: 0.5em;
  margin-bottom: 1em;
  padding-left: 0.25em;
  justify-self: start;
}

@media(min-width: 1000px) {
  .d-contents.d-contents-float {
    height: 0;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: center;
    padding-right: 3em;
    padding-left: 2em;
  }
}

.d-contents nav h3 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 1em;
}

.d-contents li {
  list-style-type: none
}

.d-contents nav > ul {
  padding-left: 0;
}

.d-contents ul {
  padding-left: 1em
}

.d-contents nav ul li {
  margin-top: 0.6em;
  margin-bottom: 0.2em;
}

.d-contents nav a {
  font-size: 13px;
  border-bottom: none;
  text-decoration: none
  color: rgba(0, 0, 0, 0.8);
}

.d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6)
}

.d-contents nav > ul > li > a {
  font-weight: 600;
}

.d-contents nav > ul > li > ul {
  font-weight: inherit;
}

.d-contents nav > ul > li > ul > li {
  margin-top: 0.2em;
}


.d-contents nav ul {
  margin-top: 0;
  margin-bottom: 0.25em;
}

.d-article-with-toc h2:nth-child(2) {
  margin-top: 0;
}


/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure img {
  width: 100%;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

/* Citations */

d-article .citation {
  color: inherit;
  cursor: inherit;
}

div.hanging-indent{
  margin-left: 1em; text-indent: -1em;
}

/* Citation hover box */

.tippy-box[data-theme~=light-border] {
  background-color: rgba(250, 250, 250, 0.95);
}

.tippy-content > p {
  margin-bottom: 0;
  padding: 2px;
}


/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* Include appendix styles here so they can be overridden */

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

/* Include footnote styles here so they can be overridden */

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}



/* Anchor.js */

.anchorjs-link {
  /*transition: all .25s linear; */
  text-decoration: none;
  border-bottom: none;
}
*:hover > .anchorjs-link {
  margin-left: -1.125em !important;
  text-decoration: none;
  border-bottom: none;
}

/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


.sidebar-section.custom {
  font-size: 12px;
  line-height: 1.6em;
}

.custom p {
  margin-bottom: 0.5em;
}

/* Styles for listing layout (hide title) */
.layout-listing d-title, .layout-listing .d-title {
  display: none;
}

/* Styles for posts lists (not auto-injected) */


.posts-with-sidebar {
  padding-left: 45px;
  padding-right: 45px;
}

.posts-list .description h2,
.posts-list .description p {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

.posts-list .description h2 {
  font-weight: 700;
  border-bottom: none;
  padding-bottom: 0;
}

.posts-list h2.post-tag {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  padding-bottom: 12px;
}
.posts-list {
  margin-top: 60px;
  margin-bottom: 24px;
}

.posts-list .post-preview {
  text-decoration: none;
  overflow: hidden;
  display: block;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding: 24px 0;
}

.post-preview-last {
  border-bottom: none !important;
}

.posts-list .posts-list-caption {
  grid-column: screen;
  font-weight: 400;
}

.posts-list .post-preview h2 {
  margin: 0 0 6px 0;
  line-height: 1.2em;
  font-style: normal;
  font-size: 24px;
}

.posts-list .post-preview p {
  margin: 0 0 12px 0;
  line-height: 1.4em;
  font-size: 16px;
}

.posts-list .post-preview .thumbnail {
  box-sizing: border-box;
  margin-bottom: 24px;
  position: relative;
  max-width: 500px;
}
.posts-list .post-preview img {
  width: 100%;
  display: block;
}

.posts-list .metadata {
  font-size: 12px;
  line-height: 1.4em;
  margin-bottom: 18px;
}

.posts-list .metadata > * {
  display: inline-block;
}

.posts-list .metadata .publishedDate {
  margin-right: 2em;
}

.posts-list .metadata .dt-authors {
  display: block;
  margin-top: 0.3em;
  margin-right: 2em;
}

.posts-list .dt-tags {
  display: block;
  line-height: 1em;
}

.posts-list .dt-tags .dt-tag {
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0.3em 0.4em;
  margin-right: 0.2em;
  margin-bottom: 0.4em;
  font-size: 60%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

.posts-list img {
  opacity: 1;
}

.posts-list img[data-src] {
  opacity: 0;
}

.posts-more {
  clear: both;
}


.posts-sidebar {
  font-size: 16px;
}

.posts-sidebar h3 {
  font-size: 16px;
  margin-top: 0;
  margin-bottom: 0.5em;
  font-weight: 400;
  text-transform: uppercase;
}

.sidebar-section {
  margin-bottom: 30px;
}

.categories ul {
  list-style-type: none;
  margin: 0;
  padding: 0;
}

.categories li {
  color: rgba(0, 0, 0, 0.8);
  margin-bottom: 0;
}

.categories li>a {
  border-bottom: none;
}

.categories li>a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
}

.categories .active {
  font-weight: 600;
}

.categories .category-count {
  color: rgba(0, 0, 0, 0.4);
}


@media(min-width: 768px) {
  .posts-list .post-preview h2 {
    font-size: 26px;
  }
  .posts-list .post-preview .thumbnail {
    float: right;
    width: 30%;
    margin-bottom: 0;
  }
  .posts-list .post-preview .description {
    float: left;
    width: 45%;
  }
  .posts-list .post-preview .metadata {
    float: left;
    width: 20%;
    margin-top: 8px;
  }
  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.5em;
    font-size: 16px;
  }
  .posts-with-sidebar .posts-list {
    float: left;
    width: 75%;
  }
  .posts-with-sidebar .posts-sidebar {
    float: right;
    width: 20%;
    margin-top: 60px;
    padding-top: 24px;
    padding-bottom: 24px;
  }
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

.downlevel .posts-list .post-preview {
  color: inherit;
}



</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // add anchors
  if (window.anchors) {
    window.anchors.options.placement = 'left';
    window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
  }


  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  // move refs into #references-listing
  $('#references-listing').replaceWith($('#refs'));

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-contents a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('details, div.sourceCode, pre, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // remove code block used to force  highlighting css
  $('.distill-force-highlighting-css').parent().remove();

  // remove empty line numbers inserted by pandoc when using a
  // custom syntax highlighting theme
  $('code.sourceCode a:empty').remove();

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // article with toc class
    $('.d-contents').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // add orcid ids
    $('.authors-affiliations').find('.author').each(function(i, el) {
      var orcid_id = front_matter.authors[i].orcidID;
      if (orcid_id) {
        var a = $('<a></a>');
        a.attr('href', 'https://orcid.org/' + orcid_id);
        var img = $('<img></img>');
        img.addClass('orcid-id');
        img.attr('alt', 'ORCID ID');
        img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
        a.append(img);
        $(this).append(a);
      }
    });

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // remove d-appendix and d-footnote-list local styles
    $('d-appendix > style:first-child').remove();
    $('d-footnote-list > style:first-child').remove();

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // hoverable references
    $('span.citation[data-cites]').each(function() {
      var refHtml = $('#ref-' + $(this).attr('data-cites')).html();
      window.tippy(this, {
        allowHTML: true,
        content: refHtml,
        maxWidth: 500,
        interactive: true,
        interactiveBorder: 10,
        theme: 'light-border',
        placement: 'bottom-start'
      });
    });

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-contents').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<!--/radix_placeholder_distill-->
  <script src="../../site_libs/header-attrs-2.8/header-attrs.js"></script>
  <script src="../../site_libs/jquery-1.11.3/jquery.min.js"></script>
  <script src="../../site_libs/popper-2.6.0/popper.min.js"></script>
  <link href="../../site_libs/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="../../site_libs/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="../../site_libs/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="../../site_libs/anchor-4.2.2/anchor.min.js"></script>
  <script src="../../site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="../../site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="../../site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Uncertainty in Neural Networks","description":"Using MC Dropout to get probability intervals for neural network predictions.","authors":[{"author":"Jonny Law","authorURL":{},"affiliation":"&nbsp;","affiliationURL":"#","orcidID":""}],"publishedDate":"2021-08-16T00:00:00.000+00:00","citationText":"Law, 2021"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<a href="../../index.html" class="title">Bayesian Statistics and Functional Programming</a>
<input id="distill-search" class="nav-search hidden" type="text" placeholder="Search..."/>
</div>
<div class="nav-right">
<a href="../../index.html">Home</a>
<a href="../../about.html">About</a>
<a href="https://twitter.com/lawsy">Twitter</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Uncertainty in Neural Networks</h1>
<!--radix_placeholder_categories-->
<!--/radix_placeholder_categories-->
<p><p>Using MC Dropout to get probability intervals for neural network predictions.</p></p>
</div>

<div class="d-byline">
  Jonny Law true 
  
<br/>2021-08-16
</div>

<div class="d-article">
<p>Recently, I have been reading <a href="https://www.manning.com/books/probabilistic-deep-learning">Probabilistic Deep Learning</a> which introduces Bayesian methods for fitting Neural Networks using Tensorflow and Keras. One case study consists of a non-linear regression problem for a sinusoidal curve. Additionally, the curve is considered to have heteroskedastic variance, meaning the variance changes along the domain of the function. In this post I will consider approximating a non-linear function with constant (homoskedastic) variance and quantifying the uncertainty. I will be using PyTorch - because why not.</p>
<p>Uncertainty is an important concept in machine learning, if we have confidence in the predictions of model we are able to make more informed decisions. However, neural networks trained using traditional back-propagation typically return a single point estimate by training to minimise a loss function. There have been many attempts to incorporate uncertainty into neural networks, the most principled way is to fit a Bayesian Neural Network (BNNs) by placing prior distributions on the weights and calculating the posterior distribution using Bayes’ theorem:</p>
<p><span class="math display">\[p(\theta| y) = \frac{p(\theta)p(y|\theta)}{\int_\theta p(y|\theta)p(\theta)}\]</span></p>
<p>Where <span class="math inline">\(\theta\)</span> represents the parameters in the neural network, ie. the weights and biases. <span class="math inline">\(p(\theta)\)</span> is the prior distribution, and <span class="math inline">\(p(y|\theta)\)</span> is the likelihood. These can all be specified. However, calculating the evidence, the denominator in the equation, <span class="math inline">\(\int_\theta p(y|\theta)p(\theta)\)</span> is intractable analytically in most real-world problems, including BNNs. The gold standard for computing these integrals for real-world problems is <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">MCMC</a>. In the case of modern deep learning models, the parameter space is too high dimensional (ie. there are too many parameters!) for these methods to be computationally feasible (although <span class="citation" data-cites="izmailov2021bayesian">(<a href="#ref-izmailov2021bayesian" role="doc-biblioref">Izmailov et al. 2021</a>)</span> did use HMC for fitting NNs to the CIFAR-10 image dataset and IMDB review dataset using 512 TPUv3 devices). Hence, we look to approximate methods, such as variational inference, which can place additional assumptions on the form on the prior and posterior distributions. There are also non-Bayesian methods (such as deep ensembles) which can be evaluated to have desirable properties such as calibration. From the Wikipedia for <a href="https://en.wikipedia.org/wiki/Calibration_(statistics)">statistical calibration</a> “As Philip Dawid puts it,”a forecaster is well calibrated if, for example, of those events to which he assigns a probability 30 percent, the long-run proportion that actually occurs turns out to be 30 percent“.”</p>
<p>In this post we will investigate a single easy to implement method for uncertainty estimation, MC dropout <span class="citation" data-cites="gal2016dropout">(<a href="#ref-gal2016dropout" role="doc-biblioref">Gal and Ghahramani 2016</a>)</span>, applied to a regression problem.</p>
<p>Let’s start by creating simulating data from a simple model:</p>
<p><span class="math display">\[
\begin{aligned}
y_i &amp;\sim \mathcal{N}(\mu_i, \sigma^2), \\
\mu_i &amp;= a \sin(x_i + b).
\end{aligned}
\]</span></p>
<p>Where <span class="math inline">\(a=2\)</span>, <span class="math inline">\(b=-3\)</span> and <span class="math inline">\(\sigma=1\)</span>.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_data(xs):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> <span class="kw">lambda</span> x: a <span class="op">*</span> np.sin(x <span class="op">+</span> b)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> [(x, np.random.normal(loc<span class="op">=</span>mean(x), scale<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> x <span class="kw">in</span> xs]</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    x, y <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>data)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(x), np.array(y)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">10</span>, size<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> generate_data(xs)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>plt.scatter(x, y)</span></code></pre></div>
<p><img src="mc-dropout-uncertainty_files/figure-html5/unnamed-chunk-1-1.png" width="624" /></p>
</div>
<p>Create a neural net which can learn the non-linear function. We’ll use the class <code>nn.ModuleList</code> to allow us to experiment with different numbers of hidden layers. According to the MC Dropout paper, we must apply dropout to each layer in order for the procedure to be equivalent to variational inference - under a set of assumptions.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NonLinearRegression(nn.Module):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dropout, hidden_size, hidden_layers):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(NonLinearRegression, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">input</span> <span class="op">=</span> nn.Linear(<span class="dv">1</span>, hidden_size)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linears <span class="op">=</span> nn.ModuleList(</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>          [nn.Linear(hidden_size, hidden_size) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_layers)])</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropouts <span class="op">=</span> nn.ModuleList(</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>          [nn.Dropout(dropout) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_layers)])</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output <span class="op">=</span> nn.Linear(hidden_size, <span class="dv">1</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.<span class="bu">input</span>(x)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> l, d <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.linears, <span class="va">self</span>.dropouts):</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>          x <span class="op">=</span> l(x)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>          x <span class="op">=</span> F.relu(x)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>          x <span class="op">=</span> d(x)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output(x)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
</div>
<p>Split the data randomly into training and testing.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(np.array(<span class="bu">list</span>(x)), np.array(<span class="bu">list</span>(y)))</span></code></pre></div>
</div>
<p>Test the untrained model input and output shapes by making a prediction using the training data.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NonLinearRegression(<span class="fl">0.5</span>, <span class="dv">32</span>, <span class="dv">1</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>x_input <span class="op">=</span> torch.tensor(x_train, dtype<span class="op">=</span>torch.<span class="bu">float</span>).unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>model(x_input).shape</span></code></pre></div>
<pre><code>torch.Size([75, 1])</code></pre>
</div>
<p>Use Skorch to avoid writing the training boilerplate. Use the <code>.fit</code> method provided by Skorch.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skorch.regressor <span class="im">import</span> NeuralNetRegressor</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> NeuralNetRegressor(</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    module<span class="op">=</span>NonLinearRegression,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    module__dropout<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    module__hidden_size<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    module__hidden_layers<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span>torch.optim.AdamW,</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    iterator_train__shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    max_epochs<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>x_train_input <span class="op">=</span> torch.tensor(x_train, dtype<span class="op">=</span>torch.<span class="bu">float</span>).unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>net.fit(x_train_input, torch.tensor(y_train, dtype<span class="op">=</span>torch.<span class="bu">float</span>))</span></code></pre></div>
<pre><code>&lt;class &#39;skorch.regressor.NeuralNetRegressor&#39;&gt;[initialized](
  module_=NonLinearRegression(
    (input): Linear(in_features=1, out_features=64, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linears): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): Linear(in_features=64, out_features=64, bias=True)
    )
    (dropouts): ModuleList(
      (0): Dropout(p=0.1, inplace=False)
      (1): Dropout(p=0.1, inplace=False)
      (2): Dropout(p=0.1, inplace=False)
    )
    (output): Linear(in_features=64, out_features=1, bias=True)
  ),
)</code></pre>
</div>
<p>We can visualise the learning curve for this model below.</p>
<div class="layout-chunk" data-layout="l-body">
<p><img src="mc-dropout-uncertainty_files/figure-html5/unnamed-chunk-1-3.png" width="624" /></p>
</div>
<p>We can see in the figure below that the predictions are in the right region, but using only a point prediction we miss out on capturing the uncertainty.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>x_test_input <span class="op">=</span> torch.tensor(x_test, dtype<span class="op">=</span>torch.<span class="bu">float</span>).unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> net.predict(x_test_input)</span></code></pre></div>
</div>
<div class="layout-chunk" data-layout="l-body">
<p><img src="mc-dropout-uncertainty_files/figure-html5/unnamed-chunk-1-5.png" width="624" /></p>
</div>
<h2 id="measurement-error-with-distribution">Measurement error with Distribution</h2>
<p>We can represent the <a href="https://en.wikipedia.org/wiki/Uncertainty_quantification">aleatoric uncertainty</a>, ie. the uncertainty inherent in the data using a Normal distribution. We know the observation distribution of the data generating process is Normal with a standard deviation (called scale in PyTorch) of 1.0. We can learn this additional parameter using PyTorch, to do this we can introduce a new loss function and alter the forward function of the Neural Network module to include the standard deviation parameter. First we’ll consider how to write the new loss function, we need to calculate the log-likelihood of the observations then we wish to maximise this. Neural networks in PyTorch have optimisers with minimise the loss function, hence we will minimise the negative log-likelihood. Let’s first consider the probability density function of a Normal distribution with mean <span class="math inline">\(\mu \in \mathbb{R}\)</span> and standard deviation <span class="math inline">\(\sigma \in \mathbb{R}^+\)</span>:</p>
<p><span class="math display">\[p(y | \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{1}{2\sigma^2}(y-\mu)^2\right)\]</span></p>
<p>We have multiple observations which are independent and normally distributed, hence for each batch we will need to calculate the product of the likelihood:</p>
<p><span class="math display">\[p(\mathbb{y}|\mu, \sigma) = \prod_{i=1}^B \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{1}{2\sigma^2}(y_i-\mu)^2\right),\]</span></p>
<p>Where <span class="math inline">\(B\)</span> is the batch size, this involves multiplying small numbers together which can result in arithmetic underflow, hence we work on the log-scale which changes the calculation to addition:</p>
<p><span class="math display">\[\log p(\mathbb{y}|\mu, \sigma) =  - \frac{B}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^B (y_i-\mu)^2  \]</span></p>
<p>In the case of the Neural net, <span class="math inline">\(\mu\)</span> is the output and <span class="math inline">\(\sigma\)</span> is an additional parameter. We can code the log-likelihood by converting this function into PyTorch or using the build in distributions.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributions <span class="im">import</span> Normal</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>normal_model <span class="op">=</span> NonLinearRegression(<span class="fl">0.1</span>, <span class="dv">64</span>, <span class="dv">3</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> model(x_input)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> torch.tensor(y_train, dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> inputs</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>log_sigma <span class="op">=</span> torch.tensor(<span class="fl">0.0</span>, dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate manually</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>manual_ll <span class="op">=</span> <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> math.log(<span class="fl">2.0</span> <span class="op">*</span> math.pi) <span class="op">-</span> <span class="op">\</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>  log_sigma <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> ((labels <span class="op">-</span> mu) <span class="op">/</span> log_sigma.exp()) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the built in log_prob function</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>log_likelihood <span class="op">=</span> Normal(mu, log_sigma.exp()).log_prob(labels)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>log_likelihood.<span class="bu">sum</span>(), manual_ll.<span class="bu">sum</span>()</span></code></pre></div>
<pre><code>(tensor(-597.9736, grad_fn=&lt;SumBackward0&gt;), tensor(-597.9736, grad_fn=&lt;SumBackward0&gt;))</code></pre>
</div>
<p>We can modify the <code>nn.Module</code> to return an additional parameter representing the log of the standard deviation of the Normal distribution. Additionally, the forward model will return the predicted mean and the global standard deviation - this is necessary when using Skorch, since the standard form of the loss function is <code>def loss(y_pred, y_true)</code>.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NonLinearRegressionNormal(nn.Module):</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dropout, hidden_size, hidden_layers):</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(NonLinearRegressionNormal, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">input</span> <span class="op">=</span> nn.Linear(<span class="dv">1</span>, hidden_size)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linears <span class="op">=</span> nn.ModuleList(</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>          [nn.Linear(hidden_size, hidden_size) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_layers)])</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropouts <span class="op">=</span> nn.ModuleList(</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>          [nn.Dropout(dropout) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_layers)])</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output <span class="op">=</span> nn.Linear(hidden_size, <span class="dv">1</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.log_sigma <span class="op">=</span> nn.Parameter(torch.tensor(<span class="fl">1.0</span>))</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.<span class="bu">input</span>(x)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> l, d <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.linears, <span class="va">self</span>.dropouts):</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>          x <span class="op">=</span> l(x)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>          x <span class="op">=</span> F.relu(x)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>          x <span class="op">=</span> d(x)</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output(x)</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x, <span class="va">self</span>.log_sigma</span></code></pre></div>
</div>
<p>The standard deviation, <code>sigma</code>, is constant, so in a custom training loop (not using skorch) we could simply pass the model definition to the loss function directly and extract the <code>sigma</code> parameter as follows. This would mean the <code>forward</code> function of the <code>nn.Module</code> can remain the same as the first model. The loss function could look like the following:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normal_pdf(model, X, y_true):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  preds <span class="op">=</span> model(X)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> Normal(preds, model.log_sigma.exp()).log_prop(y_true).<span class="bu">sum</span></span></code></pre></div>
</div>
<p>Then create a loss function as an <code>nn.Module</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skorch.regressor <span class="im">import</span> NeuralNetRegressor</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NormalLoss(nn.Module):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span>  <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">super</span>(NormalLoss, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> forward(<span class="va">self</span>, inputs, labels):</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    mu, log_sigma <span class="op">=</span> inputs</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    log_likelihood <span class="op">=</span> Normal(mu, log_sigma.exp()).log_prob(labels)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span> log_likelihood.<span class="bu">sum</span>()</span></code></pre></div>
</div>
<p>Train the model as usual:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>net_normal <span class="op">=</span> NeuralNetRegressor(</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    module<span class="op">=</span>NonLinearRegressionNormal,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    module__dropout<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    module__hidden_size<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    module__hidden_layers<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span>torch.optim.AdamW,</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    iterator_train__shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    criterion<span class="op">=</span>NormalLoss,</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    max_epochs<span class="op">=</span>epochs,</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>x_train_input <span class="op">=</span> torch.tensor(x_train, dtype<span class="op">=</span>torch.<span class="bu">float</span>).unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>net_normal.fit(x_train_input, torch.tensor(y_train, dtype<span class="op">=</span>torch.<span class="bu">float</span>))</span></code></pre></div>
<pre><code>&lt;class &#39;skorch.regressor.NeuralNetRegressor&#39;&gt;[initialized](
  module_=NonLinearRegressionNormal(
    (input): Linear(in_features=1, out_features=64, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linears): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): Linear(in_features=64, out_features=64, bias=True)
    )
    (dropouts): ModuleList(
      (0): Dropout(p=0.1, inplace=False)
      (1): Dropout(p=0.1, inplace=False)
      (2): Dropout(p=0.1, inplace=False)
    )
    (output): Linear(in_features=64, out_features=1, bias=True)
  ),
)</code></pre>
</div>
<p>Then calculate some predictions, the actual function is plotted using blue, with the predictions and prediction interval plotted in red. This is an MLE solution with a 95% confidence interval.</p>
<div class="layout-chunk" data-layout="l-body">
<p><img src="mc-dropout-uncertainty_files/figure-html5/unnamed-chunk-1-7.png" width="624" /></p>
</div>
<h2 id="uncertainty-with-mc-dropout">Uncertainty with MC Dropout</h2>
<p>We have looked at how to incorporate aleatoric uncertainty, to understand the uncertainty in the parameters (epistemic uncertainty) we can use MC Dropout. Dropout is a method of avoiding overfitting at training time by removing “connections” in a neural network. However, if we leave dropout on when making predictions, then we create an ensemble of models which output slightly different predictions. It turns out that this is equivalent Bayesian variational inference with some assumptions. We can then calculate the mean and and uncertainty intervals we wish.</p>
<p>We can easily implement MC dropout and visualise the uncertainty provided with this method. First, we extract the module from the Skorch <code>NeuralNet</code> class and put it in train mode, this means we make predictions with dropout ON.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>net_normal.module_.train()</span></code></pre></div>
<pre><code>NonLinearRegressionNormal(
  (input): Linear(in_features=1, out_features=64, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (linears): ModuleList(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): Linear(in_features=64, out_features=64, bias=True)
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (dropouts): ModuleList(
    (0): Dropout(p=0.1, inplace=False)
    (1): Dropout(p=0.1, inplace=False)
    (2): Dropout(p=0.1, inplace=False)
  )
  (output): Linear(in_features=64, out_features=1, bias=True)
)</code></pre>
</div>
<p>Let’s write a convenience function for making multiple predictions and combining them into a single numpy array.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_predictions(model, x, n):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Make multiple predictions</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  mus, sigmas <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>[model(x) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n)])</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Sample from the observation distribution using each mean and the global sigma</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> [Normal(mu, log_sigma.exp()).sample().detach().numpy() <span class="cf">for</span> mu <span class="kw">in</span> <span class="bu">list</span>(mus)]</span></code></pre></div>
</div>
<p>We then have a collection of predictions which we can use to calculate summaries using <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">monte carlo</a>. We can calculate the expectation by calculating the mean of all the predictions, and probability intervals by ordering the predictions and selecting the appropriate values, using <code>np.quantile</code>.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_probability_interval(preds, interval):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  lower_int <span class="op">=</span> (<span class="fl">1.</span> <span class="op">-</span> interval) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>  upper_int <span class="op">=</span> interval <span class="op">+</span> lower_int</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculate percentiles and mean</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>  lower <span class="op">=</span> np.quantile(preds, lower_int, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>  mean <span class="op">=</span> np.mean(preds, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>  upper <span class="op">=</span> np.quantile(preds, upper_int, axis<span class="op">=</span><span class="dv">0</span>) </span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> lower, mean, upper</span></code></pre></div>
</div>
<p>First, we will predict a single point, calculate the mean and the 89% probability interval.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>test_point <span class="op">=</span> <span class="fl">5.0</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> get_predictions(net_normal.module_, </span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    torch.tensor(test_point, dtype<span class="op">=</span>torch.<span class="bu">float</span>).unsqueeze(<span class="op">-</span><span class="dv">1</span>), </span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1000</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>lower, mean, upper <span class="op">=</span> get_probability_interval(preds, <span class="fl">0.89</span>)</span></code></pre></div>
</div>
<div class="layout-chunk" data-layout="l-body">
<p><img src="mc-dropout-uncertainty_files/figure-html5/unnamed-chunk-1-9.png" width="624" /></p>
</div>
<p>Next, we can use the same method to predict several points and give the impression of a function. If we extend the <span class="math inline">\(\sin\)</span> function we can see that the uncertainty on the inputs which are out of the domain of the training examples is quite large. We can evaluate the calibration of these intervals in domain and out of domain by creating a new training split which omits data in a certain interval.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>x_in <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">15</span>, <span class="dv">50</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> get_predictions(</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    net_normal.module_, </span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    torch.tensor(x_in, dtype<span class="op">=</span>torch.<span class="bu">float</span>).unsqueeze(<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1000</span>)</span></code></pre></div>
</div>
<div class="layout-chunk" data-layout="l-body">
<p><img src="mc-dropout-uncertainty_files/figure-html5/unnamed-chunk-1-11.png" width="624" /></p>
</div>
<h2 id="calculate-coverage-of-probability-interval">Calculate coverage of probability interval</h2>
<p>We can calculate the coverage of the probability interval using an experiment. We first train a neural network generating data from the same noisy function. We have <span class="math inline">\(\mu = 5\sin(x + 3)\)</span> and the observations corrupted by Gaussian noise, <span class="math inline">\(y_i \sim \mathcal{N}(\mu, 1^2)\)</span>. Then we sample 100 random uniform test points between 0 and 10 and make probabilistic predictions by calculating 1,000 predictions using MC Dropout and calculating 95% probability intervals. This should account for epistemic uncertainty in the parameters and aleatoric uncertainty inherent in the observed data. We then calculate the proportion of predictions which fall into the interval, this should be close to 95%.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> coverage_experiment():</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Generate training data</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>  x_train <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">50</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>  _, y_train <span class="op">=</span> generate_data(x_train)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Fit model to training data</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>  x_train_input <span class="op">=</span> torch.tensor(x_train, dtype<span class="op">=</span>torch.<span class="bu">float</span>).unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>  net_normal.fit(x_train_input, torch.tensor(y_train, dtype<span class="op">=</span>torch.<span class="bu">float</span>))</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Generate testing data from the same generative model</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>  n_test <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>  x_test <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">10</span>, n_test)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>  _, y_test <span class="op">=</span> generate_data(x_test)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>  net_normal.module_.train()</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculate predictions on test data</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>  preds <span class="op">=</span> get_predictions(</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>      net_normal.module_,</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>      torch.tensor(x_test, dtype<span class="op">=</span>torch.<span class="bu">float</span>).unsqueeze(<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>      <span class="dv">1000</span>)</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>  lower, mean, upper <span class="op">=</span> get_probability_interval(preds, <span class="fl">0.95</span>)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculate proportion of predictions which fall into interval</span></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>  in_interval <span class="op">=</span> <span class="bu">sum</span>([l <span class="op">&lt;=</span> y <span class="op">&lt;=</span> u <span class="cf">for</span> y, l, u <span class="kw">in</span> <span class="bu">zip</span>(y_test, lower, upper)])</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> in_interval <span class="op">/</span> n_test</span></code></pre></div>
</div>
<p>Let’s repeat this experiment 100 times and plot a histogram of the resulting coverage.</p>
<div class="layout-chunk" data-layout="l-body">
<p><img src="mc-dropout-uncertainty_files/figure-html5/unnamed-chunk-1-13.png" width="624" /></p>
</div>
<div class="sourceCode" id="cb23"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-gal2016dropout" class="csl-entry" role="doc-biblioentry">
Gal, Yarin, and Zoubin Ghahramani. 2016. <span>“Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.”</span> In <em>International Conference on Machine Learning</em>, 1050–59. PMLR.
</div>
<div id="ref-izmailov2021bayesian" class="csl-entry" role="doc-biblioentry">
Izmailov, Pavel, Sharad Vikram, Matthew D Hoffman, and Andrew Gordon Wilson. 2021. <span>“What Are Bayesian Neural Network Posteriors Really Like?”</span> <em>arXiv Preprint arXiv:2104.14421</em>.
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
  <h3 id="references">References</h3>
  <div id="references-listing"></div>
  <h3 id="citation">Citation</h3>
  <p>For attribution, please cite this work as</p>
  <pre class="citation-appendix short">Law (2021, Aug. 16). Bayesian Statistics and Functional Programming: Uncertainty in Neural Networks. Retrieved from https://jonnylaw.rocks/posts/2021-08-16-mc-dropout-uncertainty/</pre>
  <p>BibTeX citation</p>
  <pre class="citation-appendix long">@misc{law2021uncertainty,
  author = {Law, Jonny},
  title = {Bayesian Statistics and Functional Programming: Uncertainty in Neural Networks},
  url = {https://jonnylaw.rocks/posts/2021-08-16-mc-dropout-uncertainty/},
  year = {2021}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
