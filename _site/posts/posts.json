[
  {
    "path": "posts/2021-01-24-neural-networks-from-scratch/",
    "title": "Neural Networks in R",
    "description": "This post explores how to create a simple neural network to learn a linear function and a non-linear function using both standard R and the Torch library for R.",
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2021-02-02",
    "categories": [
      "R",
      "Deep Learning"
    ],
    "contents": "\nIn this post we will see how to train a neural network model using R and the Torch R library which is a port of the Python torch library without dependencies on Python. Torch provides tensors (n-dimensional arrays), automatic differentiation of tensors, optimisation routines and additional helpers for common deep learning tasks such as computer vision and audio processing.\nA neural network is built of layers. A layer consists of a set of weights, which are the parameters of the layer and an activation function (this can have additional parameters too). A single layer looks like the linear predictor in a generalised linear model,\n\\[\n\\eta = g(x^Tw),\n\\]\nwhere \\(\\eta\\) is the linear predictor, \\(g\\) is the linking function and \\(w\\) represent the coefficients of the covariates \\(x\\). In machine learning, \\(x\\) is referred to simply as the input, \\(w\\) are the weights and \\(g\\) is the activation function. There is often an additional parameter, termed the intercept in generalised linear models and the bias in machine learning. This can be rolled in to the weight vector by appending a one to the input / covariates. We can encapsulate this logic in a function\n\n\n\nWe also require a loss function, to understand how well our model is fitting to the data. For a regression problem we can use squared loss.\n\n\n\nWe will define a simple linear regression problem, our observations are noisy observations of a straight line.\n\\[y \\sim \\mathcal{N}(x^Tw, 2.0)\\]\n\n\n\nWe wish to learn the relationship between the inputs \\(x\\) and the outputs \\(y\\) using the model. To understand how well the model fits the observed data, we make a prediction by passing an observed input to the model (which is defined as a single layer), then we calculate how far the prediction is from the observed output using the squared loss function. The activation function is linear, or the identity function (function(x) x).\n\n[1] 3938.428\n\nTo find the best fit for this model to the observed data, we need to manipulate the weights to reduce the loss function. We can think of the weights as parameterising a family of related models, some of which may fit the data well.\nOptimisation\nTo find the maximum of a function in calculus, we first calculate the derivative and determine the point at which the slope is equal to zero. This can find both maximums and minimums, so we can additionally calculate the second derivative and if it’s negative then we have a maximum. This is fine for linear optimisation, however when it comes to non-linear optimisation we have to be more creative. We can use gradient descent to take steps in the opposite direction of the gradient to find the minimum of a non-linear function\n\n\n\nWe must calculate the derivative of the network with respect to the weight parameters. For a single layer network with univariate inputs, a linear activation function and a squared loss function the derivative is\n\\[\\frac{d \\text{ network}}{dw} = \\frac{d}{dw} (y - x^Tw)^2 = -2x^T(y-x^Tw).\\]\nWe can encapsulate this derivative as a function.\n\n\n\nWe can check the analytically calculated gradient using Torch. First we use our calculation of the gradient.\n\n          [,1]\n[1,] 0.4072788\n[2,] 0.3393112\n\nThen we must write the forward function of the model, pred and calculate the loss using the functions available on Torch tensors. We can then call backward() which performs reverse mode automatic differentiation then we can access the grad attribute of any tensor which has requires_grad = TRUE.\n\ntorch_tensor\n 0.4073\n 0.3393\n[ CPUFloatType{2,1} ]\n\nWe can fit this simple model by writing a training loop which updates the parameters using gradient descent. We keep track of the loss function at each iteration of gradient descent and plot it.\n\n\nobserved <- tibble(x, y) %>%\n  sample_n(50)\n\n\n\n\n\n\n\n\n\nSince this model is so small, consisting of only two weights. We can plot the actual function learned by the model using geom_abline.\n\n\n\nWe can try to use this model on a simple non-linear regression problem, of course we probably won’t do very well here! We define the regression problem as\n\\[y \\sim \\mathcal{N}(4\\sin(x), 1^2).\\]\nWe plot the true function and the observed values in red below.\n\n\n\n\n\n\nWe can then plot the learned function against the observed values and the true function. We can see that a straight line is not a good fit for this data, we need more flexibility in the network.\n\n\n\nUsing Torch\nIf we want to approximate a non-linear function we best use non-linear activation functions. We can calculate the derivative of each layer using automatic differentiation. We will use the R Torch library. We now initialise a torch tensor with the same values as x and pass it through the layers. We must re-write the layer and loss functions assuming the input is a torch_tensor. First we will re-write the linear example using Torch\n\nEpoch:  10    Loss:  109.8399 \nEpoch:  20    Loss:  90.48958 \nEpoch:  30    Loss:  74.59881 \nEpoch:  40    Loss:  61.54904 \nEpoch:  50    Loss:  50.83236 \nEpoch:  60    Loss:  42.03164 \nEpoch:  70    Loss:  34.80434 \nEpoch:  80    Loss:  28.86917 \nEpoch:  90    Loss:  23.99511 \nEpoch:  100    Loss:  19.99245 \nEpoch:  110    Loss:  16.70539 \nEpoch:  120    Loss:  14.00601 \nEpoch:  130    Loss:  11.78924 \nEpoch:  140    Loss:  9.968783 \nEpoch:  150    Loss:  8.473798 \nEpoch:  160    Loss:  7.246089 \nEpoch:  170    Loss:  6.237879 \nEpoch:  180    Loss:  5.409916 \nEpoch:  190    Loss:  4.729981 \nEpoch:  200    Loss:  4.171605 \n\n\n\n\nTry the non-linear example with multiple layers and a non-linear activation function from the first layer (where the input goes). We’ll also try a different optimizer, Adam.\n\nEpoch:  10    Loss:  8.857554 \nEpoch:  20    Loss:  5.104505 \nEpoch:  30    Loss:  3.292954 \nEpoch:  40    Loss:  1.361338 \nEpoch:  50    Loss:  0.6081427 \nEpoch:  60    Loss:  0.6318361 \nEpoch:  70    Loss:  0.5428851 \nEpoch:  80    Loss:  0.5050789 \nEpoch:  90    Loss:  0.4780622 \nEpoch:  100    Loss:  0.4523385 \n\nWe can plot the predictions alongside the observed values and the true function.\n\n\n\n\n\n\n",
    "preview": "posts/2021-01-24-neural-networks-from-scratch/neural-networks-from-scratch_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-02-02T09:59:39+00:00",
    "input_file": "neural-networks-from-scratch.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-05-01-hidden-markov-model/",
    "title": "Functional Programming and Hidden Markov Models",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2020-05-01",
    "categories": [
      "Bayesian",
      "R"
    ],
    "preview": "posts/2020-05-01-hidden-markov-model/distill-preview.png",
    "last_modified": "2020-07-30T08:18:13+01:00",
    "input_file": {},
    "preview_width": 831,
    "preview_height": 303
  },
  {
    "path": "posts/2020-04-19-multi-state-survival-models/",
    "title": "Multi State Models",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2020-04-19",
    "categories": [
      "R",
      "Bayesian"
    ],
    "preview": "posts/2020-04-19-multi-state-survival-models/distill-preview.png",
    "last_modified": "2020-07-30T11:57:36+01:00",
    "input_file": {},
    "preview_width": 769,
    "preview_height": 216
  },
  {
    "path": "posts/2020-03-17-tidy-tuesday-the-office/",
    "title": "Tidy Tuesday: The Office",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2020-03-17",
    "categories": [
      "tidy-tuesday",
      "R",
      "Bayesian"
    ],
    "preview": "posts/2020-03-17-tidy-tuesday-the-office/distill-preview.png",
    "last_modified": "2020-07-30T08:18:13+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-03-10-tidy-tuesday-us-tuition-data/",
    "title": "Tidy Tuesday: US Tuition Data",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2020-03-10",
    "categories": [
      "tidy-tuesday",
      "R"
    ],
    "preview": "posts/2020-03-10-tidy-tuesday-us-tuition-data/distill-preview.png",
    "last_modified": "2020-07-30T08:18:13+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-03-04-harrier_league_data/",
    "title": "Releasing Harrier League Data",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2020-03-04",
    "categories": [
      "R"
    ],
    "preview": "posts/2020-03-04-harrier_league_data/distill-preview.png",
    "last_modified": "2020-07-30T08:18:13+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-03-03-tidy_tuesday_nhl_data/",
    "title": "Tidy Tuesday: NHL Goalscorers",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2020-03-03",
    "categories": [
      "R",
      "tidy-tuesday"
    ],
    "preview": "posts/2020-03-03-tidy_tuesday_nhl_data/distill-preview.png",
    "last_modified": "2020-07-30T13:48:11+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-11-04-parsing-strava/",
    "title": "Analysing .fit files in R",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-11-04",
    "categories": [
      "R"
    ],
    "preview": "posts/2019-11-04-parsing-strava/distill-preview.png",
    "last_modified": "2020-07-30T08:18:13+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-08-05-ad_r/",
    "title": "Forward Mode AD in R",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-08-05",
    "categories": [
      "R"
    ],
    "preview": {},
    "last_modified": "2020-07-30T08:18:13+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-07-31-hmc/",
    "title": "Hamiltonian Monte Carlo in R",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-07-31",
    "categories": [
      "R",
      "Bayesian"
    ],
    "preview": "posts/2019-07-31-hmc/distill-preview.png",
    "last_modified": "2020-07-30T16:48:08+01:00",
    "input_file": "hmc.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-06-14-bayesian-linear-regression/",
    "title": "Bayesian Linear Regression with Gibbs Sampling in R",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-06-14",
    "categories": [
      "R",
      "Bayesian"
    ],
    "preview": "posts/2019-06-14-bayesian-linear-regression/distill-preview.png",
    "last_modified": "2020-07-30T11:46:56+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-04-16-multi-armed-bandits/",
    "title": "Multi-armed Bandits in Scala",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-04-16",
    "categories": [
      "Scala"
    ],
    "preview": "posts/2019-04-16-multi-armed-bandits/distill-preview.png",
    "last_modified": "2020-07-30T13:45:12+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-04-15-scala-and-jupyter-notebook-with-almond/",
    "title": "Scala and Jupyter Notebook with Almond",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-04-15",
    "categories": [
      "Scala"
    ],
    "preview": "posts/2019-04-15-scala-and-jupyter-notebook-with-almond/distill-preview.png",
    "last_modified": "2020-07-30T11:46:44+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-02-25-rejection_sampling/",
    "title": "Bayesian Inference using rejection sampling",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-02-25",
    "categories": [
      "R",
      "Bayesian"
    ],
    "preview": "posts/2019-02-25-rejection_sampling/distill-preview.png",
    "last_modified": "2020-07-30T11:46:40+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-02-25-sampling/",
    "title": "Sampling from a distribution with a known CDF",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-02-25",
    "categories": [
      "R"
    ],
    "preview": "posts/2019-02-25-sampling/distill-preview.png",
    "last_modified": "2020-07-30T11:46:41+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-02-22-national_xc/",
    "title": "A Statistical Model for Finishing Positions at the National Cross Country",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-02-22",
    "categories": [
      "R"
    ],
    "preview": "posts/2019-02-22-national_xc/distill-preview.png",
    "last_modified": "2020-07-30T13:47:03+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-02-11-metropolis_r/",
    "title": "Efficient Markov chain Monte Carlo in R with Rcpp",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2019-02-11",
    "categories": [
      "R",
      "Bayesian"
    ],
    "preview": "posts/2019-02-11-metropolis_r/distill-preview.png",
    "last_modified": "2020-07-30T11:56:45+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2017-10-26-harrier-league-cross-country/",
    "title": "Harrier League Cross Country",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2017-10-26",
    "categories": [
      "R"
    ],
    "preview": {},
    "last_modified": "2020-07-30T11:43:29+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-04-23-BreezeMcmc/",
    "title": "MCMC with Scala Breeze",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2017-04-23",
    "categories": [
      "Scala",
      "Bayesian"
    ],
    "preview": "posts/2017-04-23-BreezeMcmc/distill-preview.png",
    "last_modified": "2020-07-30T11:43:25+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2017-02-21-AkkaClient/",
    "title": "An Akka HTTP Client with JSON Parsing",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2017-02-21",
    "categories": [
      "Scala"
    ],
    "preview": {},
    "last_modified": "2020-07-30T11:43:22+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-01-04-FailureInFunctionalProgramming/",
    "title": "Using Monads for Handling Failures and Exceptions",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2017-01-04",
    "categories": [
      "Scala"
    ],
    "preview": {},
    "last_modified": "2020-07-30T11:43:21+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-12-13-SeasonalDlm/",
    "title": "Seasonal DLM",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2016-12-13",
    "categories": [
      "Bayesian",
      "Scala"
    ],
    "preview": "posts/2016-12-13-SeasonalDlm/distill-preview.png",
    "last_modified": "2020-07-30T11:43:20+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2016-12-12-KalmanFilter/",
    "title": "The Kalman Filter in Scala",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2016-12-12",
    "categories": [
      "Scala",
      "Bayesian"
    ],
    "contents": "\nA Dynamic Linear Model (DLM) is a special type of state space model, where the state and observation equations are Normally distributed and linear. A general DLM can be written as follows:\n\\[\\begin{aligned} y_t &= F_t x_t + \\nu_t,  &\\nu_t &\\sim \\mathcal{N}(0, V_t) \\\\\nx_t &= G_tx_{t-1} + \\omega_t &\\omega_t &\\sim \\mathcal{N}(0, W_t), \\end{aligned}\\]\n\\(y_t\\) represents the observation of the process at time \\(t\\), \\(x_t\\) is the value of the unobserved state at time \\(t\\). The observation error \\(\\nu_t\\) and the system error \\(\\omega_t\\) are independent and identically distributed Normal random variables. \\(F_t\\) is the observation matrix which transforms the state space to the observation, \\(G_t\\) is the state transition matrix.\nForward Simulating from the DLM\nA first order polynomial DLM with constant \\(V\\) and \\(W\\), and \\(F_t = 1\\), \\(G_t = 1\\). can be simulated in scala as follows:\nimport breeze.stats.distributions.Gaussian\n\ncase class Data(time: Time, observation: Observation, state: Option[State])\ncase class Parameters(v: Double, w: Double, m0: Double, c0: Double)\n\ndef simulate(p: Parameters): Stream[Data] = {\nval stateSpace = Stream.iterate(Gaussian(p.m0, sqrt(p.c0)).draw)(x => Gaussian(x, sqrt(p.w)).draw)\n  stateSpace.zipWithIndex map { case (x, t) =>\n    Data(t, x + Gaussian(0, sqrt(p.v)).draw, Some(x)) \n  }\n}\n\nval p = Parameters(3.0, 0.5, 0.0, 10.0)\n// simulate 16 different realisations of 100 observations, representing 16 stations\nval data = (1 to 16) map (id => (id, simulate(p).take(100).toVector))\nWe use the built in streaming library’s iterate function to specify the evolution of the latent state. The initial state \\(x_0\\) is a sample drawn from a normal distribution with mean \\(m_0\\) and variance \\(C_0\\), \\(x_t \\sim \\mathcal{N}(x_0 ; m_0, C_0)\\). The Breeze numerical computing library provides many statistical and mathematical functions is used. Subsequent states are generated by adding \\(\\mathcal{N}(0, W)\\) to the previous state:\n\\[x_t = x_{t-1} + \\omega, \\qquad \\omega \\sim \\mathcal{N}(0, V)\\]\nWe then construct a Stream of Data objects. The Data object has a timestamped observation and an optional state space. We construct the observation at time \\(t\\) by simply adding the observation noise to the state space \\(y_t \\sim \\mathcal{N}(y_t | x_t, V)\\). The state is optional because we can’t observe the state of real data, only simulated data will have a known state.\nA graph of the data from four “stations”, produced using ggplot2 in R is shown in the figure below.\n\n\n\nAn Aside on Referential Transparency\nNote that the function simulate is not referentially transparent, meaning the function will return a different Stream of data each time we run it. Referential transparency is important in functional programming, to allow us to easily reason about complex programs. The Breeze library implements another object for stateful random number generation, the Process object. The MarkovChain object can be used to construct a process without drawing explicitly from the distribution until we run the program. Firstly define a single step of the random walk:\nimport breeze.stats.distributions._\n\ndef step_rw(p: Parameters): Double => Rand[Double] = \n  x => Gaussian(x, p.w)\nIf step_rw is supplied with a set of Parameters, it returns a function from the current state, which is assumed to be materialised, to the next state, which is a Rand[Double]. The actual random number isn’t generated until we sample from distribution represented by Rand. Next, we can construct a MarkovChain using the transition kernel stepRw:\nval random_walk: Process[Double] = MarkovChain(0.0)(step_rw(p))\nrandom_walk.\n  steps.\n  take(100)\nThe Kalman Filter\nThe Kalman Filter can be used to determine the posterior distribution of the state space given the current observation and the \\(p(x_t|D_{t})\\), where \\(D_t = \\{Y_t, D_{t-1}\\}\\) and \\(D_0\\) is the initial information, comprising of the parameters mode parameters \\(W_0\\) and \\(V_0\\) and the initial state \\(x_0 \\sim N(m_0, C_0)\\). The full treatment of the Kalman Filter can be found in the excellent Bayesian Forecasting for Dynamic Models by West and Harrison.\nI will present the filtering equations for the simple model in this post. Suppose we start with the posterior distribution of \\(x_{t-1} \\sim N(m_{t-1}, C_{t-1})\\). The first thing we need to do is advance the state, the equation to advance the state is a simple Markov transition \\(x_t = x_{t-1} + \\omega_t\\). We simply add the system variance, the system variance is drawn from a Normal distribution with zero mean and variance \\(W\\). The sum of two Normal distributions is Normal with the mean and variance added, hence the prior for \\((x_t | D_{t-1}) \\sim N(m_{t-1}, C_{t-1} + W)\\).\nNext we need to construct the observation, using the observation equation which is commonly called the one-step forecast for the series, \\(y_t = x_t + \\nu_t\\), since \\(\\nu_t\\) is Normally distributed with zero mean and variance \\(V\\), the distribution of the one step forecast is, \\((y_t|D_{t-1}) \\sim N(m_{t-1}, C_{t-1} + W + V)\\).\nNow, we observe the true value of \\(y_t\\) and are able to construct the posterior distribution of \\((x_t | D_t) \\sim N(m_t, C_t)\\). \\(m_t = m_{t-1} + A_t e_t\\) and \\(C_t = A_tV\\). \\(A_t = \\frac{C_{t-1} + W}{ C_{t-1} + W + V}\\) is known as the regression coefficient, and \\(e_t = Y_t - m_{t-1}\\). This result can be shown using properties of the multivariate normal distribution, and is presented in full in Bayesian Forecasting for Dynamic Models by West and Harrison.\nWe now have the equations required to program up the Kalman Filter using Scala.\ncase class FilterState(data: Data, p: Parameters)\n\ndef filter(p: Parameters): (FilterState, Data) => FilterState = (s, d) => {\n  val r = s.p.c0 + p.w\n  val q = r + p.v\n  val e = d.observation - s.p.m0\n\n  // kalman gain\n  val k = r / q\n  val c1 = k * p.v\n  \n  // return the data with the expectation of the hidden state and the updated Parameters\n  FilterOut(Data(d.time, d.observation, Some(m1)), Parameters(p.v, p.w, m1, c1))\n} \nThe function filter simply takes in Parameters and one observation, represented by Data and returns the updated parameters required for the next step of the filter. Now we need to write a function which filters a sequence of Data, and returns a sequence consisting of the latent states, which we can do using the function scanLeft.\nA simplified function signature of scanLeft is given by: scanLeft[A](l: List[A], z: A)(f: (A, A) => A): List[A]. A list, l with elements of type A and an initial value, z also of type A if passed to a Function2 and accumulated into another list with elements of type A. The function f is applied to each element of the list pairwise, starting the the head of the lift and the zero element, z. Consider calculating the sum of a list of numbers:\nval numbers = List(1,2,3,4,5)\ndef sum(a: Int, b: Int): Int = a + b\n\nnumbers.scanLeft(0)(sum)\n// List(0, 1, 3, 6, 10, 15)\nThe first calculation is (0 + 1) = 1, which is then used as the first argument in the pairwise sum function, then the second calculation is (1 + 2) = 3, the result of which is used again in the next application of sum. Each intermediate step of the calculation is retained and appended to a list to be output when the list of number is exhausted, so we end up with a cumulative sum, List(0, 1, 3, 6, 10, 15). We can use scanLeft and the Function2, filter to calculate and retain the latent states in a DLM:\ndef filterSeries(data: Seq[Data], initParams: Parameters): Seq[FilterOut] = {\n  val initFilter = FilterState(data.head, params, 0.0) // initialise the filter\n\n  data.\n    scanLeft(initFilter)(filter(initParams)).\n    drop(1)\n}\nNow, we can apply the filter to all the stations simultaneously:\ndata.\n  groupBy{ case (id, _) => id }. //groups by id\n  map{ case (id, idAndData) =>\n  (id, idAndData map (x => x._2)) }. // changes into (id, data) pairs\n  map{ case (id, data) =>\n  (id, filterSeries(data.sortBy(_.time), p)) } // apply the filter to the sorted data\nWe can now plot the results of the filtering using R and ggplot2, overlaid with 95% prediction intervals.\n\n\nfiltered = read_csv(\n  here::here(\"notebooks/data/filteredDlm.csv\"),\n  c(\"stationId\", \"time\", \"observation\", \"stateMean\", \"m\", \"c\")\n)\n\n## calculate upper and lower 95% bounds of the state estimate\nfiltered %>%\n  select(-observation) %>%\n  inner_join(data, by = c(\"time\", \"stationId\")) %>%\n  filter(stationId %in% 1:4) %>%\n  mutate(upper = qnorm(p = 0.975, mean = m, sd = sqrt(c)), \n         lower = qnorm(p = 0.025, mean = m, sd = sqrt(c))) %>%\n  select(-c, -m, -observation) %>%\n  gather(key, value, -time, -stationId, -upper, -lower) %>%\n  ggplot(aes(x = time, y = value, linetype = key)) + \n  geom_line() +\n  geom_ribbon(aes(x = time, ymin = lower, ymax = upper), alpha = 0.3) +\n  facet_wrap(~stationId, scales = \"free\")\n\n\n\n\nThe full code is available in a notebook file and an ammonite script in the GitHub Repo associated with this blog.\n\n\n\n",
    "preview": "posts/2016-12-12-KalmanFilter/distill-preview.png",
    "last_modified": "2021-02-02T09:59:24+00:00",
    "input_file": "KalmanFilter.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2016-12-01-PracticalAkkaStreams/",
    "title": "Practical Introduction to Akka Streaming",
    "description": {},
    "author": [
      {
        "name": "Jonny Law",
        "url": {}
      }
    ],
    "date": "2016-12-01",
    "categories": [
      "Scala"
    ],
    "preview": {},
    "last_modified": "2020-07-30T11:43:16+01:00",
    "input_file": {}
  }
]
