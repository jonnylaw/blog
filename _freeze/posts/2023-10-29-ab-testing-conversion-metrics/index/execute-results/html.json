{
  "hash": "e32001163f89ff4ce0623f6c3ad203fb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"A/B Testing with Conversion Metrics\"\ndescription: \"Using the Fisher exact test to analyse the results of an A/B test with a conversion metric outcome.\"\ndate: \"2023-10-29\"\ndraft: true\n---\n\n\n\n\n\n\n\n\nAn A/B test is the name given to a randomised experiment to determine which variant, A or B, performs better according to a pre-selected metric. For example consider a website with a call-to-action to download our application. A content marketer may want to try a new, more exciting phrase to encourage users to download the application, but how can you know that this new phrase is driving more downloads? Maybe the development team has implemented new features which are responsible for the growth in users by word-of-mouth, or targeted search engine advertising is delivering more valuable leads.\n\nIn this blog post we will show an example of a frequentist null-hypothesis significance testing (NHST) method to answer the question of which variant is best. One advantage of frequentist testing is that it is well-established and widely understood, but it is not without its critics. We will use the Fisher exact test to analyse the results of an A/B test with a conversion metric outcome.\n\n## Non-statistical considerations\n\nFirst, there are some non-statistical considerations which can invalidate the assumptions of our A/B test. \n\n- How we are assigning users to variants\n- How large an effect do we want to detect\n- How many users can we actually get\n- How much does it cost to run this experiment\n- We want a conclusion which is understandable\n\n## Statistical considerations\n\nThere are several statistical considerations to take into account.\n\n1. We want to assign users randomly to each variant\n2. We want to determine the sample size to reach a practical effect size\n3. We want to conclude using statistical significance\n4. We can't test multiple times (no early peeking)\n\n# Null Hypothesis Significance Testing\n\nFirst we specify the null hypothesis, then we assume the data is generated under the assumption that the null hypothesis is true. If the data is unlikely according to the null hypothesis, then we reject the null hypothesis. Note that we can never accept the null hypothesis.\n\n# Fisher exact test\n\nThe Fisher-Exact test uses the Hypergeometric distribution, this distribution is equivalent to a bag containing $n$ black and $m$ white balls, then selecting a single ball and see if it is a black or a white. This is very similar to the conversion experiment for the A/B test, we could consider pulling a black ball as a successful conversion and a white as a bounce from the page. \nContingency table\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(results <- matrix(c(95, 80, 5, 20),\n       nrow = 2,\n       dimnames = list(\n         A = c(\"not converted\", \"converted\"),\n         B = c(\"not converted\", \"converted\")\n       )))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               B\nA               not converted converted\n  not converted            95         5\n  converted                80        20\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfisher.test(results, alternative = \"two.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  results\np-value = 0.002197\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  1.621308 16.815780\nsample estimates:\nodds ratio \n  4.715836 \n```\n\n\n:::\n:::\n\n\n\n\n\n## Statistical Power\n\nStatistical power is defined as the probability of rejecting the null hypothesis, given that it is false. We can calculate the statistical power using simulation.\n\n1. Simulate data from a hypothetical experiment $n$ times\n  - Specify the sample size for each variant, A and B\n  - Specify the true conversion for each variant\n  - Simulate from the Binomial distribution with these parameters\n2. Apply the statistical test to each hypothetical observation and collect the p-values\n3. Determine the proportion of tests which have a p-value below the threshold (usually 0.05)\n\nWe can apply this formula to any of the tests we consider in this blog post, we can even re-use the function we use to simulate the data.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_data <- function(n, p_a, p_b) {\n  a_converted <- rbinom(1, n, p_a)\n  b_converted <- rbinom(1, n, p_b)\n\n  matrix(\n    c(a_converted, b_converted, n - a_converted, n - b_converted),\n         nrow = 2,\n  )\n}\n\nsimulate_data(200, 0.05, 0.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]   16  184\n[2,]   16  184\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nexperiment <- function(n, p_a, p_b) {\n  obs <- simulate_data(n, p_a, p_b)\n  tryCatch(fisher.test(obs)$p.value, error = function(e) e,  TRUE)\n}\n\ncalculate_power <- function(sample_size, p_a = 0.5, p_b = 0.6, reps = 3000) {\n  sum(replicate(experiment(sample_size, p_a, p_b) < 0.05, n = reps, )) / reps\n}\n\npower_results <- tibble(\n  sample_size = seq(100, 1000, 100),\n  power = map_dbl(.x = sample_size, .f = ~ calculate_power(.))  \n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npower_results %>% \n  ggplot(aes(x = sample_size, y = power)) + \n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Power of two-sided Fisher exact test\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Determining the required Sample Size\n\nTo determine the required sample size we can use a bisection search. We start with a sample size, calculate the power, then adjust the sample size based on the power. We can use the `optimise` function in R to find the sample size which gives us the desired power.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncalculate_sample_size <- function(power_target, n_start, tol = 0.05) {\n  max_reps <- 100\n  n <- n_start\n  current_power <- calculate_power(sample_size = n)\n  n_reps <- 1\n  while (abs(current_power - power_target) > tol || n_reps > max_reps) {\n    if (current_power < power_target) {\n      nlast <- n\n      n <- 2 * n\n    } else if (is.null(nlast)) {\n      nlast <- n\n      n <- n / 2\n    } else {\n      new_n <- (n + nlast) / 2\n      nlast <- n\n      n <- new_n\n    }\n    current_power <- calculate_power(sample_size = n)\n    n_reps <- n_reps + 1\n  }\n  n\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncalculate_sample_size(0.8, 100, 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 400\n```\n\n\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}