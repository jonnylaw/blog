{
  "hash": "3a57e753f66d2ac552c20f6a2e6dca7b",
  "result": {
    "markdown": "---\ntitle: \"Multi-armed Bandits in Scala\"\nauthor: \"Jonny Law\"\ndate: '2019-04-16'\nslug: multi-armed-bandits\ncategories: \n  - Scala\n---\n\n\n\n\n## Setting up the Environment\n\nThis post uses [Almond](https://almond.sh/) in order to run Scala code in a Jupyter notebook. See [my previous post](/2019/04/15/scala-and-jupyter-notebook-with-almond/) to learn how to setup Jupyter, Ammonite and Almond. That post examined using the Scala libraries EvilPlot (including inline plotting in the Jupyter notebook) and Rainier for Bayesian inference in a simple linear model.\n\nThe imports required for this post are:\n\n```scala\nimport coursier.MavenRepository\n\ninterp.repositories() ++= Seq(MavenRepository(\n  \"http://dl.bintray.com/cibotech/public\"\n))\n\nimport $ivy.`com.stripe::rainier-core:0.2.2`\nimport $ivy.`com.stripe::rainier-plot:0.2.2`\nimport $ivy.`org.scalanlp::breeze:0.13.2`\n\nimport breeze.stats.distributions._\nimport breeze.linalg._\nimport almond.interpreter.api._\n```\n\nThe full working notebook can be found [here](https://github.com/jonnylaw/blog/tree/master/notebooks/multi_armed_bandit.ipynb).\n\n# A Multi-armed Bandit\n\nA multi-armed bandit is an analogy taken from the one-armed bandit slot machines where a lever is pulled and the player has an unknown probability of a prize. A multi-armed bandit is a generalisation, whereby the player is faced with multiple one-armed bandits each of which could have different rewards. The problem is to determine the best bandit to play. One way to determine this is to randomly pull levers to get information on the payout for each bandit. Assuming the probability of payout is constant in time, then after a period of exploration the player will be able to know which bandits pay the most.\n\n# Epsilon Greedy Method\n\nOne strategy to maximise the expected long-term reward from a bandit is to choose the bandit with the largest long-term reward a fraction of the time and the rest of the time choose a bandit uniformly at random in order to continually explore the space of actions. At each time step, the reward for a given action can be calculated and the long-term reward can be calculated as the function:\n\n\n$$Q_{t+1}(A) = Q_t(A) + \\frac{R_t(A) - Q_t(A)}{N_t(A)}$$\nwhere $A$ is the current action, $Q_t(A)$ is the long-term reward at time $t$ for action $A$, $R_t(A)$ is the instantaneous reward for action $A$ at time $t$ and $N_t(A)$ is the total number of times action $A$ has been performed by time step $t$. Before writing the algorithm for the epsilon greedy algorithm, first we define a few helper functions.\n\nThe first is to sample a value uniformly a random from a selection of values.\n\n```scala\ndef sample(selection: Vector[Int]): Rand[Int] = {\n    for {\n        i <- Multinomial(DenseVector.ones[Double](selection.size))\n    } yield selection(i)\n}\n```\n\nIf there are multiple actions with the same long-term reward then the next action should be selected randomly from all the actions which maximise the long term reward.\n\n```scala\ndef maxActionWithTies(longTermReward: Vector[Double]): Rand[Int] = {\n    val maxReward = longTermReward.max\n    val rewards = longTermReward.zipWithIndex.\n          filter { case (r, a) => r == maxReward }.map(_._2)\n    if (rewards.size > 1) {\n        sample(rewards)        \n    } else {\n        Rand.always(rewards.head)\n    }\n}\n```\n\nThen a single step of the epsilon greedy algorithm can be written\n\n```scala\ncase class BanditState(\n    reward: Array[Double],\n    longTermReward: Vector[Double],\n    actions: Map[Int, Int]\n)\ndef banditStep(\n    epsilon: Double,\n    reward: Int => Rand[Double],\n    selectAction: (Map[Int, Int], Vector[Double]) => Rand[Int])(s: BanditState): Rand[BanditState] = {\n    for {\n        nextAction <- selectAction(s.actions, s.longTermReward)\n        newReward <- reward(nextAction)\n        prevCount = s.actions.get(nextAction).get\n        nextCount  = prevCount + 1\n        newLongTermReward = s.longTermReward(nextAction) + (newReward - s.longTermReward(nextAction)) / nextCount\n    } yield BanditState(s.reward :+ newReward, \n                s.longTermReward.updated(nextAction, newLongTermReward),\n                s.actions.updated(nextAction, nextCount))\n}\n```\n\nFirstly, we define a `BanditState` which contains all of the rewards $R_t(A)$ for each time step, a list of length equal to the number of actions containing the long-term reward for each action. `actions` represents $N_t(A)$ using a map from the index of the action to the count of actions. The algorithm proceeds by sampling a uniform random number, if this number is less than the chosen value of epsilon, then a random action is sampled from a Multinomial distribution with equal probabilities, otherwise the algorithm selects the action which currently has the highest long-term reward. The values are updated according to the formula above.\n\nTo run this algorithm for a pre-determined number of steps, realise that it is recursive and completely determined by the count and long-term reward at the previous time step. Hence it can be implemented as a Markov chain.\n\n```scala\ndef buildActions(actions: Int): Map[Int, Int] = {\n    (0 until actions).map(a => a -> 0).toMap\n}\n\ndef epsilonGreedy(\n    epsilon: Double, \n    actions: Int, \n    reward: Int => Rand[Double],\n    n: Int): BanditState = {\n    \n    val initState = BanditState(Array(0.0), Vector.fill(10)(0.0), buildActions(actions))\n    MarkovChain(initState)(banditStep(epsilon, reward, selectGreedy(epsilon))).steps.drop(n-1).next\n}\n```\n\nWe can assume that the rewards for each of the ten actions is Normally distributed and define a suitable reward function\n\n```scala\nval qs = Gaussian(0, 1).sample(10)\n\n// The reward is selected from a N(q(A_t), 1)\ndef r(qa: Seq[Double])(action: Int): Rand[Double] = \n    Gaussian(qa(action), 1)\n    \n//  qs: IndexedSeq[Double] = Vector(\n//    -1.1319170735731177,\n//    0.5392647196381599,\n//    0.7127636875526561,\n//    0.8765526115252499,\n//    -0.9555744042626685,\n//    -0.2723645491439034,\n//    0.10029206857194808,\n//    0.3758538986470721,\n//    1.9412629812694995,\n//    1.0620845496569054\n//)\n```\n\nThen the algorithm can be run for a single multi-armed bandit\n\n```scala\nval oneBandit = epsilonGreedy(0.5, 10, r(qs), 1000)\n```\n\nThe distribution of actions at the end of 1,000 steps with epsilon = 0.5 and number of actions 10 is:\n\n::: {.cell}\n\n```{.r .cell-code}\naction_distribution <- read_csv(here::here(\"notebooks/data/action_distribution.csv\"), \n                                col_names = c(\"action\", \"count\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 10 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): action, count\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\naction_distribution %>% \n  ggplot(aes(x = action, y = count)) +\n  geom_col()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/action_distribution-1.png){width=672}\n:::\n:::\n\nThe mean reward at action 8 was highest at approximately 1.95, the epsilon-greedy algorithm prefers to take action 8.\n\n# Multiple Multi-armed Bandits\n\nNow to see the behaviour of a typical multi-armed bandit with a constant reward function, calculate the average reward for n = 2,000 10-arm bandits each with 1,000 steps.\n\n```scala\nVector.fill(2000)(DenseVector(bandit(0.1, 10, r(qs), 1000).reward)).\n  reduce(_ + _).\n  map(_ / 2000)\n```\n\nThe average reward for each time step can then be plotted, this can be used to evaluate different choices of epsilon. Different values of epsilon can be compared and the average reward can be calculated\n\n```scala\nval data = List(0.0, 0.1, 0.5).map ( eps => averageReward(2000, 1000, eps))\n```\n\n::: {.cell}\n\n```{.r .cell-code}\naverage_reward <- read_csv(here::here(\"notebooks/data/average_reward.csv\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 3003 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): epsilon, step, reward\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\naverage_reward %>% \n  ggplot(aes(x = step, y = reward, colour = as.factor(epsilon))) +\n  geom_line() +\n  geom_label(data = filter(average_reward, step == 1000), \n            aes(label = epsilon, x = 950, y = reward), hjust = .5) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Average reward by step for various values of epsilon\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}