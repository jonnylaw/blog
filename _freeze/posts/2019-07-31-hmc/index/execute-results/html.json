{
  "hash": "a264f7b0cafd29cc5759f11e0646d713",
  "result": {
    "markdown": "---\ntitle: \"Hamiltonian Monte Carlo in R\"\nauthor: \"Jonny Law\"\ndate: '2019-07-31'\nslug: hamiltonian_monte_carlo_in_R\noutput: distill::distill_article\ncategories:\n  - R\n  - Bayesian\n---\n\n\n\n\n## Introduction\n\nDetermining the posterior distribution for the parameters of a real-world Bayesian model inevitably requires calculating high-dimensional integrals. Often these are tedious or impossible to calculate by hand. Markov chain Monte Carlo (MCMC) algorithms are popular approaches, samplers such as the [Gibbs sampler](/2019-06-14-bayesian-linear-regression-gibbs) can be used to sample from models with conditionally [conjugate](https://en.wikipedia.org/wiki/Conjugate_prior) specifications and the [Metropolis-Hastings algorithm](/2019-02-11-efficient_mcmc_using_rcpp/) can be used when the conditionally conjugate form is not present. \n\nThere are downsides to these established methods, Gibbs sampling puts a restrictive form on the prior distribution. Although Metropolis-Hastings (MH) allows any prior distribution from which the probability density function can be evaluated, the proposal distribution is often a multivariate Normal distribution with mean corresponding to the previous value of the parameters. This proposal results in random walk behaviour and although the MH algorithm is guaranteed to converge it can take a long time to get satisfactory draws from the posterior distribution.\n\nThe gradient of the un-normalised log-posterior distribution can be used to explore the posterior distribution more efficiently. Hamiltonian Monte Carlo (HMC) is an MCMC method which utilises a discretisation of Hamilton's equations in order to model a physical system where the parameters are represented by the position of a particle in $\\theta \\in \\mathbb{R^d}$. In order to implement HMC, the posterior distribution is augmented with a momentum vector, $\\phi$, which is used to propose updates to the position which can be far away from the initial position.\n\n## HMC in R\n\nHamilton's equations are discretised and a \"leapfrog\" update is used. These leapfrog steps can be written as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleapfrog_step <- function(gradient, step_size, position, momentum, d) {\n  momentum1 <- momentum + gradient(position) * 0.5 * step_size\n  position1 <- position + step_size * momentum1\n  momentum2 <- momentum1 + gradient(position1) * 0.5 * step_size\n\n  matrix(c(position1, momentum2), ncol = d*2)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nleapfrogs <- function(gradient, step_size, l, position, momentum, d) {\n  for (i in 1:l) {\n    pos_mom <- leapfrog_step(gradient, step_size, position, momentum, d)\n    position <- pos_mom[seq_len(d)]\n    momentum <- pos_mom[-seq_len(d)]\n  }\n  pos_mom\n}\n```\n:::\n\n\nThe log-acceptance can be written as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_acceptance <- function(propPosition,\n                           propMomentum,\n                           position,\n                           momentum,\n                           log_posterior) {\n  log_posterior(propPosition) + sum(dnorm(propMomentum, log = T)) - \n    log_posterior(position) - sum(dnorm(momentum, log = T))\n}\n```\n:::\n\n\nIn order to propose a new set of parameters a random momentum vector is drawn from the Normal distribution and used in the leapfrog steps. To ensure detailed balance and that the stationary distribution of the Markov chain is equivalent to the target distribution, a metropolis step is used accepting the newly proposed parameters with log-probability equal to the `log_acceptance` defined above. To implement this in R, we compare a log uniform random number to the log acceptance criteria. A single HMC step can be written as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhmc_step <- function(log_posterior, gradient, step_size, l, position) {\n  d <- length(position)\n  momentum <- rnorm(d)\n  pos_mom <- leapfrogs(gradient, step_size, l, position, momentum, d)\n  propPosition <- pos_mom[seq_len(d)]\n  propMomentum <- pos_mom[-seq_len(d)]\n  a <- log_acceptance(propPosition, propMomentum, position, momentum, log_posterior)\n  if (log(runif(1)) < a) {\n    propPosition\n  } else {\n    position\n  }\n}\n```\n:::\n\n\nThe HMC algorithm can be written as\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhmc <- function(log_posterior, gradient, step_size, l, initP, m) {\n  out <- matrix(NA_real_, nrow = m, ncol = length(initP))\n  out[1, ] <- initP\n  for (i in 2:m) {\n    out[i, ] <- hmc_step(log_posterior, gradient, step_size, l, out[i-1,])\n  }\n  out\n}\n```\n:::\n\n\n## An Example Model: Bivariate Normal Model\n\nThe same bivariate Normal model from a [previous post implementing the Metropolis algorithm](/2019/02/11/efficient_mcmc_using_rcpp) is used. See that post for details of deriving the log-likelihood and choice of prior distributions for the parameters.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\nFrom the previous post, the log-posterior distribution is the sum of the log-prior and the log-likelihood. The log-likelihood is given by:\n\n\n$$\\log p(y|\\mu, \\Sigma) = \\sum_{j=1}^2\\left(-\\frac{N}{2}\\log(2\\pi\\sigma_{j}^2) - \\frac{1}{2\\sigma_{j}^2}\\sum_{i=1}^N(y_{ij}-\\mu_j)^2\\right)$$\nWhere $\\Sigma = \\operatorname{diag}(\\sigma_1, \\sigma_2)$, the prior distributions are chosen to be:\n\n$$\\begin{align}\n\np(\\mu_j) &= \\mathcal{N}(0, 3), \\\\\np(\\sigma_j) &= \\textrm{Gamma}(3, 3), \\quad j = 1, 2.\n\\end{align}$$\n\nThe log-pdf of these distributions are:\n\n\n$$\\begin{align}\n\\log p(\\mu_j) &= -\\frac{1}{2}\\log(18\\pi)-\\frac{\\mu_j^2}{18} \\\\\n\\log p(\\sigma_j) &= \\alpha\\log(\\beta)-\\log(\\Gamma(\\alpha)) + (\\alpha-1)\\log(\\sigma_j)-\\beta \\sigma_j\n\\end{align}$$\n\nThe gradient of the log-posterior with respect to each of the paramters can be written as:\n\n$$\\begin{align}\n\n\\frac{\\partial \\ell}{\\partial \\mu_j} &= \\frac{1}{\\sigma_j^2}\\sum_{i=1}^N(y_{ij}-\\mu_j) - \\frac{\\mu_j}{9}, \\\\\n\\frac{\\partial \\ell}{\\partial \\sigma_j} &= -\\frac{N}{\\sigma_j} +  \\frac{1}{\\sigma_j^3}\\sum_{i=1}^N(y_{ij}-\\mu_j)^2 + \\frac{2}{\\sigma_j}-3, \\quad j = 1, 2.\n\\end{align}$$\n\nIn R the gradient can be programmed by hand:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngradient <- function(ys) {\n  function(theta) {\n    mu <- c(theta[1], theta[3])\n    sigma <- c(theta[2], theta[4])\n    n <- nrow(ys)\n    c(1/sigma[1]^2*sum(ys[,1] - mu[1]) - mu[1]/9,\n      -n/sigma[1] + sum((ys[,1] - mu[1])^2) / sigma[1]^3 + 2/sigma[1] - 3,\n      1/sigma[2]^2*sum(ys[,2] - mu[2]) - mu[2]/9,\n      -n/sigma[2] + sum((ys[,2] - mu[2])^2) / sigma[2]^3 + 2/sigma[2] - 3)\n  }\n}\n```\n:::\n\n\nTo ensure the value of the gradient is correct we can compare it to a numerical approximation of the gradient using the [https://cran.r-project.org/web/packages/numDeriv/numDeriv.pdf](numDeriv) package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\napprox_gradient <- function(xs, theta) {\n    grad(log_posterior(xs), theta)\n}\n\ncompare_gradient <- function(theta, tol) {\n  abs(gradient(xs)(theta) - approx_gradient(xs, theta)) < tol\n}\n\ncompare_gradient(theta, 1e-3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE TRUE TRUE TRUE\n```\n:::\n:::\n\n\nIt appears the calculated derivative is correct. Next, HMC works best when the leapfrog proposal can propose unconstrained values of the parameters which lie on the whole real line. A `transform` function is defined for the parameters, $\\theta$ which calculates the exponential of the standard deviation parameters. The log-posterior is calculated using the transformed values, the appropriate transformation and inverse transformation functions can be written as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntransform <- function(theta) {\n  c(theta[1], exp(theta[2]), theta[3], exp(theta[4]))\n}\n\ninv_transform <- function(theta) {\n  c(theta[1], log(theta[2]), theta[3], log(theta[4]))\n}\n```\n:::\n\n\nThe leapfrog step proposal is calculated using the unconstrained parameters, hence the derivative of the log-jacobian of the transformation is required to be added to the value of the gradient of the log-density. Then the derivative of the log-jacobian is calculated to get the value of the gradient corresponding to the unconstrained parameters in the leapfrog step.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_jacobian <- function(theta) {\n  c(0, theta[2], 0, theta[4])\n}\n\nderiv_log_jacobian <- function(theta) {\n  c(0, 1, 0, 1)\n}\n```\n:::\n\n\nThe derivative of the log-jacobian contributes the value 1 to each of the partial derivatives $\\frac{\\partial \\ell}{\\partial \\sigma_j}, j = 1, 2.$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# evaluate the log-posterior on the appropriate scale, using the transform function\nbounded_log_posterior <- function(xs) {\n  function(theta) {\n    log_posterior(xs)(transform(theta)) + sum(log_jacobian(theta))\n  }\n}\n\nbounded_gradient <- function(xs) {\n  function(theta) {\n    gradient(xs)(transform(theta)) + deriv_log_jacobian(theta)\n  }\n}\n```\n:::\n\n\nThe HMC algorithm can be run in parallel using the [furrr](https://davisvaughan.github.io/furrr/) package as described in my post about the [Metropolis algorithm](2019/02/11/efficient_mcmc_using_rcpp). First the `hmc` function is used in another function which returns a dataframe called `hmc_df`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhmc_df <- function(log_posterior, gradient, step_size, l, initP, m, parameter_names) {\n  mat <- hmc(log_posterior, gradient, step_size, l, initP, m)\n  colnames(mat) <- parameter_names\n  as.data.frame(mat) %>% \n    mutate(iteration = row_number())\n}\n```\n:::\n\n\nThen the function is used in `future_map_dfr`:\n\n\n::: {.cell hash='index_cache/html/hmc_b49e86f1cae48c61f571e6a57b221efc'}\n\n```{.r .cell-code}\nfuture::plan(future::multiprocess)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Strategy 'multiprocess' is deprecated in future (>= 1.20.0). Instead,\nexplicitly specify either 'multisession' or 'multicore'. In the current R\nsession, 'multiprocess' equals 'multicore'.\n```\n:::\n\n```{.r .cell-code}\nstart_time <- Sys.time()\nout_hmc <-\n  furrr::future_map_dfr(\n    .x = 1:2,\n    .f = function(x)\n      hmc_df(\n        bounded_log_posterior(xs),\n        bounded_gradient(xs),\n        0.01,\n        4,\n        inv_transform(theta),\n        10000,\n        c(\"mu1\", \"sigma1\", \"mu2\", \"sigma2\")\n      ),\n    .id = \"chain\"\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n```\n:::\n\n```{.r .cell-code}\nend_time <- Sys.time()\nhmc_time <- end_time - start_time\n```\n:::\n\n\nThe traceplots of both chains are plotted below.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n## Evaluating the efficiency\n\nIn order to compare between Markov chain Monte Carlo algorithms the amount of information from each correlated sample can be measured. This is termed the effective sample size, corresponding to the number of effectively independent draws from the posterior distribution. The code below calculates the ESS of each parameter in the chain using `ess_bulk` from the R interface to stan `rstan` library. The ESS per second can be calculated which is a measure of the efficiency of the sampler\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_hmc %>% \n  summarise_at(2:5, rstan::ess_bulk) %>% \n  mutate_all(~ . / as.numeric(hmc_time))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       mu1   sigma1      mu2   sigma2\n1 10169.87 3713.415 1147.888 4983.936\n```\n:::\n:::\n\n\nThis could be compared to a similar method, such as the [Metropolis algorithm](2019-02-11-efficient_mcmc_rcpp). To determine which algorithm is the most efficient for sampling from the posterior distribution of the bivariate Normal model.\n\n\n::: {.cell hash='index_cache/html/time-metropolis_f5274da653bc727d790786d705312a9e'}\n\n```{.r .cell-code}\nproposal <- function(x) {\n  z = rnorm(4, sd = 0.05)\n  c(x[1] + z[1], x[2] * exp(z[2]),\n    x[3] + z[3], x[4] * exp(z[4]))\n}\nnames(theta) <- c(\"mu1\", \"sigma1\", \"mu2\", \"sigma2\")\n\nstart_time <- Sys.time()\nout_met <- metropolis(theta, log_posterior(xs), proposal, 1e4, chains = 2, parallel = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n```\n:::\n\n```{.r .cell-code}\nend_time <- Sys.time()\nmetropolis_time <- end_time - start_time\n```\n:::\n\n\nThe ESS/s can be computed for the Metropolis algorithm\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_met %>% \n  summarise_at(2:5, rstan::ess_bulk) %>% \n  mutate_all(~ . / as.numeric(metropolis_time))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 4\n  accepted   mu1 sigma1   mu2\n     <dbl> <dbl>  <dbl> <dbl>\n1   12850. 1780.  1052.  424.\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}