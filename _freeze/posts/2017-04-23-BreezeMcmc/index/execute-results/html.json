{
  "hash": "67ef1e7fb0163c029ef67a9128f9d5b1",
  "result": {
    "markdown": "---\ntitle: \"MCMC with Scala Breeze\"\nauthor: \"Jonny Law\"\ndate: \"2017-04-23\"\ncategories:\n  - [Scala, Bayesian]\n---\n\n\n\n\n# Bivariate Gaussian Model\n\n[Scala Breeze](https://github.com/scalanlp/breeze) is a numerical computing library, which also provides facilities for statistical computing. For instance, implementations of distributions and Markov chain Monte Carlo (MCMC), which can be used for solving the integrals required in Bayesian modelling. In this post, I am going to simulate data from a bivariate Gaussian model and use the Scala Breeze library to recover the mean and the variance of the bivariate Gaussian distribution.\n\nThe model can be written as\n\n\n$$ \\begin{pmatrix}X_1 \\\\ X_2\\end{pmatrix} \\sim \\textrm{MVN}\n  \\begin{pmatrix}\n    \\begin{pmatrix}\\mu_1 \\\\ \\mu_2 \\end{pmatrix}, \n    \\begin{pmatrix} \\sigma & 0 \\\\ 0 & \\sigma \\end{pmatrix} \n  \\end{pmatrix} $$\n\nThe model has three parameters, the mean of each variable and the variance which is shared. $X_1$ and $X_2$ are independent and hence can be simulated from separate univariate Gaussian distributions:\n\n```scala\nimport breeze.stats.distributions._\nimport breeze.linalg._\n\ncase class Parameters(mu: DenseVector[Double], sigma: Double)\n\ndef model(params: Parameters) = \n  MultivariateGaussian(params.mu, diag(DenseVector.fill(2)(params.sigma)))\n```\n\nA simulation from the bivariate Gaussian model is plotted below, the mean for x is 2.0, the mean for y is 3.0 and the variance for each dimension is 0.5.\n\n```scala\nval p = Parameters(DenseVector(2.0, 3.0), 0.5)\nval data = model(p).sample(100)\n```\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/bivariate-normal-plot-1.png){width=672}\n:::\n:::\n\nIt is simple to write a function to calculate the log-likelihood of this model:\n\n```scala\ndef likelihood(points: Seq[DenseVector[Double]])(p: Parameters) =\n    points.map { point => \n      MultivariateGaussian(p.mu, diag(DenseVector.fill(2)(p.sigma))).logPdf(point)\n    }.reduce((x, y) => x + y)\n```\n\nWe take a sequence of observations, called `points`, since we know each point is simulated independently from the same distribution, then we simple `map` over the sequence of points the likelihood using the supplied value of the `Parameters`. The `reduce` operation then applies a pairwise function to each element of the list, in this case addition to get the value of the log-likelihood.\n\nFor a full Bayesian inference, we must specify a prior distribution on the parameters, let's choose a multivariate Gaussian distribution on the mean and a Gamma distribution for the precision (the inverse of the variance, $\\tau = 1/\\sigma^2$). The Gamma distribution in Breeze is parameterised in terms of shape and scale, the mean of the Gamma distribution with shape $k = 1/2$ and scale $\\theta = 2$ is $k\\theta = 1 = 1/\\sigma^2$:\n\n```scala\ndef prior(p: Parameters) = {\n  MultivariateGaussian(DenseVector(2.0, 3.0), diag(DenseVector.fill(2)(3.0))).logPdf(p.mu) +\n    Gamma(shape = 0.5, scale = 2.0).logPdf(1/(p.sigma * p.sigma))\n}\n```\n\nThe posterior distribution is proportional to the prior times the likelihood:\n\n$$p(\\theta | x) \\propto p(x | \\theta) p(\\theta)$$\n\n\nWe can define the un-normalised log-posterior in Scala\n\n```scala\ndef logMeasure = (p: Parameters) => likelihood(data)(p) + prior(p)\n```\n\nThe computational challenge for Bayesian inference is to determine the normalising constant for the posterior distribution. The full Bayes' theorem is specified as:\n\n\n$$p(\\theta | x) = \\frac{p(x | \\theta) p(\\theta)}{\\int_\\theta p(x|\\theta)p(\\theta)d\\theta}$$\n\nAs we can see from the full equation, the normalising constant is an integral. This integral is typically intractable for complex problems. However, we can construct a Markov chain with a stationary distribution equal to the posterior. \n\nThe Markov chain Monte Carlo method we will be using is a Metropolis-Hastings algorithm with a symmetric random walk proposal. First, we propose a new value of the parameters, $\\theta^*$ from the parameter proposal distribution, then we accept them with probability $\\min(1, A)$, where $A$ is:\n\n$$A = \\frac{p(x|\\theta^*)p(\\theta^*)}{p(x|\\theta)p(\\theta)}$$\n\n\nSo if the likelihood multiplied by the prior is larger at the proposed value of the parameters than the previous value, we always accept, otherwise, we may reject. In this way, we can explore the parameter space. In a well tuned sampler, the algorithm will not accept every proposed value of the parameters, otherwise we are NOT exploring the whole of the parameter posterior, just areas of high posterior density. In this case we can increase the variance of the proposal distribution to get the acceptance rate down to approximately 30-40%. A random walk proposal function can be written in Scala\n\n```scala\nimport breeze.numerics.exp\n\ndef propose(scale: Double)(p: Parameters) = \n  for {\n    innov <- MultivariateGaussian(DenseVector.fill(3)(0.0), diag(DenseVector.fill(3)(scale)))\n    mu = p.mu + innov(0 to 1)\n    sigma = p.sigma * exp(innov(2))\n  } yield Parameters(mu, sigma)\n```\n\nHere, the value of sigma is proposed on the log-scale, since sigma is expected to be positive. Now, we have all we need to build the sampler using breeze:\n\n```scala\nMarkovChain.metropolis(p, propose(0.05))(logMeasure)\n```\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/bivariate-normal-parameters-1.png){width=672}\n:::\n:::\n\n\nThe full code required to run the MCMC in Breeze can be found in this [gist](https://gist.github.com/jonnylaw/b75147d08ea89ec1b78fa94d5a4d2f7d). Note that Breeze is a required dependency.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}