{
  "hash": "c901b0cc324805f8c31641cabef1c4b9",
  "result": {
    "markdown": "---\ntitle: Bayesian Inference using rejection sampling\nauthor: Jonny Law\ndate: '2019-02-25'\nslug: rejection-sampling\ncategories:\n  - [R, Bayesian]\n---\n\n\n\n\n# Coin Flip Model\n\nAs an example, consider a (possibly biased) coin flip experiment. The parameter of interest is the probability of heads $p_h$. A Beta distribution is chosen for the prior of $p_h$, $p(p_h) = \\mathcal{B}(\\alpha, \\beta)$. The Beta distribution has support between 0 and 1, which is appropriate for a probability. The likelihood of a coin flip is Bernoulli, however the coin should be flipped several times in order to learn the parameter $p_h$. The distribution for $n$ independent Bernoulli trials is the Binomial distribution, hence the likelihood can be written as $\\textrm{Bin}(Y;n,p_h)$. The coin is flipped $n = 10$ times and the results are displayed below:\n\n\n$$\\begin{equation*}\n  \\label{eq:4}\n\\{H, H, T, H, H, H, T, T, H, T\\}  \n\\end{equation*}$$\n\nThen $Y = 6$, and it remains to determine the posterior distribution of the parameter $p_h$ representing the probability of obtaining heads. Applying Bayes theorem:\n\n$$\\begin{align*}\n\n  p(p_h|Y) &= \\frac{p(p_h)\\textrm{Bin}(Y;n,p_h)}{\\int_0^1 p(Y|p_h) dp_h} \\\\\n           &\\propto p_h^{\\alpha - 1}(1-p_h)^{\\beta-1}{n \\choose Y}p_h^Y(1-p_h)^{n-Y} \\\\\n           &= p_h^{Y + \\alpha - 1}(1 - p_h)^{n-Y + \\beta - 1}\n\\end{align*}$$\n\nThe posterior distribution is a Beta distribution, $\\mathcal{B}(Y + \\alpha, n - Y + \\beta)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nY <- 6\nn <- 10\n\nalpha <- 3\nbeta <- 3\n\nprior <- function(x) dbeta(x, alpha, beta)\nposterior <- function(x) dbeta(x, alpha + Y, n - Y + beta)\n\nggplot(data = data.frame(x = 0), mapping = aes(x = x)) +\n  stat_function(fun = prior, aes(colour = \"Prior\")) + xlim(0, 1) +\n  stat_function(fun = posterior, aes(colour = \"Posterior\")) +\n  xlab(\"p_h\") +\n  ylab(\"Density\") +\n  theme(\n    text = element_text(size = 20),\n    legend.title = element_blank(),\n    legend.position = c(0.2, 0.9),\n    legend.text = element_text(size = rel(1.3))\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n# Rejection Sampler\n\nThe rejection sampler is an algorithm which produces exact samples from the\ntarget distribution. Consider a problem where it is straightforward to evaluate\nthe posterior density $p(\\cdot)$ up to a constant. The rejection sampler\nalgorithm proceeds as follows; start by sampling a value from a proposal\ndistribution $\\psi^\\star \\sim q(\\cdot)$, then accept the proposed value with\nprobability $p(\\psi^\\star)/Mq(\\psi)$, where $M$ is an upper bound on $p/q$. The algorithm below shows a single step of the rejection sampler algorithm which returns a single sample from the target distribution $p(\\cdot)$.\n\n1. Propose $\\psi^\\star \\sim q(\\cdot)$\n2. Continuously sample $u \\sim U[0, 1]$ and check the condition in step 3.\n3. If $u < \\frac{p(\\psi^\\star)}{Mq(\\psi)}$, set $\\psi^\\star$ as a sample from $p$\n4. Repeat 1-3 until enough samples are attained\n\nThe figure below shows the empirical posterior\ndistribution found for the coin flip experiment overlaid with the analytic\nposterior distribution. This algorithm performs well for low-dimensional\nproblems, but finding the upper bound $M$ can be challenging. The rejection algorithm does not work well in higher dimensions as many proposed moves are rejected. Adding extra dimensions to the problem results in the exponential increase in volume, this is known as the curse of dimensionality. More sophisticated algorithms are required for high dimensional target distributions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform one rejection step\nrejection_sample <- function(prop, propPdf, log_density) {\n  u <- runif(1)\n  y <- prop(1)\n  \n  if (log(u) < log_density(y) - propPdf(y)) {\n    y\n  } else {\n    rejection_sample(prop, propPdf, log_density)\n  }\n}\n\nlog_density <- function(alpha, beta, Y, n) {\n  function(theta) {\n    dbeta(theta, alpha, beta, log = T) + dbinom(Y, n, theta, log = T)\n  }\n}\n\nsamples <- 1000\nlden <- log_density(alpha, beta, Y, n)\nrejection_samples <-\n  replicate(samples, rejection_sample(runif, function(x)\n    dunif(x, log = T), lden))\n\nggplot(tibble(rejection_samples)) +\n  geom_histogram(aes(x = rejection_samples, y = ..density..), alpha = 0.4) +\n  stat_function(fun =  posterior, aes(colour = \"Analytic posterior\")) +\n  theme(legend.position = \"none\") +\n  xlab(\"p_h\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![Empirical Posterior distribution for the coin flip problem using 1,000 samples from the rejection sampler with a Uniform(0, 1) proposal distribution and M = 1. The analytic posterior distribution is plotted as a solid red line.](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}