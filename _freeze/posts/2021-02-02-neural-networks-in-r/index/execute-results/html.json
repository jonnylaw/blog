{
  "hash": "707bcbe68ece85d1bd34a4262dc6576c",
  "result": {
    "markdown": "---\ntitle: \"Neural Networks in R\"\ndescription: |\n  This post explores how to create a simple neural network to learn a linear function and a non-linear function using both standard R and the Torch library for R.\nauthor: \"Jonny Law\"\ndate: \"2021-02-02\"\ncategories:\n  - [R, Deep Learning]\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.5     ✔ purrr   0.3.4\n✔ tibble  3.1.6     ✔ dplyr   1.0.8\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidyr' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'readr' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'dplyr' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(torch)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'torch' was built under R version 4.1.2\n```\n:::\n\n```{.r .cell-code}\ntheme_set(theme_minimal())\n```\n:::\n\n\nIn this post we will see how to train a neural network model using `R` and the [Torch](https://torch.mlverse.org/) `R` library which is a port of the Python torch library without dependencies on Python. Torch provides tensors (n-dimensional arrays), automatic differentiation of tensors, optimisation routines and additional helpers for common deep learning tasks such as computer vision and audio processing. \n\nA neural network is built of layers. A layer consists of a set of weights, which are the parameters of the layer and an activation function (this can have additional parameters too). A single layer looks like the linear predictor in a generalised linear model,\n\n\n$$\n\\eta = g(x^Tw),\n$$\n\n\nwhere $\\eta$ is the linear predictor, $g$ is the linking function and $w$ represent the coefficients of the covariates $x$. In machine learning, $x$ is referred to simply as the input, $w$ are the weights and $g$ is the activation function. There is often an additional parameter, termed the intercept in generalised linear models and the bias in machine learning. This can be rolled in to the weight vector by appending a one to the input / covariates. We can encapsulate this logic in a function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer <- function(input, weight, activation) \n  activation(input %*% weight)\n```\n:::\n\n\nWe also require a loss function, to understand how well our model is fitting to the data. For a regression problem we can use squared loss. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nloss <- function(pred, y) {\n  sum((y - pred)^2)\n}\n```\n:::\n\n\nWe will define a simple linear regression problem, our observations are noisy observations of a straight line.\n\n\n$$y \\sim \\mathcal{N}(x^Tw, 2.0)$$\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(-10, 10, length.out=100)\ny <- rnorm(100, mean = 2 * x + 3, sd = 2.0)\nqplot(x, y, geom = \"point\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\nWe wish to learn the relationship between the inputs $x$ and the outputs $y$ using the model. To understand how well the model fits the observed data, we make a prediction by passing an observed input to the model (which is defined as a single layer), then we calculate how far the prediction is from the observed output using the squared loss function. The activation function is linear, or the identity function (`function(x) x`).\n\n::: {.cell}\n\n```{.r .cell-code}\nw <- matrix(c(1, 1))\nx_intercept <- cbind(1, x)\nprediction <- layer(x_intercept, w, function(x) x)\nloss(prediction, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4470.924\n```\n:::\n:::\n\nTo find the best fit for this model to the observed data, we need to manipulate the weights to reduce the loss function. We can think of the weights as parameterising a family of related models, some of which may fit the data well.\n\n## Optimisation\n\nTo find the maximum of a function in calculus, we first calculate the derivative and determine the point at which the slope is equal to zero. This can find both maximums and minimums (or saddle points), so we can additionally calculate the second derivative and if it's negative then we have a maximum. This is fine for linear optimisation, however when it comes to non-linear optimisation we have to be more creative. We can use gradient descent to take steps in the opposite direction of the gradient to find the minimum of a non-linear function\n\n::: {.cell}\n\n```{.r .cell-code}\ngradient_step <- function(learning_rate, params, grad_params) \n  params - learning_rate * grad_params\n```\n:::\n\nWe must calculate the derivative of the network with respect to the weight parameters. For a single layer network with univariate inputs, a linear activation function and a squared loss function the derivative is \n\n$$\\frac{d \\text{ network}}{dw} = \\frac{d}{dw} (y - x^Tw)^2 = -2x^T(y-x^Tw).$$\n\n\nWe can encapsulate this derivative as a function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnetwork_gradient <- function(x, y, w) {\n  -2 * t(x) %*% (y - x %*% w)\n}\n```\n:::\n\n\nWe can check the analytically calculated gradient using Torch. First we use our calculation of the gradient.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_test <- matrix(rnorm(2))\nw <- matrix(rnorm(2))\ny_test <- matrix(rnorm(1))\nnetwork_gradient(t(x_test), y_test, w)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]\n[1,] 0.15098599\n[2,] 0.05417119\n```\n:::\n:::\n\n\nThen we must write the forward function of the model, `pred` and calculate the `loss` using the functions available on Torch tensors. We can then call `backward()` which performs reverse mode automatic differentiation then we can access the `grad` attribute of any tensor which has `requires_grad = TRUE`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnetwork_gradient_torch <- function(x, y, w) {\n  pred <- w$t()$mm(x)\n  loss <- (y - pred)$pow(2)$sum()\n  loss$backward()\n  w$grad\n}\n\nnetwork_gradient_torch(x = torch_tensor(as.matrix(x_test)),\n                       y = torch_tensor(as.matrix(y_test)),\n                       w = torch_tensor(as.matrix(w), requires_grad = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 0.1510\n 0.0542\n[ CPUFloatType{2,1} ]\n```\n:::\n:::\n\n\nWe can fit this simple model by writing a training loop which updates the parameters using gradient descent. We keep track of the loss function at each iteration of gradient descent and plot it. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nobserved <- tibble(x, y) %>%\n  sample_n(50)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntraining_loop <- function(x, y, init_w, epochs, learning_rate) {\n  # Record all changes to parameters and training loss\n  ws <- matrix(NA_real_, nrow = epochs + 1, ncol = length(init_w))\n  ws[1,] = init_w\n  losses <- numeric(length(y))\n  \n  for (i in seq_len(epochs)) {\n    # Pad the input with 1 for intercept/bias\n    input <- cbind(rep(1, times = length(x)), x)\n    \n    # Made a prediction using the model\n    pred <- layer(input, ws[i,], function(z) z)\n    \n    # We can calculate and log/print the loss \n    losses[i] <- loss(y, pred)\n    \n    # calculate the gradient at this point\n    gradient <- network_gradient(x = input, y = y, w = ws[i, ])\n    \n    # Update using gradient descent\n    ws[i + 1, ] <- gradient_step(learning_rate, \n                                 ws[i, ], \n                                 gradient)\n  }\n  \n  list(weights = ws, losses = losses)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nout <- training_loop(x = scale(observed$x), y = scale(observed$y), c(1, 1), 500, 1e-4)\nqplot(x = seq_along(out$losses), y = out$losses, geom = \"line\") +\n  labs(x = \"Epoch\", y = \"Loss\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/loss-curve-1.png){width=672}\n:::\n:::\n\n\nSince this model is so small, consisting of only two weights. We can plot the actual function learned by the model using `geom_abline`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(observed, aes(scale(x), scale(y))) +\n  geom_point() +\n  geom_abline(intercept = out$weights[501, 1], slope = out$weights[501,2]) + \n  labs(x = \"x\", y = \"y\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/learned-function-1.png){width=672}\n:::\n:::\n\n\nWe can try to use this model on a simple non-linear regression problem, of course we probably won't do very well here!  We define the regression problem as\n\n\n$$y \\sim \\mathcal{N}(4\\sin(x), 1^2).$$\n\nWe plot the true function and the observed values in red below.\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 100\nx <- seq(-5, 5, length.out=n)\ny <- 4 * sin(x)\ny_obs <- 4 * sin(x) + rnorm(n, sd = 1)\nnon_linear <- tibble(x, y_obs) %>% sample_n(20)\nqplot(x, y, geom = \"line\") +\n  geom_point(data = non_linear, aes(x = x, y = y_obs, colour = \"Observed\")) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/true-function-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nout <- training_loop(non_linear$x, non_linear$y_obs, c(1, 1), 200, 1e-4)\nqplot(x = seq_along(out$losses), y = out$losses, geom = \"line\") +\n  labs(x = \"Epoch\", y = \"Loss\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/losses-non-linear-1.png){width=672}\n:::\n:::\n\nWe can then plot the learned function against the observed values and the true function. We can see that a straight line is not a good fit for this data, we need more flexibility in the network.\n\n::: {.cell}\n\n```{.r .cell-code}\nqplot(x, y, geom = \"line\", colour = \"truth\") +\n  geom_point(data = non_linear, aes(x, y_obs, colour = \"observed\")) +\n  geom_abline(intercept = out$weights[201,1], slope = out$weights[201,1]) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n## Using Torch\n\nIf we want to approximate a non-linear function, we best use non-linear activation functions. We can calculate the derivative of each layer using automatic differentiation. We will use the R [Torch](https://torch.mlverse.org) library. We now initialise a torch tensor with the same values as x and pass it through the layers. We must re-write the layer and loss functions assuming the input is a `torch_tensor`. First we will re-write the linear example using Torch\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- nn_linear(1, 1)\n\nloss <- function(y, pred) {\n  (y - pred)$pow(2)$sum()\n}\n\ntrain_torch <- function(x, y, model, loss, epochs) {\n  alpha <- 1e-4\n  x_in <- torch_tensor(as.matrix(x))\n  for (i in seq_len(epochs)) {\n    pred <- model(x_in)\n    losses <- loss(torch_tensor(y), pred)\n    model$zero_grad()\n    losses$backward()\n    if (i %% 10 == 0)\n      cat(\"Epoch: \", i, \"   Loss: \", losses$item(), \"\\n\")\n    with_no_grad({\n      model$parameters %>% \n        purrr::walk(function(param) param$sub_(alpha * param$grad))\n    })\n  }\n  model\n}\n\ntrained_model <- train_torch(scale(observed$x), scale(observed$y), model, loss, 200)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch:  10    Loss:  58.6479 \nEpoch:  20    Loss:  48.48629 \nEpoch:  30    Loss:  40.14468 \nEpoch:  40    Loss:  33.2971 \nEpoch:  50    Loss:  27.67593 \nEpoch:  60    Loss:  23.06152 \nEpoch:  70    Loss:  19.27356 \nEpoch:  80    Loss:  16.16403 \nEpoch:  90    Loss:  13.6114 \nEpoch:  100    Loss:  11.51595 \nEpoch:  110    Loss:  9.795779 \nEpoch:  120    Loss:  8.383685 \nEpoch:  130    Loss:  7.224489 \nEpoch:  140    Loss:  6.272895 \nEpoch:  150    Loss:  5.491727 \nEpoch:  160    Loss:  4.850457 \nEpoch:  170    Loss:  4.324034 \nEpoch:  180    Loss:  3.891886 \nEpoch:  190    Loss:  3.53713 \nEpoch:  200    Loss:  3.245904 \n```\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(observed, aes(scale(x), scale(y))) +\n  geom_point() +\n  geom_abline(intercept = trained_model$parameters$bias$item(), slope = trained_model$parameters$weight$item())\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-torch-linear-1.png){width=672}\n:::\n:::\n\nTry the non-linear example with multiple layers and a non-linear activation function from the first layer (where the input goes). We'll also try a different optimizer, Adam.\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_torch <- function(x, y, model, loss, epochs, learning_rate) {\n  x_in <- x\n  optimiser <- optim_adam(model$parameters, lr = learning_rate)\n  for (i in seq_len(epochs)) {\n    pred <- model(x_in)\n    losses <- loss(y, pred)\n    model$zero_grad()\n    losses$backward()\n    if (i %% 10 == 0)\n      cat(\"Epoch: \", i, \"   Loss: \", losses$item(), \"\\n\")\n    optimiser$step()\n  }\n  model\n}\n\nmodel <- nn_sequential(\n  nn_linear(1, 64),\n  nn_relu(),\n  nn_linear(64, 1)\n)\n\ntrained_model <-\n  train_torch(torch_tensor(as.matrix(non_linear$x)),\n              torch_tensor(as.matrix(non_linear$y_obs)),\n              model,\n              nnf_mse_loss,\n              100, \n              0.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch:  10    Loss:  6.853224 \nEpoch:  20    Loss:  2.670638 \nEpoch:  30    Loss:  1.007208 \nEpoch:  40    Loss:  0.7993301 \nEpoch:  50    Loss:  0.683284 \nEpoch:  60    Loss:  0.6011851 \nEpoch:  70    Loss:  0.5383755 \nEpoch:  80    Loss:  0.5088955 \nEpoch:  90    Loss:  1.185604 \nEpoch:  100    Loss:  0.5675938 \n```\n:::\n:::\n\nWe can plot the predictions alongside the observed values and the true function.\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions <- tibble(x, prediction = as_array(trained_model(torch_tensor(x)$unsqueeze(-1))))\n\nqplot(x, y, geom = \"line\", colour = \"truth\") +\n  geom_line(data = predictions, aes(x, prediction, colour = \"fitted\")) +\n  geom_point(data = non_linear, aes(x, y_obs, colour = \"observed\")) +\n  labs(colour = \"\") +\n  theme(legend.position = c(0.9, 0.9))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/non-linear-model-1.png){width=672}\n:::\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}