{
  "hash": "169a8054f2a696f1895c4c5a5d86c4f7",
  "result": {
    "markdown": "---\ntitle: \"Uncertainty in Neural Networks\"\ndescription: |\n  Using MC Dropout to get probability intervals for neural network predictions.\ndate: \"2021-08-16\"\nbibliography: references.bib  \ncategories:\n  - Python\n  - Deep Learning\n  - Bayesian\n---\n\n\n\n\nRecently, I have been reading [Probabilistic Deep Learning](https://www.manning.com/books/probabilistic-deep-learning) which introduces Bayesian methods for fitting Neural Networks using Tensorflow and Keras. One case study consists of a non-linear regression problem for a sinusoidal curve. Additionally, the curve is considered to have heteroskedastic variance, meaning the variance changes along the domain of the function. In this post I will consider approximating a non-linear function with constant (homoskedastic) variance and quantifying the uncertainty. I will be using PyTorch - because why not.\n\nUncertainty is an important concept in machine learning, if we have confidence in the predictions of model we are able to make more informed decisions. However, neural networks trained using traditional back-propagation typically return a single point estimate by training to minimise a loss function. There have been many attempts to incorporate uncertainty into neural networks, the most principled way is to fit a Bayesian Neural Network (BNNs) by placing prior distributions on the weights and calculating the posterior distribution using Bayes' theorem:\n\n\n$$p(\\theta| y) = \\frac{p(\\theta)p(y|\\theta)}{\\int_\\theta p(y|\\theta)p(\\theta)}$$\n\nWhere $\\theta$ represents the parameters in the neural network, ie. the weights and biases. $p(\\theta)$ is the prior distribution, and $p(y|\\theta)$ is the likelihood. These can all be specified. However, calculating the evidence, the denominator in the equation, $\\int_\\theta p(y|\\theta)p(\\theta)$ is intractable analytically in most real-world problems, including BNNs. The gold standard for computing these integrals for real-world problems is [MCMC](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo). In the case of modern deep learning models, the parameter space is too high dimensional (ie. there are too many parameters!) for these methods to be computationally feasible (although [@izmailov2021bayesian] did use HMC for fitting NNs to the CIFAR-10 image dataset and IMDB review dataset using 512 TPUv3\ndevices).  Hence, we look to approximate methods, such as variational inference, which can place additional assumptions on the form on the prior and posterior distributions. There are also non-Bayesian methods (such as deep ensembles) which can be evaluated to have desirable properties such as calibration. From the Wikipedia for [statistical calibration](https://en.wikipedia.org/wiki/Calibration_(statistics)) \"As Philip Dawid puts it, \"a forecaster is well calibrated if, for example, of those events to which he assigns a probability 30 percent, the long-run proportion that actually occurs turns out to be 30 percent\".\"\n\nIn this post we will investigate a single easy to implement method for uncertainty estimation, MC dropout [@gal2016dropout], applied to a regression problem.\n\nLet's start by creating simulating data from a simple model:\n\n$$\n\n\\begin{aligned}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma^2), \\\\\n\\mu_i &= a \\sin(x_i + b).\n\\end{aligned}\n\n$$\n\nWhere $a=2$, $b=-3$ and $\\sigma=1$.\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_data(xs):\n    a = 5\n    b = 3\n    mean = lambda x: a * np.sin(x + b)\n    data = [(x, np.random.normal(loc=mean(x), scale=1, size=1))\n            for x in xs]\n    x, y = zip(*data)\n    return np.array(x), np.array(y)\n\n\nxs = np.random.uniform(0, 10, size=100)\nx, y = generate_data(xs)\nplt.scatter(x, y)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\nCreate a neural net which can learn the non-linear function. We'll use the class `nn.ModuleList` to allow us to experiment with different numbers of hidden layers. According to the MC Dropout paper, we must apply dropout to each layer in order for the procedure to be equivalent to variational inference - under a set of assumptions.\n\n::: {.cell}\n\n```{.python .cell-code}\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\nclass NonLinearRegression(nn.Module):\n    def __init__(self, dropout, hidden_size, hidden_layers):\n        super(NonLinearRegression, self).__init__()\n        self.input = nn.Linear(1, hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        self.linears = nn.ModuleList(\n          [nn.Linear(hidden_size, hidden_size) for i in range(hidden_layers)])\n        self.dropouts = nn.ModuleList(\n          [nn.Dropout(dropout) for i in range(hidden_layers)])\n        self.output = nn.Linear(hidden_size, 1)\n\n    def forward(self, x):\n        x = self.input(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        for l, d in zip(self.linears, self.dropouts):\n          x = l(x)\n          x = F.relu(x)\n          x = d(x)\n        x = self.output(x)\n        return x\n```\n:::\n\nSplit the data randomly into training and testing.\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(np.array(list(x)), np.array(list(y)))\n```\n:::\n\nTest the untrained model input and output shapes by making a prediction using the training data.\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = NonLinearRegression(0.5, 32, 1)\nx_input = torch.tensor(x_train, dtype=torch.float).unsqueeze(-1)\nmodel(x_input).shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([75, 1])\n```\n:::\n:::\n\nUse Skorch to avoid writing the training boilerplate. Use the `.fit` method provided by Skorch.\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom skorch.regressor import NeuralNetRegressor\n\nepochs = 500\n\nnet = NeuralNetRegressor(\n    module=NonLinearRegression,\n    module__dropout=0.1,\n    module__hidden_size=64,\n    module__hidden_layers=3,\n    optimizer=torch.optim.AdamW,\n    iterator_train__shuffle=True,\n    max_epochs=500,\n    verbose=0\n)\n\nx_train_input = torch.tensor(x_train, dtype=torch.float).unsqueeze(-1)\nnet.fit(x_train_input, torch.tensor(y_train, dtype=torch.float))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n  module_=NonLinearRegression(\n    (input): Linear(in_features=1, out_features=64, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (linears): ModuleList(\n      (0): Linear(in_features=64, out_features=64, bias=True)\n      (1): Linear(in_features=64, out_features=64, bias=True)\n      (2): Linear(in_features=64, out_features=64, bias=True)\n    )\n    (dropouts): ModuleList(\n      (0): Dropout(p=0.1, inplace=False)\n      (1): Dropout(p=0.1, inplace=False)\n      (2): Dropout(p=0.1, inplace=False)\n    )\n    (output): Linear(in_features=64, out_features=1, bias=True)\n  ),\n)\n```\n:::\n:::\n\nWe can visualise the learning curve for this model below.\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/plot-learning-curve-3.png){width=672}\n:::\n:::\n\nWe can see in the figure below that the predictions are in the right region, but using only a point prediction we miss out on capturing the uncertainty.\n\n::: {.cell}\n\n```{.python .cell-code}\nx_test_input = torch.tensor(x_test, dtype=torch.float).unsqueeze(-1)\npreds = net.predict(x_test_input)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/plot-predictions-5.png){width=672}\n:::\n:::\n\n## Measurement error with Distribution\n\nWe can represent the [aleatoric uncertainty](https://en.wikipedia.org/wiki/Uncertainty_quantification), ie. the uncertainty inherent in the data using a Normal distribution. We know the observation distribution of the data generating process is Normal with a standard deviation (called scale in PyTorch) of 1.0. We can learn this additional parameter using PyTorch, to do this we can introduce a new loss function and alter the forward function of the Neural Network module to include the standard deviation parameter. First we'll consider how to write the new loss function, we need to calculate the log-likelihood of the observations then we wish to maximise this. Neural networks in PyTorch have optimisers with minimise the loss function, hence we will minimise the negative log-likelihood. Let's first consider the probability density function of a Normal distribution with mean $\\mu \\in \\mathbb{R}$ and standard deviation $\\sigma \\in \\mathbb{R}^+$:\n\n$$p(y | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2}(y-\\mu)^2\\right)$$\n\n\nWe have multiple observations which are independent and normally distributed, hence for each batch we will need to calculate the product of the likelihood:\n\n\n$$p(\\mathbb{y}|\\mu, \\sigma) = \\prod_{i=1}^B \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2}(y_i-\\mu)^2\\right),$$\n\nWhere $B$ is the batch size, this involves multiplying small numbers together which can result in arithmetic underflow, hence we work on the log-scale which changes the calculation to addition:\n\n$$\\log p(\\mathbb{y}|\\mu, \\sigma) =  - \\frac{B}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^B (y_i-\\mu)^2  $$\n\n\nIn the case of the Neural net, $\\mu$ is the output and $\\sigma$ is an additional parameter. We can code the log-likelihood by converting this function into PyTorch or using the build in distributions.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom torch.distributions import Normal\nimport math\n\nnormal_model = NonLinearRegression(0.1, 64, 3)\ninputs = model(x_input)\nlabels = torch.tensor(y_train, dtype=torch.float)\n\nmu = inputs\nlog_sigma = torch.tensor(0.0, dtype=torch.float)\n\n# Calculate manually\nmanual_ll = - 0.5 * math.log(2.0 * math.pi) - \\\n  log_sigma - 0.5 * ((labels - mu) / log_sigma.exp()) ** 2\n\n# Use the built in log_prob function\nlog_likelihood = Normal(mu, log_sigma.exp()).log_prob(labels)\n\nlog_likelihood.sum(), manual_ll.sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(tensor(-617.9488, grad_fn=<SumBackward0>), tensor(-617.9488, grad_fn=<SumBackward0>))\n```\n:::\n:::\n\n\nWe can modify the `nn.Module` to return an additional parameter representing the log of the standard deviation of the Normal distribution. Additionally, the forward model will return the predicted mean and the global standard deviation - this is necessary when using Skorch, since the standard form of the loss function is `def loss(y_pred, y_true)`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\nclass NonLinearRegressionNormal(nn.Module):\n    def __init__(self, dropout, hidden_size, hidden_layers):\n        super(NonLinearRegressionNormal, self).__init__()\n        self.input = nn.Linear(1, hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        self.linears = nn.ModuleList(\n          [nn.Linear(hidden_size, hidden_size) for i in range(hidden_layers)])\n        self.dropouts = nn.ModuleList(\n          [nn.Dropout(dropout) for i in range(hidden_layers)])\n        self.output = nn.Linear(hidden_size, 1)\n        self.log_sigma = nn.Parameter(torch.tensor(1.0))\n\n    def forward(self, x):\n        x = self.input(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        for l, d in zip(self.linears, self.dropouts):\n          x = l(x)\n          x = F.relu(x)\n          x = d(x)\n        x = self.output(x)\n        return x, self.log_sigma\n```\n:::\n\n\nThe standard deviation, `sigma`, is constant, so in a custom training loop (not using skorch) we could simply pass the model definition to the loss function directly and extract the `sigma` parameter as follows. This would mean the `forward` function of the `nn.Module` can remain the same as the first model. The loss function could look like the following:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef normal_pdf(model, X, y_true):\n  preds = model(X)\n  return Normal(preds, model.log_sigma.exp()).log_prop(y_true).sum\n```\n:::\n\n\nThen create a loss function as an `nn.Module`:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom skorch.regressor import NeuralNetRegressor\n\nclass NormalLoss(nn.Module):\n  def  __init__(self):\n    super(NormalLoss, self).__init__()\n\n  def forward(self, inputs, labels):\n    mu, log_sigma = inputs\n    log_likelihood = Normal(mu, log_sigma.exp()).log_prob(labels)\n\n    return - log_likelihood.sum()\n```\n:::\n\n\nTrain the model as usual:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nepochs = 500\n\nnet_normal = NeuralNetRegressor(\n    module=NonLinearRegressionNormal,\n    module__dropout=0.1,\n    module__hidden_size=64,\n    module__hidden_layers=3,\n    optimizer=torch.optim.AdamW,\n    iterator_train__shuffle=True,\n    criterion=NormalLoss,\n    max_epochs=epochs,\n    verbose=0\n)\n\nx_train_input = torch.tensor(x_train, dtype=torch.float).unsqueeze(-1)\nnet_normal.fit(x_train_input, torch.tensor(y_train, dtype=torch.float))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n  module_=NonLinearRegressionNormal(\n    (input): Linear(in_features=1, out_features=64, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (linears): ModuleList(\n      (0): Linear(in_features=64, out_features=64, bias=True)\n      (1): Linear(in_features=64, out_features=64, bias=True)\n      (2): Linear(in_features=64, out_features=64, bias=True)\n    )\n    (dropouts): ModuleList(\n      (0): Dropout(p=0.1, inplace=False)\n      (1): Dropout(p=0.1, inplace=False)\n      (2): Dropout(p=0.1, inplace=False)\n    )\n    (output): Linear(in_features=64, out_features=1, bias=True)\n  ),\n)\n```\n:::\n:::\n\n\nThen calculate some predictions, the actual function is plotted using blue, with the predictions and prediction interval plotted in red. This is an MLE solution with a 95% confidence interval.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/plot-confidence-interval-7.png){width=672}\n:::\n:::\n\n\n## Uncertainty with MC Dropout\n\nWe have looked at how to incorporate aleatoric uncertainty, to understand the uncertainty in the parameters (epistemic uncertainty) we can use MC Dropout. Dropout is a method of avoiding overfitting at training time by removing \"connections\" in a neural network. However, if we leave dropout on when making predictions, then we create an ensemble of models which output slightly different predictions. It turns out that this is equivalent Bayesian variational inference with some assumptions. We can then calculate the mean and and uncertainty intervals we wish.\n\nWe can easily implement MC dropout and visualise the uncertainty provided with this method. First, we extract the module from the Skorch `NeuralNet` class and put it in train mode, this means we make predictions with dropout ON.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnet_normal.module_.train()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNonLinearRegressionNormal(\n  (input): Linear(in_features=1, out_features=64, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (linears): ModuleList(\n    (0): Linear(in_features=64, out_features=64, bias=True)\n    (1): Linear(in_features=64, out_features=64, bias=True)\n    (2): Linear(in_features=64, out_features=64, bias=True)\n  )\n  (dropouts): ModuleList(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Dropout(p=0.1, inplace=False)\n    (2): Dropout(p=0.1, inplace=False)\n  )\n  (output): Linear(in_features=64, out_features=1, bias=True)\n)\n```\n:::\n:::\n\n\nLet's write a convenience function for making multiple predictions and combining them into a single numpy array.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef get_predictions(model, x, n):\n  # Make multiple predictions\n  mus, sigmas = zip(*[model(x) for _ in range(n)])\n\n  # Sample from the observation distribution using each mean and the global sigma\n  return [Normal(mu, log_sigma.exp()).sample().detach().numpy() for mu in list(mus)]\n```\n:::\n\n\nWe then have a collection of predictions which we can use to calculate summaries using [monte carlo](https://en.wikipedia.org/wiki/Monte_Carlo_method). We can calculate the expectation by calculating the mean of all the predictions, and probability intervals by ordering the predictions and selecting the appropriate values, using `np.quantile`. \n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef get_probability_interval(preds, interval):\n  lower_int = (1. - interval) / 2\n  upper_int = interval + lower_int\n\n  # Calculate percentiles and mean\n  lower = np.quantile(preds, lower_int, axis=0)\n  mean = np.mean(preds, axis=0)\n  upper = np.quantile(preds, upper_int, axis=0) \n    \n  return lower, mean, upper\n```\n:::\n\n\nFirst, we will predict a single point, calculate the mean and the 89% probability interval.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntest_point = 5.0\npreds = get_predictions(net_normal.module_, \n    torch.tensor(test_point, dtype=torch.float).unsqueeze(-1), \n    1000)\n\nlower, mean, upper = get_probability_interval(preds, 0.89)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/plot-single-prediction-9.png){width=672}\n:::\n:::\n\n\nNext, we can use the same method to predict several points and give the impression of a function. If we extend the $\\sin$ function we can see that the uncertainty on the inputs which are out of the domain of the training examples is quite large. We can evaluate the calibration of these intervals in domain and out of domain by creating a new training split which omits data in a certain interval.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx_in = np.linspace(-5, 15, 50)\n\npreds = get_predictions(\n    net_normal.module_, \n    torch.tensor(x_in, dtype=torch.float).unsqueeze(-1),\n    1000)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/plot-multiple-predictions-11.png){width=672}\n:::\n:::\n\n\n## Calculate coverage of probability interval\n\nWe can calculate the coverage of the probability interval using an experiment. We first train a neural network generating data from the same noisy function. We have $\\mu = 5\\sin(x + 3)$ and the observations corrupted by Gaussian noise, $y_i \\sim \\mathcal{N}(\\mu, 1^2)$. Then we sample 100 random uniform test points between 0 and 10 and make probabilistic predictions by calculating 1,000 predictions using MC Dropout and calculating 95% probability intervals. This should account for epistemic uncertainty in the parameters and aleatoric uncertainty inherent in the observed data. We then calculate the proportion of predictions which fall into the interval, this should be close to 95%.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef coverage_experiment():\n  # Generate training data\n  x_train = np.linspace(0, 10, 50)\n  _, y_train = generate_data(x_train)\n\n  # Fit model to training data\n  x_train_input = torch.tensor(x_train, dtype=torch.float).unsqueeze(-1)\n  net_normal.fit(x_train_input, torch.tensor(y_train, dtype=torch.float))\n\n  # Generate testing data from the same generative model\n  n_test = 100\n  x_test = np.random.uniform(0, 10, n_test)\n  _, y_test = generate_data(x_test)\n\n  net_normal.module_.train()\n\n  # Calculate predictions on test data\n  preds = get_predictions(\n      net_normal.module_,\n      torch.tensor(x_test, dtype=torch.float).unsqueeze(-1),\n      1000)\n\n  lower, mean, upper = get_probability_interval(preds, 0.95)\n\n  # Calculate proportion of predictions which fall into interval\n  in_interval = sum([l <= y <= u for y, l, u in zip(y_test, lower, upper)])\n  return in_interval / n_test\n```\n:::\n\n\nLet's repeat this experiment 100 times and plot a histogram of the resulting coverage.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/plot-coverage-13.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}