{
  "hash": "2b7acc6ddc4a291f3bfce41bfafc380c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Functional Programming and Hidden Markov Models\ndate: '2020-05-01'\nslug: multi-state-survival-models-part-2\ncategories:\n  - Bayesian\n  - R\n---\n\n\n\nThe hidden Markov model is a state-space model with a discrete latent state, $x_{1:T}$ and noisy observations $y_{1:T}$. The model can be described mathematically as\n\n$$p(y_{1:T}, x_{1:T}) = p(x_1)p(y_1|x_1)\\prod_{t=2}^Tp(y_t|x_t)p(x_t|x_{t-1})$$\n\nWhere $y_{1:T} = y_1, \\dots, y_T$ represents the sequence of observed values and $x_{1:T} = x_1, \\dots, x_T$ is the sequence of latent, unobserved values. The state space is assumed to be finite and countable, $X \\in \\{1,\\dots,K\\}$ and the time gaps between each observation are constant. The observation distribution can be either continuous or discrete.\n\nThe model can be visualised using a state-transition diagram where observed nodes are rectangular and latent nodes are circular. The arrows represent any transitions which can be made and also convey conditional independence assumptions in the Model. The state forms a first order Markov process, which means $p(x_t|x_{t-1},\\dots,x_1) = p(x_t|x_{t-1})$ and each observation is conditionally independent of all others given the corresponding value of the latent state at that time, $y_t \\perp\\!\\!\\!\\perp y_{1:t-1},y_{t+1:T}$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/hmm-1.png){width=90%}\n:::\n:::\n\n\n\n## Example: The occasionally dishonest casino\n\nThe casino can choose to use a fair dice, in which case the observation distribution is categorical with probabilities $p = \\{\\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}, \\frac{1}{6}\\}$. The casino can also choose a loaded dice which has the following probabilities, $p = \\{\\frac{1}{10}, \\frac{1}{10}, \\frac{1}{10}, \\frac{1}{10}, \\frac{1}{10}, \\frac{1}{5}\\}$, hence it is more likely to roll a six with the loaded dice.\n\nWe want to infer when the casino is using the loaded dice, the latent state $x \\in \\{L, F\\}$ for loaded and fair respectively. We assume we already know the transition matrix\n\n$$\nP = \\begin{pmatrix}\n\\alpha & 1 - \\alpha \\\\\n1 - \\beta & \\beta\n\\end{pmatrix}.\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbuild_transition_matrix <- function(theta) {\n  a <- theta[1]; b <- theta[2]\n  matrix(c(a, 1 - a, 1 - b, b), byrow = T, nrow = 2)\n}\n```\n:::\n\n\nThe observation distribution is $p(y_t|x_t = j)$. This implies we have a two element vector since the state can take one of two values $j = \\{L, F\\}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobservation <- function(y) {\n  c(ifelse(y == 6, 1 / 5, 1 / 10), 1 / 6)\n}\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormalise <- function(a) {\n  a / sum(a)\n}\n\nsim_observation <- function(x) {\n  ifelse(\n      x == 1, \n      sample(6, size = 1), \n      sample(6, size = 1, prob = normalise(c(rep(1, times = 5),  5)))\n    )\n}\n\ntransition <- function(x, P) {\n  sample(2, size = 1, prob = P[x, ])\n}\n\nsim_markov <- function(n, P) {\n  x <- numeric(n)\n  y <- numeric(n)\n  x[1] <- 1\n  y[1] <- sim_observation(x[1])\n  for (i in seq_len(n - 1)) {\n    x[i + 1] <- transition(x[i], P)\n    y[i + 1] <- sim_observation(x[i + 1])\n  }\n  tibble(time = seq_len(n), x, y)\n}\n```\n:::\n\n\nWe can simulate data from this process by specifying values of the parameters, $\\alpha = 0.3$ and $\\beta = 0.1$. This means if the casino is using the loaded dice we will transition to the fair dice with probability $1 - \\alpha$ and if the casino is using the fair dice there is a $1 - \\beta$ probability of transitioning to the loaded dice. The algorithm to simulate from this Hidden Markov model is\n\n1. Specify the initial state of the dice, $x_1 = L$\n2. Simulate an initial observation conditional on the dice used, $y_1 \\sim p(y_1|x_1)$\n3. Simulate the next transition by drawing from a categorical distribution with probabilities corresponding to the row of the transition matrix corresponding the current state, $P_{x_t, \\cdot}$\n4. Simulate an observation conditional on the state, $y_t \\sim p(y_t|x_t)$\n5. Repeat 3 - 4 until the desired number of realisations are simulated\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- 0.3\nb <- 0.1\nP <- build_transition_matrix(c(a, b))\nsims <- sim_markov(300, P)\n```\n:::\n\n\nThe plot below shows 300 simulations from the occasionally dishonest casino.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsims %>% \n  pivot_longer(c(\"x\", \"y\"), names_to = \"key\", values_to = \"value\") %>% \n  ggplot(aes(x = time, y = value)) +\n  geom_step() +\n  facet_wrap(~key, ncol = 1, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## Filtering\n\nNow we wish to identify when the casino is using the loaded dice. We can use the forward filtering algorithm. The first step of the forward algorithm is to the prediction step:\n\n$$p(x_t = k|y_{1:t-1}) = \\sum_{j=1}^K p(x_t = k|x_{t-1}=j)p(x_{t-1}=k|y_{1:t-1})$$\n\nThen we observe a new value of the process $y_t$, and perform the update step where we apply Bayes' Theorem \n\n$$\n\\begin{aligned}\np(x_t = k \\mid y_{1:t}) = p(x_t = k\\mid y_t,y_{1:t-1}) &= \\frac{p(y_{t}|x_t = k, y_{1:t-1})p(x_t = k|y_{1:t-1})}{p(y_{t})} \\\\\n&= \\frac{p(y_{t}|x_t = k)p(x_t = k|y_{1:t-1})}{\\sum_{j=1}^Kp(y_{t}|x_t = j)p(x_t = j|y_{1:t-1})}\n\\end{aligned}\n$$\n\nWhich can be calculated recursively by defining $\\alpha_t(k) = p(x_t = k \\mid y_{1:t})$ then we have the recursive update\n\n$$\\begin{aligned}\n\\alpha_t(k) &= p(y_{t}|x_t = k)p(x_t = k|y_{1:t-1}) \\\\\n&= p(y_{t}|x_t = k)\\sum_{j=1}^Kp(x_t = k|x_{t-1} = j)p(x_{t-1}\\mid y_{1:t-1})\\\\\n&= p(y_{t}|x_t = k)\\sum_{j=1}^Kp(x_t = k|x_{t-1} = j)\\alpha_{t-1}(k)\n\\end{aligned}$$\n\nEssentially we have use the posterior of the previous time point, $\\alpha_{t-1}(k)$ as the prior for next observation. Then we advance the latent-state using the transition distribution $p(x_t = k|x_{t-1} = j)$ and calculate the likelihood of the observation at time $t$ using the observation distribution $p(y_{t}|x_t = k)$.\n\nTo implement the forward algorithm in `R` we can use a higher-order function, a [fold](https://en.wikipedia.org/wiki/Fold_%28higher-order_function%29), from the `R` package [purrr](https://purrr.tidyverse.org/). A higher-order function is a function which accepts a function as an argument or returns a function instead of a value such as a `double` or `int`. This might seem strange at first, but it is very useful and quite common in statistics. Consider maximising a function using an optimisation routine, we pass in a function and the initial arguments and the optimisation function and the function is evaluated at many different values until a plausible maximum is found. This is the basis of the `optim` function in R.\n\nHigher-order functions can be motivated by considering a foundational principle of functional programming, to write pure functions which do not mutate state. A pure function is one which returns the same output value for the same function arguments. This means we can't mutate state by sampling random numbers, write to disk or a database etc. Advanced functional programming languages, such as Haskell, encapsulates this behaviour in Monads. However, Monads and other higher-kinded types are not present in `R`. While we can't use all the useful elements of functional programming in `R`, we can use some, such as higher-order functions.\n\nOne result of avoiding mutable state is that we can't write a for-loop, since a for-loop has a counter which is mutated at each iteration (`i = i + 1`). To overcome this apparent obstacle we can use recursion. Consider the simple example of adding together all elements in a vector, if we are naive we can write a for-loop.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseq <- 1:10\ntotal <- 0\nfor (i in seq_along(seq)) {\n  total = total + seq[i]\n}\ntotal\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 55\n```\n\n\n:::\n:::\n\n\nThis implementation has two variables which are mutated to calculate the final results. To avoid mutating state, we can write a recursive function which calls itself.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloop <- function(total, seq) {\n  if (length(seq) == 0) {\n    total\n  } else {\n    loop(total + seq[1], seq[-1])\n  }\n}\n\nloop(0, 1:10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 55\n```\n\n\n:::\n:::\n\n\n`R` does not have [tail-call elimination](https://en.wikipedia.org/wiki/Tail_call) and hence this recursive function will not work with long sequences, however it does not mutate any state. We can generalise this function to be a higher-order function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfold <- function(init, seq, f) {\n  if (length(seq) == 0) {\n    init\n  } else {\n    fold(f(init, seq[1]), seq[-1], f)\n  }\n}\n```\n:::\n\n\nHere the function `fold` applies the user-specified binary function `f` to the initial value `init` and the first element of the sequence. The result of applying `f` to these values is then used as the next initial value with the rest of the sequence. We can use this to calculate any binary reduction we can think of.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfold(1, seq, function(x, y) x * y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3628800\n```\n\n\n:::\n:::\n\n\nThis example is equivalent to the `reduce` function in [purrr](https://purrr.tidyverse.org/). `purrr::reduce` by default can be used to combine the elements of a vector or list using a binary function starting with the first element in the list. For instance we can calculate the sum of a vector of numbers\n\n\n::: {.cell}\n\n```{.r .cell-code}\npurrr::reduce(1:10, function(x, y) x + y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 55\n```\n\n\n:::\n:::\n\n\nWe can also use the function shorthand provided in `purrr`, where `function(x, y) x + y` can be written `~ .x + .y`.\n\nOther arguments provided to the `reduce` function can change its behaviour such as reversing the direction by changing the `.direction` argument (which will not affect the above computation, since addition is associative, ie. $(1 + (2 + 3)) = ((1 + 2) + 3)$). We can also provide an initial value (`.init`) to the computation, instead of starting with the first (or last) element of the list.\n\n`purrr::accumulate` is similar to reduce, however it does not discard intermediate computations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npurrr::accumulate(1:10, `+`)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  1  3  6 10 15 21 28 36 45 55\n```\n\n\n:::\n:::\n\n\nHence, if we change the direction this will change the output.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npurrr::accumulate(1:10, `+`, .dir = \"backward\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 55 54 52 49 45 40 34 27 19 10\n```\n\n\n:::\n:::\n\n\nThese functions can appear strange at first, however they don't suffer from common problems such as off-by-one errors when writing a for-loop with indexing. \n\nThe `accumulate` function can be used to write the forward algorithm by first writing a single step in the forward algorithm. The function `forward_step` accepts the current smoothed state at time t-1, `alpha`, and the observed value at time t, `y`. The arguments `observation` and `P` represent the observation distribution and the transition matrix respectively and remain constant in this example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforward_step <- function(alpha, y, observation, P) {\n  normalise(observation(y) * t(P) %*% alpha)\n}\n```\n:::\n\n\nThe forward algorithm can then be written using the `accumulate` function by first calculating the initial value of `alpha` and using this as the value `.init` then the function `forward_step` is used with the values of `observation` and `P` set. `accumulate` then takes uses the initial value, `.init` and the first value of the observations, `ys` (technically the second since we use the first to initialise `alpha`) to produce the next `alpha` value. This new `alpha` value is passed to the next invocation of `forward_step` along with the next observed value and so on until the observation vector is exhausted. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nforward <- function(ys, x0, observation, P) {\n  alpha <- normalise(observation(ys[1]) * x0)\n  purrr::accumulate(\n    ys[-1],\n    forward_step,\n    observation = observation,\n    P = P,\n    .init = alpha\n  )\n}\n```\n:::\n\n\nWe assume that the dice used for the initial roll can be either loaded or fair with equal probability.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforward_path <- forward(sims$y, x0 = c(0.5, 0.5), observation, P)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiltered_path <- purrr::map_dbl(forward_path, which.max)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  time = seq_len(length(filtered_path)),\n  filtered_state = filtered_path,\n  p_loaded = purrr::map_dbl(forward_path, ~ .[1])\n) %>% \n  inner_join(sims, by = \"time\") %>% \n  pivot_longer(c(\"x\", \"y\", \"p_loaded\"), names_to = \"key\", values_to = \"value\") %>% \n  ggplot(aes(x = time, y = value)) +\n  geom_step() +\n  facet_wrap(~key, ncol = 1, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n## Parameter inference\n\nWe can calculate the log-probability of the evidence using the forward algorithm, this is the sum of un-normalised filtering distribution\n\n$$\\log p(y_{1:T}) = \\log \\sum_{i=1}^T\\sum_{j=1}^K p(x_t=j\\mid y_{1:t-1})p(y_t|x_t = j)$$\n\nThis can be used in a Metropolis-Hastings algorithm to determine the posterior distribution of the parameters in the transition matrix, $\\alpha$ and $\\beta$. We can keep a running total of log-likelihood by returning a list from the forward step containing the log-likelihood and the posterior probability of the states given the observation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nll_step <- function(state, y, observation, P) {\n  unnorm_state <- observation(y) * t(P) %*% state[[2]]\n  list(\n    state[[1]] + sum(log(unnorm_state)),\n    normalise(unnorm_state)\n  )\n}\n```\n:::\n\n\nTo return only the log-likelihood we can use `purrr::reduce`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_likelihood <- function(ys, x0, observation, P) {\n  alpha <- normalise(observation(ys[1]) * x0)\n  init <- list(0, alpha)\n  purrr::reduce(ys, function(x, y) ll_step(x, y, observation, P), .init = init)[[1]]\n}\n```\n:::\n\n\nWe can use this marginal-likelihood in a Metropolis-Hastings algorithm. We define the prior on the parameters of the transition matrix to be independent Gamma distributions with shape, $\\alpha = 3$, and rate $\\beta = 3/0.1$. The log-posterior is the sum of the log-likelihood calculated using the forward filtering algorithm and the log-prior.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_prior <- function(theta) {\n  sum(dgamma(theta, shape = 3, rate = 3 / 0.1, log = T))\n}\n\nlog_posterior <- function(theta) {\n  transition_matrix <- build_transition_matrix(theta)\n  log_likelihood(sims$y, c(1, 0), observation, transition_matrix) + log_prior(theta)\n}\n```\n:::\n\n\nThe proposal distribution is a normal centered at the un-constrained value of the parameter. We use the logit function to transform $\\alpha$ and $\\beta$ from $\\operatorname{logit}:[0, 1] \\rightarrow \\mathbb{R}$ then propose using a Normal distribution centered as the un-constrained value and proceed to transform the parameter back to the original scale using the logistic function, $\\operatorname{logistic}:\\mathbb{R} \\rightarrow [0, 1]$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic <- function(x) {\n  1 / (1 + exp(-x))\n}\n\nlogit <- function(p) {\n  log(p / (1 - p))\n}\n\nproposal <- function(theta) {\n  a <- theta[1]; b <- theta[2]\n  transformed <- logit(c(a, b))\n  prop <- rnorm(2, mean = transformed, sd = 0.1)\n  logistic(prop)\n}\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninit_theta <- c(a, b)\nnames(init_theta) <- c(\"alpha\", \"beta\")\n\niters_metropolis <- metropolis(\n  theta = init_theta,\n  log_posterior,\n  proposal,\n  m = 1e4,\n  chains = 4,\n  parallel = T\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRegistered S3 method overwritten by 'future':\n  method               from      \n  all.equal.connection parallelly\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'future' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'purrr' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: UNRELIABLE VALUE: Future (NULL) unexpectedly generated random numbers\nwithout specifying argument 'seed'. There is a risk that those random numbers\nare not statistically sound and the overall results might be invalid. To fix\nthis, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced. To disable this check, use 'seed=NULL', or set option\n'future.rng.onMisuse' to \"ignore\".\n```\n\n\n:::\n:::\n\n\nWe draw 10,000 iterations from the Metropolis algorithm, the parameter diagnostics are plotted below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nactual_values <- tibble(\n  parameter = c(\"alpha\", \"beta\"),\n  actual_value = c(a, b)\n)\n\niters_metropolis %>% \n  group_by(chain) %>%\n  mutate(iteration = row_number()) %>% \n  filter(iteration > 5e3) %>%\n  pivot_longer(3:4, names_to = \"parameter\", values_to = \"value\") %>% \n  plot_diagnostics()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}